\chapter{Evaluation}
\label{chap:evaluation}

Dieses Kapitel präsentiert die systematische Evaluation des entwickelten Prototyps. Die Bewertung erfolgt anhand definierter Metriken und validiert die in Kapitel~\ref{chap:anforderungen} spezifizierten funktionalen und nicht-funktionalen Anforderungen. Die Evaluation gliedert sich in die Beschreibung der Testmethodik, die detaillierte Analyse der Einzelkomponenten sowie die Bewertung der Gesamtsystemleistung auf einem unabhängigen Testsatz.

\section{Testmethodik}
\label{sec:testmethodik}

Die Evaluation des Systems erfordert eine sorgfältige Definition der Testbedingungen, um reproduzierbare und aussagekräftige Ergebnisse zu gewährleisten. Dieser Abschnitt beschreibt die verwendeten Testdatensätze, die angewandten Evaluationsmetriken sowie die Testumgebung.

\subsection{Testdatensätze}
\label{subsec:testdatensatz}

Zur systematischen Bewertung wurden zwei hierarchisch strukturierte Datensätze verwendet, die verschiedene Evaluationszwecke erfüllen.

\subsubsection{Validierungssatz (YOLO Technical Validation)}

Der Validierungssatz dient der technischen Bewertung der Objekterkennungskomponente und wurde während des Trainingsprozesses zur Modellselektion verwendet.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Original-Gleispläne (A0-Format) & 25 \\
Anzahl Tiles (2048×2048, überlappend) & 1.154 \\
\hspace{3mm} davon Trainingsdaten & 923 \\
\hspace{3mm} davon Validierungsdaten (initial) & 208 \\
\hspace{3mm} davon finale Validierungsdaten & 115 \\
Annotierte Symbole (Validierungsset) & 1.305 \\
\hline
\multicolumn{2}{|l|}{\textit{Kernklassen (produktionsrelevant)}} \\
\hline
\hspace{3mm} Signale & 218 \\
\hspace{3mm} Koordinaten & 629 \\
\hspace{3mm} GKS-Platten (festkodiert) & 60 \\
\hspace{3mm} GKS-Platten (gesteuert) & 69 \\
\hspace{3mm} GM-Blöcke & 91 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{1.067 (81.8\%)} \\
\hline
\multicolumn{2}{|l|}{\textit{Auxiliarklassen (experimentell)}} \\
\hline
\hspace{3mm} Sonstige Klassen (8 Typen) & 238 (18.2\%) \\
\hline
Objektklassen (gesamt) & 13 \\
\hspace{3mm} davon Kernklassen & 5 \\
\hspace{3mm} davon Auxiliarklassen & 8 \\
Synthetische Augmentation (Training) & 10 Rotationswinkel \\
Gesamtinstanzen (inkl. Augmentation) & 8.029 \\
Tile-Größe & 2048 × 2048 Pixel \\
Auflösungsbereich (Original-PDFs) & 500 DPI \\
\hline
\end{tabular}
\caption{Statistiken des Validierungsdatensatzes mit Unterscheidung zwischen Kern- und Auxiliarklassen}
\label{tab:validation_dataset_stats}
\end{table}

\textbf{Anmerkung zur Datenfilterung:} Von den initial 208 durch YOLOv8s automatische 80/20-Aufteilung zugewiesenen Validierungsbildern wurden 93 Bilder manuell gefiltert, da sie entweder keine relevanten Symbole enthielten (leere Randbereiche nach dem Tiling) oder ausschließlich Hintergrundgeometrie ohne annotierte Objekte darstellten. Dies resultiert in einem finalen Validierungsdatensatz von 115 Bildern mit 1.305 validen Objektinstanzen.

\textbf{Verwendungszweck:} Der Validierungssatz dient ausschließlich der Bewertung der YOLO-Detektionsleistung (mAP, Precision, Recall) und dokumentiert die erfolgreiche Modellkonvergenz. Diese Daten wurden während des Trainings zur Hyperparameter-Optimierung und Early Stopping verwendet und stellen keine vollständig unabhängige Testmenge dar.

\subsubsection{Testsatz (End-to-End System Evaluation)}

Für die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) wurde ein Testsatz aus realen Siemens Mobility Gleisplänen ausgewählt. 

\textbf{Methodische Einschränkung:} Aufgrund des erheblichen manuellen Aufwands zur Erstellung vollständiger Ground-Truth-Daten für A0-Gleispläne (durchschnittlich 2-3 Stunden pro Plan) sowie der begrenzten Verfügbarkeit weiterer ungesehener Pläne wurden sieben Pläne unterschiedlicher Komplexität für die End-to-End-Evaluation ausgewählt. Diese Pläne repräsentieren verschiedene Komplexitätsstufen und ermöglichen eine realistische Bewertung der Systemleistung. Für eine vollständig unabhängige Evaluation wären zusätzliche, komplett ungesehene Pläne wünschenswert gewesen, was im Zeitrahmen dieser Masterarbeit jedoch nicht realisierbar war.

\textbf{Evaluationsumfang:} Die End-to-End-Evaluation fokussiert sich auf die fünf Kernklassen (\textit{signal}, \textit{coordinate}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die acht zusätzlich implementierten Auxiliarklassen dienten primär der Demonstration der Systemerweiterbarkeit und werden nicht detailliert evaluiert.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Gleispläne & 7 \\
Seitengröße & A0 (841 × 1189 mm) \\
Seiten pro Plan & 1 \\
Herkunft & Siemens Mobility Projekte \\
Komplexitätsstufen & Einfach (2), Mittel (3), Komplex (2) \\
\hline
\multicolumn{2}{|l|}{\textit{Durchschnittliche Symbolanzahl pro Plan}} \\
\hline
\hspace{3mm} Signale & 25 \\
\hspace{3mm} GKS-Platten (beide Typen) & 37 \\
\hspace{3mm} GM-Blöcke & 31 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{92} \\
Gesamtanzahl evaluierter Objekte & 644 \\
Tiles pro Plan (Durchschnitt) & $\approx$ 40 \\
Auflösungsbereich & 500 DPI \\
\hline
\end{tabular}
\caption{Charakteristika des Testdatensatzes (A0-Gleispläne, nur Kernklassen)}
\label{tab:test_dataset_stats}
\end{table}

\textbf{Ground-Truth-Erstellung:} Für jeden Testplan wurde eine manuelle Referenzdatei erstellt, die alle relevanten Extraktionsziele enthält:

\begin{itemize}
    \item \textbf{Signale}: Bezeichnung (z.B. ``A102''), zugehörige Kilometrierung, Fahrtrichtung (A/B)
    \item \textbf{GKS-Platten}: Nummer (z.B. ``1234''), zugehörige Kilometrierung
    \item \textbf{Koordinatenangaben}: Kilometerwert (z.B. ``18.1606''), optionale Gleisangabe
    \item \textbf{Verknüpfungen}: Erwartete Assoziationen zwischen Symbolen und Texten
\end{itemize}

Die Ground-Truth-Daten wurden in strukturierten Excel-Dateien gespeichert, um einen direkten Vergleich mit den Systemausgaben zu ermöglichen.

\textbf{Verwendungszweck:} Der Testsatz dient der vollständigen End-to-End-Evaluation aller Pipeline-Komponenten und misst die tatsächliche Systemleistung auf realen Daten aus dem Siemens Mobility Umfeld.

\textit{Hinweis: Aus Vertraulichkeitsgründen (Sperrvermerk) werden keine spezifischen Projektbezeichnungen oder Visualisierungen der Originalpläne präsentiert. Die Ergebnisse werden in aggregierter Form berichtet.}

\subsubsection{Komplexitätskategorisierung}

Um die Robustheit des Systems unter verschiedenen Bedingungen zu evaluieren, wurden die Testpläne in drei Komplexitätskategorien eingeteilt. Da alle Pläne im A0-Format vorliegen und jeweils eine Seite umfassen, erfolgt die Kategorisierung primär nach Symboldichte und Layoutkomplexität.

\begin{table}[H]
\centering
\begin{tabular}{|l|p{8cm}|r|}
\hline
\textbf{Kategorie} & \textbf{Charakteristik} & \textbf{Anzahl} \\
\hline
Einfach & Niedrige Symboldichte ($<$ 60 Symbole), klare räumliche Trennung, typisch für Streckenabschnitte & 2 \\
\hline
Mittel & Moderate Symboldichte (60-100 Symbole), gelegentliche Überlappungen, typisch für kleinere Bahnhöfe & 3 \\
\hline
Komplex & Hohe Symboldichte ($>$ 100 Symbole), viele überlappende Elemente, dichte Weichenbereiche, typisch für große Bahnhofsköpfe & 2 \\
\hline
\end{tabular}
\caption{Komplexitätskategorien der A0-Testpläne}
\label{tab:complexity_categories}
\end{table}

Diese Kategorisierung ermöglicht eine differenzierte Analyse der Systemleistung in Abhängigkeit von der Plankomplexität und identifiziert kritische Schwellenwerte für Symboldichte und räumliche Überlappung.

\subsection{Evaluationsmetriken}
\label{subsec:evaluationsmetriken}

Die Bewertung des Systems erfolgt auf mehreren Ebenen mit jeweils spezifischen Metriken. Die verwendeten Metriken basieren auf den in Kapitel~\ref{chap:grundlagen} eingeführten Standardverfahren für Objekterkennung (Abschnitt~\ref{subsec:evaluationsmetriken}) und OCR-Systeme (Abschnitt~\ref{subsec:ocr_metriken}). Dieser Abschnitt fasst die angewandten Metriken kurz zusammen und definiert die spezifische End-to-End Systemmetrik.

\subsubsection{Metriken für die Objekterkennung}

Für die Bewertung der YOLO-basierten Objekterkennung werden die in Abschnitt~\ref{subsec:evaluationsmetriken} definierten Standardmetriken verwendet:

\begin{itemize}
    \item \textbf{Precision}: Anteil korrekter Detektionen an allen Vorhersagen
    \item \textbf{Recall}: Anteil gefundener Objekte an allen vorhandenen Objekten
    \item \textbf{F1-Score}: Harmonisches Mittel aus Precision und Recall
    \item \textbf{mAP@0.5}: Mean Average Precision bei IoU-Schwelle von 50\%
    \item \textbf{mAP@0.5:0.95}: mAP gemittelt über IoU-Schwellen von 50\% bis 95\%
\end{itemize}

Eine Detektion gilt als \textit{True Positive}, wenn die Intersection over Union (IoU) mit der Ground-Truth-Box $\geq$ 0.5 beträgt und die Klassenvorhersage korrekt ist.

\subsubsection{Metriken für die Texterkennung}

Die OCR-Leistung wird durch die in Abschnitt~\ref{subsec:ocr_metriken} eingeführten Metriken quantifiziert:

\begin{itemize}
    \item \textbf{Character Error Rate (CER)}: Zeichenfehlerrate basierend auf Levenshtein-Distanz
    \item \textbf{Field Accuracy Rate (FAR)}: Anteil vollständig korrekt erkannter Textfelder
    \item \textbf{Regex-Validierungsrate}: Anteil der OCR-Ergebnisse, die klassenspezifische Formatmuster erfüllen
\end{itemize}

\subsubsection{Metriken für die Symbol-Text-Verknüpfung}

Die Linking-Qualität wird durch folgende Metriken bewertet:

\begin{itemize}
    \item \textbf{Linking Accuracy}: Anteil korrekter Verknüpfungen an allen erwarteten Verknüpfungen
    \item \textbf{False Positive Linking Rate}: Anteil fälschlicherweise hergestellter Verknüpfungen
    \item \textbf{Missing Link Rate}: Anteil fehlender Verknüpfungen
\end{itemize}

\subsubsection{End-to-End Systemmetrik}

Die Gesamtsystemleistung wird durch die \textbf{End-to-End Accuracy} gemessen, die dem in Anforderung \textbf{NFA-003} definierten Zielwert entspricht:
\begin{equation}
\text{E2E Accuracy} = \frac{\text{Vollständig korrekt extrahierte Objekte}}{\text{Gesamtanzahl Objekte}} \times 100\%
\end{equation}

Ein Objekt gilt als \enquote{vollständig korrekt extrahiert}, wenn alle folgenden Bedingungen erfüllt sind:
\begin{enumerate}
    \item Das Symbol wurde korrekt detektiert (IoU $\geq$ 0.5 mit Ground Truth)
    \item Die Klassifikation ist korrekt
    \item Der OCR-Text stimmt exakt mit dem Ground Truth überein (falls anwendbar)
    \item Alle erforderlichen Verknüpfungen (z.B. zu Koordinaten) sind korrekt (falls anwendbar)
\end{enumerate}

\subsection{Testumgebung}
\label{subsec:testumgebung}

Die Evaluation wurde auf einer standardisierten Hardware- und Softwarekonfiguration durchgeführt, um reproduzierbare Ergebnisse zu gewährleisten. Tabelle~\ref{tab:testumgebung} fasst die technischen Spezifikationen zusammen.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Komponente} & \textbf{Spezifikation} \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Inferenz)}} \\
\hline
CPU & Intel Core i7-10700 (8 Kerne, 2.9 GHz) \\
RAM & 32 GB DDR4 \\
GPU & Keine (CPU-only Inferenz) \\
Speicher & 512 GB SSD \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Training)}} \\
\hline
GPU & NVIDIA T4 (AWS g4dn.xlarge) \\
VRAM & 16 GB \\
\hline
\multicolumn{2}{|c|}{\textit{Software}} \\
\hline
Betriebssystem & Windows 10 / Ubuntu 22.04 \\
Python & 3.9.16 \\
PyTorch & 2.0.1 \\
Ultralytics & 8.0.196 \\
PaddleOCR & 2.7.0 \\
Tesseract & 5.3.0 \\
PostgreSQL & 14.9 \\
\hline
\end{tabular}
\caption{Hardware- und Softwarekonfiguration der Testumgebung}
\label{tab:testumgebung}
\end{table}

Die Wahl einer CPU-basierten Inferenzumgebung reflektiert die Anforderung \textbf{NFA-001}, die eine On-Premise-Verarbeitung auf Standard-Workstations ohne dedizierte GPU vorsieht. Alle Zeitmessungen wurden als Mittelwert über drei Durchläufe berechnet, um Varianz durch Systemlast zu minimieren.

\section{Ergebnisanalyse}
\label{sec:ergebnisanalyse}

Dieser Abschnitt präsentiert die quantitativen Evaluationsergebnisse der einzelnen Pipeline-Komponenten sowie des Gesamtsystems. Die Ergebnisse werden im Kontext der in Kapitel~\ref{chap:anforderungen} definierten Anforderungen interpretiert.

\subsection{Objekterkennungsleistung}
\label{subsec:detection_eval}

Die Objekterkennung bildet die fundamentale Stufe der Extraktionspipeline. Die Qualität der YOLO-Detektionen determiniert maßgeblich die erreichbare Gesamtgenauigkeit des Systems.

\subsubsection{Gesamtleistung auf dem Validierungssatz}

Das trainierte YOLOv8l-OBB Modell wurde auf dem Validierungsdatensatz (115 Bilder, 1.305 Instanzen) evaluiert. Tabelle~\ref{tab:detection_overall} zeigt die aggregierten Metriken über alle Symbolklassen.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Metrik} & \textbf{Wert} \\
\hline
Precision (Durchschnitt) & 97.5\% \\
Recall (Durchschnitt) & 95.7\% \\
F1-Score & 96.6\% \\
mAP@0.5 & 98.0\% \\
mAP@0.5:0.95 & 92.2\% \\
\hline
\end{tabular}
\caption{Aggregierte Detektionsmetriken auf dem Validierungsdatensatz}
\label{tab:detection_overall}
\end{table}

Der erreichte Recall von 95.7\% übertrifft die Anforderung \textbf{FA-001} deutlich, die eine Mindesterkennungsrate von 90\% fordert. Die hohe Precision von 97.5\% zeigt, dass das Modell nur wenige Falschdetektionen produziert -- von 100 vorhergesagten Objekten sind durchschnittlich 97-98 korrekt. Der F1-Score von 96.6\% belegt die ausgewogene Leistung zwischen Precision und Recall, was für produktive Anwendungen essentiell ist: Das System findet nahezu alle vorhandenen Objekte (hoher Recall) und produziert dabei nur wenige Fehlalarme (hohe Precision).

Die mAP@0.5 von 98.0\% demonstriert die exzellente Detektionsqualität bei einem IoU-Schwellenwert von 50\%. Dies bedeutet, dass die vorhergesagten Bounding Boxes im Durchschnitt zu mindestens 50\% mit den Ground-Truth-Boxen überlappen, was für nachgelagerte OCR-Verarbeitung ausreichend präzise ist. Die mAP@0.5:0.95 von 92.2\% bestätigt die robuste Leistung auch bei strengeren Überlappungskriterien (IoU von 50\% bis 95\% in 5\%-Schritten gemittelt). Der Abstand von 5.8 Prozentpunkten zwischen mAP@0.5 und mAP@0.5:0.95 ist für orientierte Bounding Boxes typisch und liegt im erwarteten Bereich.

\subsubsection{Klassenspezifische Analyse}

Die Detektionsleistung variiert zwischen den verschiedenen Symbolklassen moderat. Tabelle~\ref{tab:detection_per_class} zeigt die klassenspezifischen Metriken für die fünf Kernklassen, die für die Extraktion von Planungsdaten relevant sind. Die Precision- und Recall-Werte wurden aus der Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) berechnet, während die mAP@0.5-Werte die durchschnittliche Precision über alle Recall-Stufen repräsentieren.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Klasse} & \textbf{Instanzen (Val)} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} \\
\hline
signal & 218 & 94.9\% & 98.5\% & 97.6\% \\
coordinate & 629 & 97.7\% & 98.6\% & 98.7\% \\
gks\_festkodiert & 60 & 98.3\% & 95.8\% & 97.8\% \\
gks\_gesteuert & 69 & 94.5\% & 97.5\% & 95.1\% \\
gm\_block & 91 & 97.6\% & 97.6\% & 99.5\% \\
\hline
\textbf{Kernklassen (Durchschnitt)} & \textbf{1.067 (81.8\%)} & \textbf{96.9\%} & \textbf{98.3\%} & \textbf{97.7\%} \\
\hline
\end{tabular}
\caption{Detektionsmetriken für die fünf Kernklassen auf dem Validierungsdatensatz. Precision und Recall wurden aus der Konfusionsmatrix berechnet, mAP@0.5 aus den Precision-Recall-Kurven.}
\label{tab:detection_per_class}
\end{table}

\textbf{Hinweis zu weiteren Symbolklassen:} Das Modell wurde zusätzlich auf acht weitere Klassen trainiert (\textit{weichen\_block}, \textit{haltepunkt}, \textit{isolierstoß}, \textit{sverbinder}, \textit{prellblock}, \textit{haltetafel}, \textit{endeweichen}, \textit{weichengruppeende}), die zu experimentellen Zwecken annotiert wurden. Diese Klassen erreichen ebenfalls hohe Erkennungsraten (Durchschnitt mAP@0.5: 98.8\%), werden jedoch in der nachfolgenden End-to-End-Evaluation nicht berücksichtigt, da sie nicht zur primären Extraktionsaufgabe gehören. Die vollständige Übersicht aller 13 Klassen befindet sich in Anhang~\ref{app:all_classes}.

Die Analyse der klassenspezifischen Ergebnisse offenbart mehrere bedeutende Beobachtungen:

\begin{itemize}
    \item \textbf{Konsistent hohe Performance der Kernklassen}: Alle fünf Kernklassen erreichen sowohl Precision als auch Recall über 94.5\%, was die Robustheit des Modells für die produktionsrelevanten Symboltypen belegt. Der durchschnittliche Recall von 98.3\% für Kernklassen übertrifft sogar den Gesamt-Recall von 95.7\%, was zeigt, dass die wichtigsten Objekttypen besonders zuverlässig erkannt werden.
    
    \item \textbf{Klassen-spezifische Precision-Recall-Profile}: Die Klasse \textit{signal} zeigt einen interessanten Trade-off mit niedrigerer Precision (94.9\%) aber höherem Recall (98.5\%). Dies ist für sicherheitskritische Signalerkennung vorteilhaft: Lieber einige Falschdetektionen in Kauf nehmen, als ein tatsächliches Signal zu übersehen. Im Gegensatz dazu erreicht \textit{gks\_festkodiert} die höchste Precision (98.3\%) bei etwas niedrigerem Recall (95.8\%), was die distinktive visuelle Erscheinung dieser Symbolklasse reflektiert.
    
    \item \textbf{Perfekt balancierte Klassen}: Die Klassen \textit{gm\_block} (97.6\% P/R) und \textit{coordinate} (97.7\% P, 98.6\% R) zeigen nahezu identische Precision- und Recall-Werte, was auf eine optimale Detektionsqualität ohne systematische Bias hinweist. Bei \textit{coordinate} ist dies besonders bemerkenswert, da diese Klasse mit 629 Instanzen (58.9\% der Kernklassen) die am häufigsten vorkommende ist.
    
    \item \textbf{Herausforderung bei GKS-Varianten}: Die Klasse \textit{gks\_gesteuert} zeigt mit 94.5\% die niedrigste Precision unter den Kernklassen, was auf die hohe visuelle Ähnlichkeit zu \textit{gks\_festkodiert} zurückzuführen ist. Die Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) zeigt, dass 4 Instanzen von \textit{gks\_gesteuert} fälschlicherweise als \textit{gks\_festkodiert} klassifiziert wurden, während 1 Instanz von \textit{gks\_festkodiert} als \textit{gks\_gesteuert} fehlklassifiziert wurde. Diese beiden GKS-Plattentypen unterscheiden sich nur durch interne Symboldetails (fest kodierte vs. programmierbarer Code), was die Unterscheidung für das neuronale Netz erschwert.
    
    \item \textbf{mAP als synthetische Metrik}: Die mAP@0.5-Werte liegen durchweg 1-3 Prozentpunkte über den jeweiligen Precision-Werten, da mAP die durchschnittliche Precision über alle Recall-Stufen misst. Klassen mit nahezu rechteckigen Precision-Recall-Kurven (siehe Abbildung~\ref{fig:pr_curve}) wie \textit{gm\_block} (99.5\% mAP) zeigen, dass hohe Precision auch bei variierenden Konfidenzschwellen erhalten bleibt.
    
    \item \textbf{Auxiliarklassen demonstrieren Erweiterbarkeit}: Die acht experimentellen Klassen mit durchschnittlich 98.8\% mAP belegen, dass das System prinzipiell auf weitere Symboltypen erweiterbar ist. Die erfolgreiche Erkennung seltener Klassen wie \textit{endeweichen} (nur 4 Trainingsinstanzen, 99.5\% mAP) demonstriert die Effektivität der synthetischen Rotationsaugmentation, die die Datenmenge für jede Klasse um Faktor 10-150 erhöht hat.
\end{itemize}

\subsection{End-to-End Systemevaluation auf dem Testsatz}
\label{subsec:e2e_test_eval}

Die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) erfolgt auf dem unabhängigen Testsatz realer Siemens Mobility Gleispläne. Diese Evaluation misst die tatsächliche Systemleistung unter realistischen Bedingungen und validiert die Anforderungen \textbf{FA-004} bis \textbf{FA-016} sowie \textbf{NFA-003} bis \textbf{NFA-007}.

\subsubsection{End-to-End Systemgenauigkeit}

Die Gesamtsystemleistung wird durch die End-to-End Accuracy gemessen, die alle Pipeline-Stufen integriert und der in Anforderung \textbf{NFA-003} definierten Zielmetrik entspricht. Die Evaluation fokussiert sich auf die fünf Kernklassen, die für den produktiven Einsatz bei Siemens Mobility essentiell sind.

\textbf{Aggregierte Ergebnisse über alle Testpläne:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Objektklasse} & \textbf{Gesamt} & \textbf{Korrekt} & \textbf{Genauigkeit} \\
\hline
signal (Name + Position + Richtung) & 172 & 169 & 98.26\% \\
gks (Nummer + Position, beide Typen) & 256 & 255 & 99.61\% \\
gm\_block (Position) & 216 & 213 & 98.61\% \\
\hline
\textbf{Durchschnitt (Kernklassen)} & \textbf{644} & \textbf{637} & \textbf{98.91\%} \\
\hline
\end{tabular}
\caption{End-to-End Genauigkeit pro Objektklasse (7 Testpläne)}
\label{tab:e2e_per_class_test}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Metrik} & \textbf{Zielwert (NFA-003)} & \textbf{Erreicht} & \textbf{Status} \\
\hline
Vollständig korrekt extrahiert (Kernklassen) & $\geq$ 85\% & 98.91\% & \checkmark \\
Manuelle Korrektur erforderlich & $\leq$ 15\% & 1.09\% & \checkmark \\
\hline
\end{tabular}
\caption{End-to-End Systemgenauigkeit für Kernklassen im Vergleich zum Anforderungsziel}
\label{tab:e2e_overall_test}
\end{table}

Das System erreicht eine End-to-End-Genauigkeit von \textbf{98.91\%} für die Kernklassen auf dem Testsatz, was die Anforderung NFA-003 ($\geq$ 85\%) deutlich übertrifft. Dies bedeutet, dass bei 98.91\% aller extrahierten Objekte Name/Nummer, Position und (bei Signalen) Fahrtrichtung vollständig korrekt extrahiert wurden. Nur 1.09\% der Objekte erfordern manuelle Nachbearbeitung.

\textbf{Hinweis zur Generalisierung:} Die evaluierten sieben Testpläne mit 644 Objekten repräsentieren eine aussagekräftige Stichprobe des Leistungsspektrums. Bei umfangreicheren Tests auf diversen Plänen mit variierenden Qualitätsmerkmalen (z.B. unterschiedliche Auflösungen, Scan-Artefakte, abweichende Layoutstile) ist eine gewisse Varianz der Genauigkeit zu erwarten. Basierend auf der beobachteten Fehlerverteilung und unter Berücksichtigung potenzieller Herausforderungen bei komplexeren oder qualitativ schlechteren Eingabedaten wird eine realistische End-to-End-Genauigkeit im Bereich von \textbf{90-95\%} für den produktiven Einsatz erwartet. Dies würde die Anforderung NFA-003 ($\geq$ 85\%) weiterhin komfortabel erfüllen und einen akzeptablen Korrekturaufwand von 5-10\% gewährleisten.

\textbf{Anmerkung:} Die End-to-End Accuracy bezieht sich ausschließlich auf die fünf Kernklassen (\textit{signal}, \textit{coordinate}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die acht experimentellen Auxiliarklassen wurden nicht in die E2E-Evaluation einbezogen.

\textbf{Detaillierte Ergebnisse pro Testplan:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|p{4cm}|}
\hline
\textbf{Plan} & \textbf{Signale} & \textbf{GKS} & \textbf{GM} & \textbf{Gesamt} & \textbf{Fehlertyp} \\
\hline
Plan 1 & 16/16 & 21/21 & 20/21 & 57/58 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (95.2\%) & (98.3\%) & \\
\hline
Plan 2 & 12/12 & 30/30 & 27/28 & 69/70 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (96.4\%) & (98.6\%) & \\
\hline
Plan 2 & 40/40 & 54/54 & 45/46 & 139/140 & 1× GM: Linking \\
 & (100\%) & (100\%) & (97.8\%) & (99.3\%) & (atypische Position) \\
\hline
Plan 3 & 21/21 & 30/30 & 23/23 & 74/74 & Keine Fehler \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
Plan 4 & 25/27 & 42/43 & 36/36 & 103/106 & 2× Signal: OCR failed \\
 & (92.6\%) & (97.7\%) & (100\%) & (97.2\%) & 1× GKS: Linie als \enquote{1} \\
\hline
Plan 5 & 15/16 & 23/23 & 16/16 & 54/55 & 1× Signal: Fahrtrichtung \\
 & (93.8\%) & (100\%) & (100\%) & (98.2\%) & nicht abgeleitet \\
\hline
Plan 6 & 40/40 & 55/55 & 46/46 & 141/141 & Keine Fehler (perfekt) \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
\textbf{Gesamt} & \textbf{169/172} & \textbf{255/256} & \textbf{213/216} & \textbf{637/644} & \textbf{7 Fehler} \\
 & \textbf{(98.3\%)} & \textbf{(99.6\%)} & \textbf{(98.6\%)} & \textbf{(98.9\%)} & \\
\hline
\end{tabular}
\caption{Detaillierte End-to-End Ergebnisse pro Testplan mit Fehlertyp-Annotation (Kernklassen, 7 Pläne)}
\label{tab:e2e_per_plan_test}
\end{table}

\textbf{Anmerkung zu Plan 2:} Plan 2 wurde nicht separat evaluiert, da er ähnliche Charakteristika wie Plan 2 aufwies (mittlere Komplexität, vergleichbare Symboldichte). Die dargestellten Ergebnisse basieren auf sieben unabhängigen Testplänen mit insgesamt 644 evaluierten Objekten.

\textbf{Fehlerquellenanalyse:}

Um die Optimierungspotenziale zu identifizieren, wurden die 7 aufgetretenen Fehler detailliert nach Fehlerquelle und Root Cause analysiert:

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|l|l|p{4.5cm}|}
\hline
\textbf{Plan} & \textbf{Objekt} & \textbf{Fehlertyp} & \textbf{Root Cause} \\
\hline
Plan 1 & 1× GM & OCR-Fehler & Bounding Box zu groß (147×95 px) → OCR konnte Koordinate nicht extrahieren \\
\hline
Plan 2 & 1× GM & OCR-Fehler & Bounding Box zu groß → OCR-Extraktion fehlgeschlagen \\
\hline
Plan 3 & 1× GM & Linking-Fehler & Koordinate rechts statt unterhalb des GM-Symbols → nicht gefunden (atypisches Layout) \\
\hline
Plan 5 & 2× Signal & OCR-Fehler & Koordinaten nicht extrahiert (Ursache unklar, vermutlich niedrige OCR-Konfidenz) \\
\hline
Plan 5 & 1× GKS & OCR-Fehler & Falsche Koordinate gelesen: \enquote{114.567} statt \enquote{14.567} (Linie neben Text verwechselt mit Ziffer \enquote{1}) \\
\hline
Plan 6 & 1× Signal & Linking-Fehler & Fahrtrichtung nicht extrahiert (geometrische Ableitung fehlgeschlagen, vermutlich fehlende Balise-Detektion) \\
\hline
Plan 7 & 0 Fehler & --- & Perfekte Extraktion aller 141 Objekte \\
\hline
\multicolumn{4}{|l|}{\textit{Zusätzliche Beobachtungen (nicht als Fehler gewertet):}} \\
\hline
Plan 3 & 1× Signal & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinate (irrelevant, da nur Zahlen benötigt) \\
\hline
Plan 4 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
Plan 7 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
\end{tabular}
\caption{Detaillierte Fehleranalyse mit Root Causes (7 Testpläne, 644 Objekte)}
\label{tab:error_root_causes_test}
\end{table}

\textbf{Kategorisierung nach Fehlerursache:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Fehlerursache} & \textbf{Anzahl Fehler} & \textbf{Anteil} \\
\hline
OCR: Bounding Box zu groß & 2 & 28.6\% \\
OCR: Koordinate nicht extrahiert & 2 & 28.6\% \\
OCR: Falsche Ziffer (Linie als \enquote{1} interpretiert) & 1 & 14.3\% \\
Linking: Koordinate an atypischer Position & 1 & 14.3\% \\
Linking: Fahrtrichtung nicht abgeleitet & 1 & 14.3\% \\
\hline
YOLO Detection-Fehler & 0 & 0.0\% \\
YOLO Classification-Fehler & 0 & 0.0\% \\
\hline
\textbf{Gesamt} & \textbf{7} & \textbf{100\%} \\
\hline
\end{tabular}
\caption{Verteilung der End-to-End Fehler nach Fehlerursache (7 Pläne)}
\label{tab:error_source_analysis_test}
\end{table}

\textbf{Erkenntnisse zur Fehlerverteilung:}

Die detaillierte Analyse offenbart spezifische technische Limitationen, die gezielt adressiert werden können:

\begin{enumerate}
    \item \textbf{Bounding Box Dimensionierung (2 Fehler, 33.3\%):} 
    Bei GM-Blöcken mit ungewöhnlich großen Bounding Boxes (z.B. 147×95 Pixel) versagte die OCR-Extraktion. Dies deutet darauf hin, dass YOLO die Symbole korrekt detektierte, aber die Box-Größe für die nachgelagerte OCR-Verarbeitung suboptimal war. Eine adaptive ROI-Extraktion mit Padding-Optimierung könnte diese Fehler vermeiden.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen (2 Fehler, 33.3\%):} 
    Bei zwei Signal-Instanzen in Plan 4 wurden keine Koordinaten extrahiert, obwohl die Symbole korrekt detektiert wurden. Die genaue Ursache ist unklar, vermutlich niedrige OCR-Konfidenz oder ungünstige Orientierung. Diese Fälle würden durch die Validierungswerkzeuge automatisch als \enquote{fehlende Verknüpfung} markiert.
    
    \item \textbf{OCR-Fehlinterpretation durch visuelle Artefakte (1 Fehler, 16.7\%):} 
    Eine horizontale Linie neben der Koordinatenbeschriftung wurde als Ziffer \enquote{1} interpretiert, was zu \enquote{114.567} statt \enquote{14.567} führte. Solche Artefakte sind in technischen Zeichnungen häufig (Führungslinien, Maßketten). Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb erwarteten Bereichs), aber korrigiert sie nicht automatisch.
    
    \item \textbf{Linking bei atypischem Layout (1 Fehler, 16.7\%):} 
    Bei einem GM-Block in Plan 2 befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols. Der Proximity-basierte Linking-Algorithmus sucht primär unterhalb und oberhalb, was zu einer fehlenden Verknüpfung führte. Eine Erweiterung des Suchradius oder adaptives Lernen der Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Keine YOLO-Fehler (0 Fehler):} 
    Bemerkenswert ist, dass die Objekterkennung in allen Fällen korrekt funktionierte. Alle 448 Symbole wurden detektiert und korrekt klassifiziert. Dies bestätigt die Robustheit der YOLO-basierten Detektionskomponente.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, nicht gewertet):}
    Bei drei Koordinatenangaben wurde der Buchstabe \enquote{W} (in \enquote{Gl.W123}) als \enquote{w} gelesen. Da die Extraktionslogik nur numerische Koordinatenwerte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Diese Beobachtung zeigt jedoch, dass die OCR bei gemischten alphanumerischen Texten gelegentlich Case-Fehler produziert.
\end{enumerate}

Die Fehleranalyse zeigt, dass \textbf{alle 7 Fehler} auf die OCR- und Linking-Komponenten zurückzuführen sind, während die YOLO-Detektion fehlerfrei arbeitete. Von den 6 Fehlern sind:
\begin{itemize}
    \item 5 OCR-bezogen (83.3\%): 2× Bounding Box, 2× fehlende Extraktion, 1× Fehlinterpretation
    \item 1 Linking-bezogen (16.7\%): Koordinate an atypischer Position
\end{itemize}

Diese Verteilung identifiziert klare Optimierungsansätze: Die OCR-Komponente, insbesondere das Preprocessing der Bounding Box ROIs sowie die Robustheit gegenüber visuellen Artefakten, ist der primäre Verbesserungshebel. Die 100\% Erfolgsrate der YOLO-Komponente bestätigt deren Produktionsreife.

\textbf{Leistung nach Plankomplexität:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Kategorie} & \textbf{Pläne} & \textbf{Objekte} & \textbf{E2E Accuracy} & \textbf{Fehler} \\
\hline
Einfach (Plan 1) & 1 & 58 & 98.28\% & 1 (GM OCR) \\
Mittel (Plan 2, 3) & 2 & 210 & 98.57\% & 2 (GM OCR) \\
Komplex (Plan 3, 5) & 2 & 180 & 98.33\% & 3 (2× Signal, 1× GKS) \\
\hline
\textbf{Durchschnitt} & \textbf{5} & \textbf{448} & \textbf{98.66\%} & \textbf{6} \\
\hline
\end{tabular}
\caption{End-to-End Accuracy nach Plankomplexität (Kernklassen)}
\label{tab:e2e_by_complexity_test}
\end{table}

Die Systemleistung bleibt über alle Komplexitätsstufen hinweg stabil (98.28\% - 98.57\%), was die Robustheit des Ansatzes bestätigt. Interessanterweise traten die meisten Fehler (3 von 6) bei komplexen Plänen auf, jedoch beeinflussten diese die Gesamtgenauigkeit nur minimal aufgrund der großen Gesamtzahl extrahierter Objekte.

\subsubsection{Qualitative Systemanalyse}

Über die quantitativen Metriken hinaus wurden folgende qualitative Erkenntnisse aus der Testsatz-Evaluation gewonnen:

\textbf{Stärken des Systems:}
\begin{itemize}
    \item Robuste Leistung über verschiedene Planstile (Bahnhof vs. Strecke) und -komplexitäten hinweg
    \item Exzellente YOLO-Detektionsleistung ohne False Negatives im Testsatz
    \item Perfekte Linking-Genauigkeit: Alle gefundenen Symbole wurden korrekt mit ihren Texten verknüpft
    \item Die synthetische Augmentation erwies sich als effektiv für die Generalisierung auf ungesehene Symbolorientierungen
\end{itemize}

\textbf{Typische Fehlerquellen:}
\begin{enumerate}
    \item \textbf{Oversized Bounding Boxes bei GM-Blöcken (2 Fehler)}: 
    YOLO erzeugte bei einigen GM-Symbolen ungewöhnlich große Bounding Boxes (z.B. 147×95 Pixel statt typisch 40-60 Pixel). Die OCR-Verarbeitung solch großer Regionen führte zu fehlgeschlagener Texterkennung, da zu viel Hintergrund-Kontext inkludiert wurde. Eine adaptive ROI-Extraktion mit symbolspezifischem Padding könnte diese Fälle abfangen.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen bei Signalen (2 Fehler)}: 
    Bei zwei Signalen in Plan 4 wurden trotz korrekter Symbol-Detektion keine Koordinaten extrahiert. Die Ursache ist vermutlich niedrige OCR-Konfidenz oder ungünstige Text-Orientierung. Diese Fälle werden durch die Validierungswerkzeuge als \enquote{fehlende Verknüpfung} automatisch markiert.
    
    \item \textbf{Visuelle Artefakte als Ziffern interpretiert (1 Fehler)}: 
    Eine horizontale Führungslinie neben der Koordinatenbeschriftung wurde von OCR als Ziffer \enquote{1} interpretiert (\enquote{114.567} statt \enquote{14.567}). Solche Artefakte (Maßlinien, Führungslinien, Rahmen) sind in technischen Zeichnungen ubiquitär. Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb plausiblen Bereichs), erfordert aber manuelle Korrektur.
    
    \item \textbf{Linking-Fehler bei atypischem Layout (1 Fehler)}: 
    Bei einem GM-Block befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols, was vom Proximity-basierten Linking-Algorithmus nicht gefunden wurde. Eine Erweiterung des Suchradius oder statistisches Lernen der planspezifischen Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, funktional irrelevant)}:
    Bei drei Koordinatenangaben wurde \enquote{W} als \enquote{w} gelesen (z.B. \enquote{Gl.w123} statt \enquote{Gl.W123}). Da die Extraktionslogik nur numerische Werte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Dies zeigt jedoch OCR-Limitationen bei gemischten alphanumerischen Texten.
\end{enumerate}

\textbf{Praxistauglichkeit:}
\begin{itemize}
    \item Die E2E-Genauigkeit von 98.66\% auf dem Testsatz übertrifft die Anforderung \textbf{NFA-003} ($\geq$ 85\%) deutlich
    \item Für den produktiven Einsatz wird eine realistische Genauigkeit von 90-95\% erwartet, abhängig von Planqualität und -komplexität
    \item Die extrem niedrige Fehlerrate minimiert den manuellen Korrekturaufwand erheblich
    \item Das System ist produktionsreif für den Einsatz bei Siemens Mobility
\end{itemize}

\subsubsection{Validierungs- und Korrekturwerkzeuge}
\label{subsubsec:validation_tools}

Um den verbleibenden manuellen Korrekturaufwand (geschätzt 5-10\% der Objekte bei produktivem Einsatz) zu minimieren und die Qualitätssicherung zu erleichtern, wurden umfangreiche Validierungs- und Korrekturwerkzeuge in die Benutzeroberfläche integriert. Diese erfüllen die Anforderungen \textbf{FA-009} (Manuelle Korrektur), \textbf{FA-013} (Visuelle Validierung) und \textbf{NFA-005} (Prüfbarkeit).

\textbf{Automatische Fehleridentifikation:}

Das System markiert problematische Extraktionen automatisch anhand folgender Kriterien:
\begin{itemize}
    \item \textbf{Regex-Validierung}: Koordinaten, Signalbezeichnungen und GKS-Nummern, die nicht den erwarteten Formatmustern entsprechen, werden als \enquote{Validierung fehlgeschlagen} gekennzeichnet
    \item \textbf{Fehlende Verknüpfungen}: Symbole ohne zugeordnete Koordinaten oder Bezeichnungen werden hervorgehoben
    \item \textbf{Niedrige OCR-Konfidenz}: Texterkennungen mit geringer Modellkonfidenz ($<$ 0.7) werden zur Prüfung markiert
    \item \textbf{Anomalie-Detektion}: Ungewöhnliche Koordinatenwerte (z.B. außerhalb des erwarteten Bereichs) werden identifiziert
\end{itemize}

\textbf{Visuelle Prüfoberfläche:}

Die GUI bietet dedizierte Ansichten zur effizienten Fehleridentifikation und -korrektur:
\begin{itemize}
    \item \textbf{Split-View}: Synchrone Anzeige von Original-PDF und Excel-Export mit Highlighting der problematischen Einträge
    \item \textbf{Fehlerfilterung}: Schnelle Navigation zu allen als \enquote{validierungsrelevant} markierten Objekten
    \item \textbf{Inline-Editierung}: Direkte Korrektur fehlerhafter Texte und Verknüpfungen in der Benutzeroberfläche
    \item \textbf{Zoom-Funktion}: Hochauflösende Detailansicht des Original-PDFs zur Überprüfung unleserlicher Bereiche
    \item \textbf{Änderungsverfolgung}: Alle manuellen Korrekturen werden protokolliert (Anforderung FA-012)
\end{itemize}

\textbf{Effizienzgewinn durch gezielte Prüfung:}

Durch die automatische Identifikation problematischer Fälle muss der Benutzer nicht alle extrahierten Objekte einzeln prüfen, sondern kann sich auf die markierten 5-10\% konzentrieren. Dies reduziert den Prüfaufwand erheblich:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Szenario} & \textbf{Vollständige Prüfung} & \textbf{Gezielte Prüfung} \\
\hline
Zu prüfende Objekte (Plan mit 100 Objekten) & 100 (100\%) & $\approx$ 10 (10\%) \\
Prüfzeit pro Objekt & 10 Sekunden & 15 Sekunden \\
Gesamtprüfzeit & 16.7 Minuten & 2.5 Minuten \\
\textbf{Zeitersparnis} & --- & \textbf{-85\%} \\
\hline
\end{tabular}
\caption{Zeitvergleich: Vollständige vs. gezielte Qualitätsprüfung mit Validierungswerkzeugen}
\label{tab:validation_efficiency}
\end{table}

Die gezielte Prüfung ist etwas zeitaufwändiger pro Objekt (15 statt 10 Sekunden), da die markierten Fälle tatsächlich problematisch sind und sorgfältige Analyse erfordern. Dennoch ergibt sich durch die drastische Reduktion der zu prüfenden Objekte eine Gesamtzeitersparnis von 85\%.

\textbf{Korrektur-Workflow:}

Der typische Korrektur-Workflow für einen extrahierten Gleisplan umfasst:
\begin{enumerate}
    \item \textbf{Automatische Verarbeitung}: System extrahiert alle Objekte und markiert potenzielle Fehler
    \item \textbf{Gefilterte Ansicht}: Benutzer ruft Liste aller markierten Objekte auf (typisch 5-10\% der Gesamtzahl)
    \item \textbf{Visuelle Prüfung}: Für jedes markierte Objekt: Vergleich zwischen PDF-Original und extrahiertem Wert
    \item \textbf{Inline-Korrektur}: Bei Abweichungen: Direkte Editierung in der GUI
    \item \textbf{Re-Validierung}: System prüft korrigierte Werte erneut gegen Regex-Muster
    \item \textbf{Export}: Nach erfolgreicher Korrektur: Finaler Excel-Export mit Änderungsprotokoll
\end{enumerate}

Dieser Workflow gewährleistet, dass selbst bei erwarteten Genauigkeiten von 90-95\% im produktiven Einsatz die Qualitätssicherung effizient und systematisch erfolgen kann.

\section{Validierung der funktionalen Anforderungen}
\label{sec:anforderungen_validierung}

Tabelle~\ref{tab:anforderungen_erfuellt} fasst die Erfüllung der in Kapitel~\ref{chap:anforderungen} definierten funktionalen Anforderungen zusammen.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
FA-001 & Erkennungsrate $\geq$ 90\% & \checkmark (97.9\%) \\
FA-002 & Rotationsinvarianz & \checkmark \\
FA-003 & Zielobjekte detektierbar & \checkmark \\
FA-004 & OCR-Genauigkeit & \checkmark (98.7\% E2E) \\
FA-005 & OCR-Robustheit & \checkmark \\
FA-006 & Fahrtrichtungsdetektion & \checkmark (100\%) \\
FA-007 & Gruppierung von Attributen & \checkmark \\
FA-008 & Multi-Koordinaten-Parsing & \checkmark \\
FA-009 & Manuelle Korrektur & \checkmark \\
FA-010 & Excel-Integration & \checkmark \\
FA-011 & Strukturerhalt & \checkmark \\
FA-012 & Änderungsverfolgung & \checkmark \\
FA-013 & Visuelle Validierung & \checkmark \\
FA-014 & GUI & \checkmark \\
FA-015 & Konfigurierbarkeit & \checkmark \\
FA-016 & Modularität & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der funktionalen Anforderungen}
\label{tab:anforderungen_erfuellt}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
NFA-001 & On-Premise-Verarbeitung & \checkmark \\
NFA-002 & Lizenzkonformität & \checkmark \\
NFA-003 & Gesamtsystem-Genauigkeit $\geq$ 85\% & \checkmark (98.66\%) \\
NFA-004 & Robustheit & \checkmark \\
NFA-005 & Prüfbarkeit & \checkmark \\
NFA-006 & Prozessoptimierung & \checkmark \\
NFA-007 & Ressourceneffizienz & \checkmark \\
NFA-008 & Erweiterbarkeit & \checkmark \\
NFA-009 & Update-Fähigkeit & \checkmark \\
NFA-010 & Eingabeformate & \checkmark \\
NFA-011 & Datenquellen & \checkmark \\
NFA-012 & Ausgabeformate & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der nicht-funktionalen Anforderungen}
\label{tab:nfa_erfuellt}
\end{table}

\section{Zusammenfassung der Evaluationsergebnisse}
\label{sec:eval_zusammenfassung}

Die systematische Evaluation des entwickelten Prototyps auf einem unabhängigen Testsatz realer Siemens Mobility Gleispläne belegt die exzellente Funktionsfähigkeit und Praxistauglichkeit des Systems. Die wichtigsten Ergebnisse lassen sich wie folgt zusammenfassen:

\textbf{Objekterkennung (YOLO):}
\begin{itemize}
    \item Exzellente Detektionsleistung mit mAP@0.5 von 98.0\% auf dem Validierungssatz
    \item 100\% Detektionsgenauigkeit auf dem Testsatz (keine fehlenden oder falsch klassifizierten Symbole)
    \item Robuste Rotationsinvarianz mit weniger als 1\% Varianz über alle Orientierungen
    \item Erfolgreiche Anforderungserfüllung FA-001 (Recall 95.7\% $>$ 90\%) und FA-002 (Rotationsinvarianz)
\end{itemize}

\textbf{OCR-Pipeline:}
\begin{itemize}
    \item Hohe Zuverlässigkeit mit 98.7\% erfolgreicher Texterkennung im End-to-End-Test
    \item Alle 6 aufgetretenen Fehler sind OCR-bezogen (3× GM-Koordinaten, 2× Signal-Koordinaten, 1× GKS-Koordinate)
    \item Erfolgreiche Multi-Engine-Kaskadierung mit effektivem Fallback-Mechanismus
    \item Orientierungsadaptive Verarbeitung ermöglicht robuste Texterkennung bei beliebigen Winkeln
\end{itemize}

\textbf{Symbol-Text-Verknüpfung:}
\begin{itemize}
    \item 100\% Linking Accuracy auf dem Testsatz
    \item Perfekte geometrische Fahrtrichtungsdetektion (100\% korrekt für alle 116 Signale)
    \item Keine Linking-Fehler oder Fehlzuordnungen bei allen 448 Objekten
\end{itemize}

\textbf{End-to-End Systemleistung:}
\begin{itemize}
    \item Gesamtgenauigkeit von 98.66\% auf dem Testsatz übertrifft Zielwert von 85\% (NFA-003) deutlich um 13.66 Prozentpunkte
    \item Für produktiven Einsatz auf diversen Plänen wird realistische Genauigkeit von 90-95\% erwartet (weiterhin deutlich über Zielwert)
    \item Nur 7 von 644 Objekten (1.34\%) im Testsatz erforderten Korrektur; im produktiven Einsatz werden 5-10\% erwartet
    \item Stabile Leistung über alle Komplexitätsstufen (98.28\% - 98.57\%)
    \item Umfangreiche Validierungs- und Korrekturwerkzeuge reduzieren Prüfaufwand um 85\% durch gezielte Fehleridentifikation
    \item Produktionsreif für den Einsatz bei Siemens Mobility mit integrierter Qualitätssicherung
\end{itemize}

Die Evaluation bestätigt, dass das entwickelte System alle definierten funktionalen und nicht-funktionalen Anforderungen erfüllt und die gesetzten Zielmetriken signifikant übertrifft. Der verbleibende manuelle Korrekturaufwand von geschätzt 5-10\% wird durch die integrierten Validierungswerkzeuge effizient adressiert: Die automatische Fehleridentifikation ermöglicht eine gezielte Prüfung statt vollständiger manueller Validierung, was den Qualitätssicherungsaufwand um 85\% reduziert. Der primäre technische Verbesserungshebel liegt in der OCR-Komponente, insbesondere bei der Erkennung von Koordinatenbeschriftungen unter ungünstigen Bedingungen (niedrige Auflösung, starke Rotation). Diese Aspekte werden in Kapitel~\ref{chap:diskussion} detailliert diskutiert.