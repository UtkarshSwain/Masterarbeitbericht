\chapter{Evaluation}
\label{chap:evaluation}

Dieses Kapitel präsentiert die systematische Evaluation des entwickelten Prototyps. Die Bewertung erfolgt anhand definierter Metriken und validiert die in Kapitel~\ref{chap:anforderungen} spezifizierten funktionalen und nicht-funktionalen Anforderungen. Die Evaluation gliedert sich in die Beschreibung der Testmethodik, die detaillierte Analyse der Einzelkomponenten sowie die Bewertung der Gesamtsystemleistung auf einem unabhängigen Testsatz.

\section{Testmethodik}
\label{sec:testmethodik}

Die Evaluation des Systems erfordert eine sorgfältige Definition der Testbedingungen, um reproduzierbare und aussagekräftige Ergebnisse zu gewährleisten. Dieser Abschnitt beschreibt die verwendeten Testdatensätze, die angewandten Evaluationsmetriken sowie die Testumgebung.

\subsection{Testdatensätze}
\label{subsec:testdatensatz}

Zur systematischen Bewertung wurden zwei hierarchisch strukturierte Datensätze verwendet, die verschiedene Evaluationszwecke erfüllen.

\subsubsection{Validierungssatz (YOLO Technical Validation)}

Der Validierungssatz dient der technischen Bewertung der Objekterkennungskomponente und wurde während des Trainingsprozesses zur Modellselektion verwendet.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Original-Gleispläne (A0-Format) & 25 \\
Anzahl Tiles (2048x2048, überlappend) & 1.154 \\
\hspace{3mm} davon Trainingsdaten & 923 \\
\hspace{3mm} davon Validierungsdaten (initial) & 208 \\
\hspace{3mm} davon finale Validierungsdaten & 115 \\
Annotierte Symbole (Validierungsset) & 1.305 \\
\hline
\multicolumn{2}{|l|}{\textit{Kernklassen (produktionsrelevant)}} \\
\hline
\hspace{3mm} Signale & 218 \\
\hspace{3mm} Koordinaten & 629 \\
\hspace{3mm} GKS-Platten (festkodiert) & 60 \\
\hspace{3mm} GKS-Platten (gesteuert) & 69 \\
\hspace{3mm} GM-Blöcke & 91 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{1.067 (81.8\%)} \\
\hline
\multicolumn{2}{|l|}{\textit{Auxiliarklassen (experimentell)}} \\
\hline
\hspace{3mm} Sonstige Klassen (8 Typen) & 238 (18.2\%) \\
\hline
Objektklassen (gesamt) & 13 \\
\hspace{3mm} davon Kernklassen & 5 \\
\hspace{3mm} davon Auxiliarklassen & 8 \\
Synthetische Augmentation (Training) & 10 Rotationswinkel \\
Gesamtinstanzen (inkl. Augmentation) & 8.029 \\
Tile-Größe & 2048 × 2048 Pixel \\
Auflösungsbereich (Original-PDFs) & 500 DPI \\
\hline
\end{tabular}
\caption{Statistiken des Validierungsdatensatzes mit Unterscheidung zwischen Kern- und Auxiliarklassen}
\label{tab:validation_dataset_stats}
\end{table}

\textbf{Anmerkung zur Datenfilterung:} Von den initial 208 durch YOLOv8s automatische 80/20-Aufteilung zugewiesenen Validierungsbildern wurden 93 Bilder manuell gefiltert, da sie entweder keine relevanten Symbole enthielten (leere Randbereiche nach dem Tiling) oder ausschließlich Hintergrundgeometrie ohne annotierte Objekte darstellten. Dies resultiert in einem finalen Validierungsdatensatz von 115 Bildern mit 1.305 validen Objektinstanzen.

\textbf{Verwendungszweck:} Der Validierungssatz dient primär der Bewertung der YOLO-Detektionsleistung (mAP, Precision, Recall) und bestätigt die erfolgreiche Modellkonvergenz. Diese Daten wurden während des Trainings zur Hyperparameter-Optimierung und Early Stopping verwendet und stellen keine vollständig unabhängige Testmenge dar.

\subsubsection{Testsatz (End-to-End System Evaluation)}

Für die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) wurde ein Testsatz aus realen Siemens Mobility Gleisplänen ausgewählt. 

\textbf{Datensatztrennung:} Die sieben Testpläne wurden zufällig aus einem separaten Datenpool ausgewählt, der zu keinem Zeitpunkt für Training oder Validierung des YOLO-Modells verwendet wurde. Diese strikte Trennung gewährleistet eine vollständig unabhängige Evaluation der Systemleistung.

\textbf{Methodische Einschränkung:} Aufgrund des erheblichen manuellen Aufwands zur Erstellung vollständiger Ground-Truth-Daten für A0-Gleispläne (durchschnittlich 2-3 Stunden pro Plan) sowie der begrenzten Verfügbarkeit weiterer ungesehener Pläne wurden sieben Pläne unterschiedlicher Komplexität für die End-to-End-Evaluation ausgewählt. Diese Pläne repräsentieren verschiedene Komplexitätsstufen und ermöglichen eine realistische Bewertung der Systemleistung. Für eine vollständig unabhängige Evaluation wären zusätzliche, komplett ungesehene Pläne wünschenswert gewesen, was im Zeitrahmen dieser Masterarbeit jedoch nicht realisierbar war.

\textbf{Evaluationsumfang:} Die End-to-End-Evaluation fokussiert sich auf die vier Ankersymbol-Klassen (\textit{signal}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die Klasse \textit{coordinate} wurde vollständig annotiert und trainiert, um Datenhomogenität zu gewährleisten; in der E2E-Evaluation werden Koordinaten jedoch nur als verknüpfte Attribute der Ankersymbole bewertet -- d.h. eine Koordinate gilt als korrekt, wenn sie dem richtigen Signal oder der richtigen GKS-Platte zugeordnet wurde. Die acht zusätzlich implementierten Auxiliarklassen dienten primär der Demonstration der Systemerweiterbarkeit; deren Koordinatenverknüpfung ist experimentell und wird nicht detailliert evaluiert.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Gleispläne & 7 \\
Seitengröße & A0 (841 × 1189 mm) \\
Seiten pro Plan & 1 \\
Herkunft & Siemens Mobility Projekte \\
Komplexitätsstufen & Einfach (2), Mittel (2), Komplex (3) \\
\hline
\multicolumn{2}{|l|}{\textit{Durchschnittliche Symbolanzahl pro Plan}} \\
\hline
\hspace{3mm} Signale & 25 \\
\hspace{3mm} GKS-Platten (beide Typen) & 37 \\
\hspace{3mm} GM-Blöcke & 31 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{92} \\
Gesamtanzahl evaluierter Objekte & 644 \\
Tiles pro Plan (Durchschnitt) & $\approx$ 40 \\
Auflösungsbereich & 500 DPI \\
\hline
\end{tabular}
\caption{Charakteristika des Testdatensatzes (A0-Gleispläne, nur Kernklassen)}
\label{tab:test_dataset_stats}
\end{table}

\textbf{Ground-Truth-Erstellung:} Für jeden Testplan wurde eine manuelle Referenzdatei erstellt, die alle relevanten Extraktionsziele enthält:

\begin{itemize}
    \item \textbf{Signale}: Bezeichnung (z.B. ``A102''), zugehörige Kilometrierung, Fahrtrichtung (A/B)
    \item \textbf{GKS-Platten}: Nummer (z.B. ``1234''), zugehörige Kilometrierung
    \item \textbf{Koordinatenangaben}: Kilometerwert (z.B. ``18.1606''), optionale Gleisangabe
    \item \textbf{Verknüpfungen}: Erwartete Assoziationen zwischen Symbolen und Texten
\end{itemize}

Die Ground-Truth-Daten wurden in strukturierten Excel-Dateien gespeichert, um einen direkten Vergleich mit den Systemausgaben zu ermöglichen.

\textbf{Verwendungszweck:} Der Testsatz dient der vollständigen End-to-End-Evaluation aller Pipeline-Komponenten und misst die tatsächliche Systemleistung auf realen Daten aus dem Siemens Mobility Umfeld.

\textit{Hinweis: Aus Vertraulichkeitsgründen (Sperrvermerk) werden keine spezifischen Projektbezeichnungen oder Visualisierungen der Originalpläne präsentiert. Die Ergebnisse werden in aggregierter Form berichtet.}

\subsubsection{Komplexitätskategorisierung}

Um die Robustheit des Systems unter verschiedenen Bedingungen zu evaluieren, wurden die Testpläne in drei Komplexitätskategorien eingeteilt. Da alle Pläne im A0-Format vorliegen und jeweils eine Seite umfassen, erfolgt die Kategorisierung primär nach Symboldichte und Layoutkomplexität.

\begin{table}[H]
\centering
\begin{tabular}{|l|p{8cm}|r|}
\hline
\textbf{Kategorie} & \textbf{Charakteristik} & \textbf{Anzahl} \\
\hline
Einfach & Niedrige Symboldichte ($<$ 60 Symbole), klare räumliche Trennung, typisch für Streckenabschnitte & 2 \\
\hline
Mittel & Moderate Symboldichte (60--100 Symbole), gelegentliche Überlappungen, typisch für kleinere Bahnhöfe & 2 \\
\hline
Komplex & Hohe Symboldichte ($>$ 100 Symbole), viele überlappende Elemente, dichte Weichenbereiche, typisch für große Bahnhofsköpfe & 3 \\
\hline
\end{tabular}
\caption{Komplexitätskategorien der A0-Testpläne}
\label{tab:complexity_categories}
\end{table}

Diese Kategorisierung ermöglicht eine differenzierte Analyse der Systemleistung in Abhängigkeit von der Plankomplexität und identifiziert kritische Schwellenwerte für Symboldichte und räumliche Überlappung.


\subsection{Evaluationsmetriken}
\label{subsec:evaluationsmetriken}

Die Bewertung des Systems erfolgt auf mehreren Ebenen mit jeweils spezifischen Metriken. Die verwendeten Metriken basieren auf den in Kapitel~\ref{chap:theoretischeundtechnischegrundlagen} eingeführten Standardverfahren für Objekterkennung (Abschnitt~\ref{subsec:theoretischeevaluationsmetriken}) und OCR-Systeme (Abschnitt~\ref{subsec:ocr_metriken}). Dieser Abschnitt fasst die angewandten Metriken kurz zusammen und definiert die spezifische End-to-End Systemmetrik.

\subsubsection{Metriken für die Objekterkennung}

Für die Bewertung der YOLO-basierten Objekterkennung werden die in Abschnitt~\ref{subsec:evaluationsmetriken} definierten Standardmetriken verwendet:

\begin{itemize}
    \item \textbf{Precision}: Anteil korrekter Detektionen an allen Vorhersagen
    \item \textbf{Recall}: Anteil gefundener Objekte an allen vorhandenen Objekten
    \item \textbf{F1-Score}: Harmonisches Mittel aus Precision und Recall
    \item \textbf{mAP@0.5}: Mean Average Precision bei IoU-Schwelle von 50\%
    \item \textbf{mAP@0.5:0.95}: mAP gemittelt über IoU-Schwellen von 50\% bis 95\%
\end{itemize}

Eine Detektion gilt als \textit{True Positive}, wenn die Intersection over Union (IoU) mit der Ground-Truth-Box $\geq$ 0.5 beträgt und die Klassenvorhersage korrekt ist.

\subsubsection{Metriken für die Texterkennung}

Die OCR-Leistung wird nicht isoliert durch zeichenbasierte Metriken wie die 
Character Error Rate (CER) bewertet, sondern nach dem Prinzip der 
\textit{Feldgenauigkeit} (vgl. Abschnitt~\ref{subsec:ocr_metriken}): Ein 
OCR-Ergebnis gilt als korrekt, wenn der extrahierte Text exakt mit dem 
Ground Truth übereinstimmt. Diese Bewertung ist in die End-to-End-Systemmetrik 
integriert, wodurch folgende Vorteile entstehen:

\begin{itemize}
    \item OCR-Fehler, die durch nachgelagerte Validierung (Regex-Muster) 
    automatisch korrigiert werden, beeinflussen das Endergebnis nicht negativ
    \item Die Metrik entspricht dem tatsächlichen Informationsbedarf: 
    „Wurde der korrekte Wert extrahiert? \enquote{statt} Wie viele Zeichen waren falsch?"
    \item Fehlerquellen können der jeweiligen Pipeline-Stufe zugeordnet werden 
    (YOLO vs. OCR vs. Linking)
\end{itemize}

Zusätzlich wird die \textbf{Regex-Validierungsrate} erfasst, die den Anteil 
der OCR-Ergebnisse quantifiziert, die klassenspezifische Formatmuster erfüllen.


\subsubsection{End-to-End Systemmetrik}

Die Gesamtsystemleistung wird durch die \textbf{End-to-End Accuracy} gemessen, die dem in Anforderung \textbf{NFA-003} definierten Zielwert entspricht:
\begin{equation}
\text{E2E Accuracy} = \frac{\text{Vollständig korrekt extrahierte Objekte}}{\text{Gesamtanzahl Objekte}} \times 100\%
\end{equation}

Ein Objekt gilt als \enquote{vollständig korrekt extrahiert}, wenn alle folgenden Bedingungen erfüllt sind:
\begin{enumerate}
    \item Das Symbol wurde korrekt detektiert (IoU $\geq$ 0.5 mit Ground Truth)
    \item Die Klassifikation ist korrekt
    \item Der OCR-Text stimmt exakt mit dem Ground Truth überein (falls anwendbar)
    \item Alle erforderlichen Verknüpfungen (z.B. zu Koordinaten) sind korrekt (falls anwendbar)
\end{enumerate}

\subsection{Testumgebung}
\label{subsec:testumgebung}

Die Evaluation wurde auf einer standardisierten Hardware- und Softwarekonfiguration durchgeführt, um reproduzierbare Ergebnisse zu gewährleisten. Tabelle~\ref{tab:testumgebung} fasst die technischen Spezifikationen zusammen.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Komponente} & \textbf{Spezifikation} \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Inferenz)}} \\
\hline
CPU & AMD Ryzen 5 PRO 5650U (6 Kerne, 2.3 GHz) \\
RAM & 32 GB DDR4 \\
GPU & Keine (CPU-only Inferenz) \\
Speicher & 512 GB SSD \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Training)}} \\
\hline
GPU & NVIDIA T4 (AWS g4dn.xlarge) \\
VRAM & 16 GB \\
\hline
\multicolumn{2}{|c|}{\textit{Software}} \\
\hline
Betriebssystem & Windows 10 / Ubuntu 22.04 \\
Python & 3.9.16 \\
PyTorch & 2.0.1 \\
Ultralytics & 8.0.196 \\
PaddleOCR & 2.7.0 \\
Tesseract & 5.3.0 \\
PostgreSQL & 14.9 \\
\hline
\end{tabular}
\caption{Hardware- und Softwarekonfiguration der Testumgebung}
\label{tab:testumgebung}
\end{table}

Die Wahl einer CPU-basierten Inferenzumgebung reflektiert die Anforderung \textbf{NFA-001}, die eine On-Premise-Verarbeitung auf Standard-Workstations ohne dedizierte GPU vorsieht. Alle Zeitmessungen wurden als Mittelwert über drei Durchläufe berechnet, um Varianz durch Systemlast zu minimieren.

\section{Ergebnisanalyse}
\label{sec:ergebnisanalyse}

Dieser Abschnitt präsentiert die quantitativen Evaluationsergebnisse der einzelnen Pipeline-Komponenten sowie des Gesamtsystems. Die Ergebnisse werden im Kontext der in Kapitel~\ref{chap:anforderungen} definierten Anforderungen interpretiert.

\subsection{Objekterkennungsleistung}
\label{subsec:detection_eval}

Die Objekterkennung bildet die fundamentale Stufe der Extraktionspipeline. Die Qualität der YOLO-Detektionen determiniert maßgeblich die erreichbare Gesamtgenauigkeit des Systems.

\subsubsection{Gesamtleistung auf dem Validierungssatz}

Das trainierte YOLOv8l-OBB Modell wurde auf dem Validierungsdatensatz (115 Bilder, 1.305 Instanzen) evaluiert. Tabelle~\ref{tab:detection_overall} zeigt die aggregierten Metriken über alle Symbolklassen.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Metrik} & \textbf{Wert} \\
\hline
Precision (Durchschnitt) & 97.5\% \\
Recall (Durchschnitt) & 95.7\% \\
F1-Score & 96.6\% \\
mAP@0.5 & 98.0\% \\
mAP@0.5:0.95 & 92.2\% \\
\hline
\end{tabular}
\caption{Aggregierte Detektionsmetriken auf dem Validierungsdatensatz}
\label{tab:detection_overall}
\end{table}

Der erreichte Recall von 95.7\% übertrifft die Anforderung \textbf{FA-001} deutlich, die eine Mindesterkennungsrate von 90\% fordert. Die hohe Precision von 97.5\% zeigt, dass das Modell nur wenige Falschdetektionen produziert -- von 100 vorhergesagten Objekten sind durchschnittlich 97-98 korrekt. Der F1-Score von 96.6\% belegt die ausgewogene Leistung zwischen Precision und Recall, was für produktive Anwendungen essentiell ist: Das System findet nahezu alle vorhandenen Objekte (hoher Recall) und produziert dabei nur wenige Fehlalarme (hohe Precision).

Die mAP@0.5 von 98.0\% demonstriert die exzellente Detektionsqualität bei einem IoU-Schwellenwert von 50\%. Dies bedeutet, dass die vorhergesagten Bounding Boxes im Durchschnitt zu mindestens 50\% mit den Ground-Truth-Boxen überlappen, was für nachgelagerte OCR-Verarbeitung ausreichend präzise ist. Die mAP@0.5:0.95 von 92.2\% bestätigt die robuste Leistung auch bei strengeren Überlappungskriterien (IoU von 50\% bis 95\% in 5\%-Schritten gemittelt). Der Abstand von 5.8 Prozentpunkten zwischen mAP@0.5 und mAP@0.5:0.95 ist für orientierte Bounding Boxes typisch und liegt im erwarteten Bereich.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/Kapitel7/BoxPR_curve.png}
    \caption{Precision-Recall-Kurven für alle Objektklassen auf dem Validierungsdatensatz. Die Fläche unter jeder Kurve entspricht dem klassenspezifischen AP-Wert. Der Gesamtwert mAP@0.5 beträgt 98,0\,\%.}
    \label{fig:pr_curve}
\end{figure}

Die Precision-Recall-Kurven in Abbildung~\ref{fig:pr_curve} visualisieren das Verhältnis zwischen Precision und Recall für verschiedene Konfidenzschwellenwerte. Die nahezu rechteckige Form der Kurven für die meisten Klassen bestätigt die hohe Detektionsqualität. Lediglich bei den GKS-Klassen zeigt sich ein geringfügig früherer Precision-Abfall bei hohen Recall-Werten, was auf die in Abschnitt~\ref{subsec:e2e_test_eval} diskutierten Verwechslungen zwischen \texttt{gks\_festkodiert} und \texttt{gks\_gesteuert} zurückzuführen ist.
\subsubsection{Klassenspezifische Analyse}

Die Detektionsleistung variiert zwischen den verschiedenen Symbolklassen moderat. Tabelle~\ref{tab:detection_per_class} zeigt die klassenspezifischen Metriken für die fünf Kernklassen, die für die Extraktion von Planungsdaten relevant sind. Precision und Recall wurden basierend auf den Werten der Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) berechnet, während die mAP@0.5-Werte die durchschnittliche Precision über alle Recall-Stufen repräsentieren.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Klasse} & \textbf{Instanzen (Val)} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} \\
\hline
signal & 218 & 94.9\% & 98.5\% & 97.6\% \\
coordinate & 629 & 97.7\% & 98.6\% & 98.7\% \\
gks\_festkodiert & 60 & 98.3\% & 95.8\% & 97.8\% \\
gks\_gesteuert & 69 & 94.5\% & 97.5\% & 95.1\% \\
gm\_block & 91 & 97.6\% & 97.6\% & 99.5\% \\
\hline
\textbf{Kernklassen (Durchschnitt)} & \textbf{1.067 (81.8\%)} & \textbf{96.9\%} & \textbf{98.3\%} & \textbf{97.7\%} \\
\hline
\end{tabular}
\caption{Detektionsmetriken für die fünf Kernklassen auf dem Validierungsdatensatz. Precision und Recall wurden aus der Konfusionsmatrix berechnet, mAP@0.5 aus den Precision-Recall-Kurven.}
\label{tab:detection_per_class}
\end{table}
\textbf{Validierung FA-001, FA-002 und FA-003:}

\textbf{FA-001 (Erkennungsrate $\geq$ 90\%):} Der erreichte Recall von 95.7\% auf dem 
Validierungssatz und 100\% auf dem Testsatz (kein Symbol übersehen) übertrifft die 
geforderte Mindesterkennungsrate deutlich.

\textbf{FA-002 (Rotationsinvarianz):} Die OBB-Architektur in Kombination mit der 
synthetischen Rotationsaugmentation (10 Winkel von $-90^\circ$ bis $+90^\circ$, vgl. 
§\ref{subsec:Modelltraining}) gewährleistet robuste Erkennung bei beliebigen 
Symbolorientierungen. Die quantitative Analyse in Tabelle~\ref{tab:rotation_analysis} 
bestätigt dies: 38 Objekte mit Steilrotation ($|\theta| > 30^\circ$) erreichen eine 
höhere Detektionskonfidenz (0.946) als kardinal orientierte Objekte (0.890).

\textbf{FA-003 (Zielobjekte):} Alle 5 Kernklassen (signal, coordinate, gks\_festkodiert, 
gks\_gesteuert, gm\_block) werden zuverlässig detektiert. Die zusätzliche Integration 
von 8 Auxiliarklassen demonstriert die Erweiterbarkeit des Systems (NFA-008).

\textbf{Hinweis zu weiteren Symbolklassen:} Das Modell wurde zusätzlich auf acht weitere Klassen trainiert (\textit{weichen\_block}, \textit{haltepunkt}, \textit{isolierstoß}, \textit{sverbinder}, \textit{prellblock}, \textit{haltetafel}, \textit{endeweichen}, \textit{weichengruppeende}), die zu experimentellen Zwecken annotiert wurden. Diese Klassen erreichen ebenfalls hohe Erkennungsraten (Durchschnitt mAP@0.5: 98.8\%), werden jedoch in der nachfolgenden End-to-End-Evaluation nicht berücksichtigt, da sie nicht zur primären Extraktionsaufgabe gehören.
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Kapitel7/confusion_matrix.png}
\caption{Konfusionsmatrix der YOLO-Klassifikation (Validierungsset). 
Die Matrix zeigt, dass 4 Instanzen von \textit{gks\_gesteuert} 
fälschlicherweise als \textit{gks\_festkodiert} klassifiziert wurden.}
\label{fig:confusion_matrix}
\end{figure}
Die Analyse der klassenspezifischen Ergebnisse offenbart mehrere bedeutende Beobachtungen:

\begin{itemize}
    \item \textbf{Konsistent hohe Performance der Kernklassen}: Alle fünf Kernklassen erreichen sowohl Precision als auch Recall über 94.5\%, was die Robustheit des Modells für die produktionsrelevanten Symboltypen belegt. Der durchschnittliche Recall von 98.3\% für Kernklassen übertrifft sogar den Gesamt-Recall von 95.7\%, was zeigt, dass die wichtigsten Objekttypen besonders zuverlässig erkannt werden.
    
    \item \textbf{Klassen-spezifische Precision-Recall-Profile}: Die Klasse \textit{signal} zeigt einen interessanten Trade-off mit niedrigerer Precision (94.9\%) aber höherem Recall (98.5\%). Dies ist für sicherheitskritische Signalerkennung vorteilhaft: Lieber einige Falschdetektionen in Kauf nehmen, als ein tatsächliches Signal zu übersehen. Im Gegensatz dazu erreicht \textit{gks\_festkodiert} die höchste Precision (98.3\%) bei etwas niedrigerem Recall (95.8\%), was die distinktive visuelle Erscheinung dieser Symbolklasse reflektiert.
    
    \item \textbf{Perfekt balancierte Klassen}: Die Klassen \textit{gm\_block} (97.6\% P/R) und \textit{coordinate} (97.7\% P, 98.6\% R) zeigen nahezu identische Precision- und Recall-Werte, was auf eine optimale Detektionsqualität ohne systematische Bias hinweist. Bei \textit{coordinate} ist dies besonders bemerkenswert, da diese Klasse mit 629 Instanzen (58.9\% der Kernklassen) die am häufigsten vorkommende ist.
    
    \item \textbf{Herausforderung bei GKS-Varianten}: Die Klasse \textit{gks\_gesteuert} zeigt mit 94.5\% die niedrigste Precision unter den Kernklassen, was auf die hohe visuelle Ähnlichkeit zu \textit{gks\_festkodiert} zurückzuführen ist. Die Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) zeigt, dass 4 Instanzen von \textit{gks\_gesteuert} fälschlicherweise als \textit{gks\_festkodiert} klassifiziert wurden, während 1 Instanz von \textit{gks\_festkodiert} als \textit{gks\_gesteuert} fehlklassifiziert wurde. Diese beiden GKS-Plattentypen unterscheiden sich nur durch interne Symboldetails (fest kodierte vs. programmierbarer Code), was die Unterscheidung für das neuronale Netz erschwert.
    
    \item \textbf{mAP als synthetische Metrik}: Die mAP@0.5-Werte liegen durchweg 1-3 Prozentpunkte über den jeweiligen Precision-Werten, da mAP die durchschnittliche Precision über alle Recall-Stufen misst. Klassen mit nahezu rechteckigen Precision-Recall-Kurven (siehe Abbildung~\ref{fig:pr_curve}) wie \textit{gm\_block} (99.5\% mAP) zeigen, dass hohe Precision auch bei variierenden Konfidenzschwellen erhalten bleibt.
    
    \item \textbf{Auxiliarklassen demonstrieren Erweiterbarkeit}: Die acht experimentellen Klassen mit durchschnittlich 98.8\% mAP belegen, dass das System prinzipiell auf weitere Symboltypen erweiterbar ist. Die erfolgreiche Erkennung seltener Klassen wie \textit{endeweichen} (nur 4 Trainingsinstanzen, 99.5\% mAP) demonstriert die Effektivität der synthetischen Rotationsaugmentation, die die Datenmenge für jede Klasse um Faktor 10-150 erhöht hat.
\end{itemize}

\subsection{End-to-End Systemevaluation auf dem Testsatz}
\label{subsec:e2e_test_eval}

Die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) erfolgt auf dem unabhängigen Testsatz realer Siemens Mobility Gleispläne. Diese Evaluation misst die tatsächliche Systemleistung unter realistischen Bedingungen und validiert die Anforderungen \textbf{FA-004} bis \textbf{FA-014} sowie \textbf{NFA-003} bis \textbf{NFA-007}.

\subsubsection{End-to-End Systemgenauigkeit}

Die Gesamtsystemleistung wird durch die End-to-End Accuracy gemessen, die alle Pipeline-Stufen integriert und der in Anforderung \textbf{NFA-003} definierten Zielmetrik entspricht. Die Evaluation fokussiert sich auf die fünf Kernklassen, die für den produktiven Einsatz bei Siemens Mobility essentiell sind.

\textbf{Aggregierte Ergebnisse über alle Testpläne:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Objektklasse} & \textbf{Gesamt} & \textbf{Korrekt} & \textbf{Genauigkeit} \\
\hline
signal (Name + Position + Richtung) & 172 & 169 & 98.26\% \\
gks (Nummer + Position, beide Typen) & 256 & 254 & 99.22\% \\
gm\_block (Position) & 216 & 213 & 98.61\% \\
\hline
\textbf{Durchschnitt (Kernklassen)} & \textbf{644} & \textbf{636} & \textbf{98.76\%} \\
\hline
\end{tabular}
\caption{End-to-End Genauigkeit pro Objektklasse (7 Testpläne)}
\label{tab:e2e_per_class_test}
\end{table}
\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Metrik} & \textbf{Zielwert (NFA-003)} & \textbf{Erreicht} & \textbf{Status} \\
\hline
Vollständig korrekt extrahiert (Kernklassen) & $\geq$ 85\% & 98.76\% & \checkmark \\
Manuelle Korrektur erforderlich & $\leq$ 15\% & 1.24\% & \checkmark \\
\hline
\end{tabular}
\caption{End-to-End Systemgenauigkeit für Kernklassen im Vergleich zum Anforderungsziel}
\label{tab:e2e_overall_test}
\end{table}
Da Signale das komplexeste Extraktionsziel darstellen (drei Attribute: Name, Koordinate, 
Fahrtrichtung), wird deren Genauigkeit in Tabelle~\ref{tab:signal_attribute_accuracy} 
detailliert aufgeschlüsselt.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Attribut} & \textbf{Gesamt} & \textbf{Korrekt} & \textbf{Genauigkeit} \\
\hline
Signalname (OCR) & 172 & 170 & 98.84\% \\
Koordinate (OCR + Linking) & 172 & 170 & 98.84\% \\
Fahrtrichtung (Geometrische Ableitung) & 172 & 171 & 99.42\% \\
\hline
\textbf{Vollständig korrekt (alle 3 Attribute)} & \textbf{172} & \textbf{169} & \textbf{98.26\%} \\
\hline
\end{tabular}
\caption{Attribut-spezifische Genauigkeit für Signale (7 Testpläne). Ein Signal gilt als 
vollständig korrekt, wenn alle drei Attribute fehlerfrei extrahiert wurden.}
\label{tab:signal_attribute_accuracy}
\end{table}

Die hohe Fahrtrichtungsgenauigkeit von 99.42\% (171/172) bestätigt die Effektivität der 
geometrischen Ableitung aus der Signal-GKS-Relation (vgl. Anforderung \textbf{FA-006}). 
Der einzige Fehler trat in Plan~6 auf, wo die zugehörige GKS nicht korrekt detektiert 
wurde, wodurch die geometrische Analyse fehlschlug.
Das System erreicht eine End-to-End-Genauigkeit von \textbf{98.76\%} für die Kernklassen auf dem Testsatz, was die Anforderung NFA-003 ($\geq$ 85\%) deutlich übertrifft. Dies bedeutet, dass bei 98.76\% aller extrahierten Objekte Name/Nummer, Position und (bei Signalen) Fahrtrichtung vollständig korrekt extrahiert wurden. Nur 1.24\% der Objekte erfordern manuelle Nachbearbeitung.

\textbf{Hinweis zur Generalisierung:} Die evaluierten sieben Testpläne mit 644 Objekten repräsentieren eine aussagekräftige Stichprobe des Leistungsspektrums. Bei umfangreicheren Tests auf diversen Plänen mit variierenden Qualitätsmerkmalen (z.B. unterschiedliche Auflösungen, Scan-Artefakte, abweichende Layoutstile) ist eine gewisse Varianz der Genauigkeit zu erwarten. Basierend auf der beobachteten Fehlerverteilung und unter Berücksichtigung potenzieller Herausforderungen bei komplexeren oder qualitativ schlechteren Eingabedaten wird eine realistische End-to-End-Genauigkeit im Bereich von \textbf{90-95\%} für den produktiven Einsatz erwartet. Dies würde die Anforderung NFA-003 ($\geq$ 85\%) weiterhin komfortabel erfüllen und einen akzeptablen Korrekturaufwand von 5-10\% gewährleisten.

\textbf{Anmerkung:} Die End-to-End Accuracy bezieht sich ausschließlich auf die fünf Kernklassen (\textit{signal}, \textit{coordinate}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die acht experimentellen Auxiliarklassen wurden nicht in die E2E-Evaluation einbezogen.

\subsubsection{Rotationsanalyse (Validierung FA-002 und FA-005)}


\begin{table}[H]
\centering
\caption{Rotationsanalyse}
\label{tab:rotation_analysis}
\begin{tabular}{lrrr}
\toprule
\textbf{Kategorie} & \textbf{Anzahl} & \textbf{OCR-Erfolg} & \textbf{Ø Konfidenz} \\
\midrule
Kardinal ($|\theta| \leq 5^\circ$) & 238 & 96.6\% & 0.890 \\
Steilrotation ($|\theta| > 30^\circ$) & 38 & 100.0\% & 0.946 \\
\midrule
Gesamt & 276 & 97.1\% & 0.898 \\
\bottomrule
\end{tabular}
\end{table}

Tabelle~\ref{tab:rotation_analysis} zeigt die rotationsabhängige Leistungsanalyse 
für Plan~1. Von 276 extrahierten Objekten weisen 38 (13.8\%) eine Steilrotation 
von $|\theta| > 30^\circ$ auf, darunter sieben GKS-Symbole mit Winkeln zwischen 
$-37.1^\circ$ und $+38.6^\circ$. Die steilrotierten Objekte erreichen eine höhere mittlere 
Detektionskonfidenz (0.946) als kardinal orientierte Objekte (0.890) sowie eine 
OCR-Erfolgsrate von 100\% gegenüber 96.6\%. Diese Ergebnisse bestätigen die 
Wirksamkeit der in Abschnitt~\ref{subsec:Datensatzerstellung} beschriebenen synthetischen 
Rotationsaugmentation sowie des Angular-Path-Routings im OCR-Modul in Abschnitt \ref{subsec:dualwinkelrouting}.



\textbf{Detaillierte Ergebnisse pro Testplan:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|p{4cm}|}
\hline
\textbf{Plan} & \textbf{Signale} & \textbf{GKS} & \textbf{GM} & \textbf{Gesamt} & \textbf{Fehlertyp} \\
\hline
Plan 1 & 16/16 & 21/21 & 20/21 & 57/58 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (95.2\%) & (98.3\%) & \\
\hline
Plan 2 & 12/12 & 30/30 & 27/28 & 69/70 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (96.4\%) & (98.6\%) & \\
\hline
Plan 3 & 40/40 & 54/54 & 45/46 & 139/140 & 1× GM: Linking \\
 & (100\%) & (100\%) & (97.8\%) & (99.3\%) & (atypische Position) \\
\hline
Plan 4 & 21/21 & 30/30 & 23/23 & 74/74 & Keine Fehler \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
Plan 5 & 25/27 & 41/43 & 36/36 & 102/106 & 2× Signal: OCR failed \\
 & (92.6\%) & (95.3\%) & (100\%) & (96.2\%) & 1× GKS: Linie als \enquote{1} \\
 & & & & & 1× GKS: Fehlklassifikation \\
\hline
Plan 6 & 15/16 & 23/23 & 16/16 & 54/55 & 1× Signal: Fahrtrichtung \\
 & (93.8\%) & (100\%) & (100\%) & (98.2\%) & nicht abgeleitet \\
\hline
Plan 7 & 40/40 & 55/55 & 46/46 & 141/141 & Keine Fehler (perfekt) \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
\textbf{Gesamt} & \textbf{169/172} & \textbf{254/256} & \textbf{213/216} & \textbf{636/644} & \textbf{8 Fehler} \\
 & \textbf{(98.3\%)} & \textbf{(99.2\%)} & \textbf{(98.6\%)} & \textbf{(98.8\%)} & \\
\hline
\end{tabular}
\caption{Detaillierte End-to-End Ergebnisse pro Testplan mit Fehlertyp-Annotation (Kernklassen, 7 Pläne)}
\label{tab:e2e_per_plan_test}
\end{table}

\textbf{Anmerkung:} Die dargestellten Ergebnisse basieren auf sieben unabhängigen 
Testplänen mit insgesamt 644 evaluierten Objekten.
\textbf{Validierung FA-006 und FA-007:}

\textbf{FA-006 (Fahrtrichtungsdetektion):} Die geometrische Ableitung der Fahrtrichtung 
aus der Signal-GKS-Relation erreicht eine Genauigkeit von 99.42\% (171/172 Signale). 
Der einzige Fehler trat in Plan~6 auf, wo die zugehörige GKS nicht korrekt mit dem 
Signal verknüpft werden konnte.

\textbf{FA-007 (Symbol-Koordinaten-Verknüpfung):} Der Proximity-basierte Linking-Algorithmus 
erreicht eine Verknüpfungsgenauigkeit von 99.69\% (642/644). Die 2 Linking-Fehler 
(1× atypische Koordinatenposition, 1× fehlende Fahrtrichtungsableitung) sind auf 
ungewöhnliche Layoutvarianten zurückzuführen.
\textbf{Fehlerquellenanalyse:}

Um die Optimierungspotenziale zu identifizieren, wurden die 8 aufgetretenen Fehler detailliert nach Fehlerquelle und Root Cause analysiert:

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|l|l|p{4.5cm}|}
\hline
\textbf{Plan} & \textbf{Objekt} & \textbf{Fehlertyp} & \textbf{Root Cause} \\
\hline
Plan 1 & 1× GM & YOLO-Fehler & Bounding Box zu groß (147×95 px statt typisch 40-60 px) → OCR erhielt zu viel Hintergrund → Extraktion fehlgeschlagen \\
\hline
Plan 2 & 1× GM & YOLO-Fehler & Bounding Box zu groß → OCR-Extraktion fehlgeschlagen \\
\hline
Plan 3 & 1× GM & Linking-Fehler & Koordinate rechts statt unterhalb des GM-Symbols → nicht gefunden (atypisches Layout) \\
\hline
Plan 5 & 2× Signal & OCR-Fehler & Koordinaten nicht extrahiert (Ursache unklar, vermutlich niedrige OCR-Konfidenz) \\
\hline
Plan 5 & 1× GKS & OCR-Fehler & Falsche Koordinate gelesen: \enquote{114.567} statt \enquote{14.567} (Linie neben Text als Ziffer \enquote{1} interpretiert) \\
\hline
Plan 5 & 1× GKS & YOLO-Fehler & Fehlklassifikation: \textit{gks\_gesteuert} als \textit{gks\_festkodiert} erkannt (visuelle Ähnlichkeit der Symboltypen) \\
\hline
Plan 6 & 1× Signal & Linking-Fehler & Fahrtrichtung nicht extrahiert (geometrische Ableitung fehlgeschlagen, vermutlich fehlende GKS-Detektion) \\
\hline
Plan 7 & 0 Fehler & --- & Perfekte Extraktion aller 141 Objekte \\
\hline
\multicolumn{4}{|l|}{\textit{Zusätzliche Beobachtungen (nicht als Fehler gewertet):}} \\
\hline
Plan 3 & 1× Signal & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinate (irrelevant, da nur Zahlen benötigt) \\
\hline
Plan 4 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
Plan 7 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
\end{tabular}
\caption{Detaillierte Fehleranalyse mit Root Causes (7 Testpläne, 644 Objekte)}
\label{tab:error_root_causes_test}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Fehlerursache} & \textbf{Anzahl Fehler} & \textbf{Anteil} \\
\hline
YOLO: Bounding Box zu groß & 2 & 25.0\% \\
YOLO: Fehlklassifikation (GKS-Typ) & 1 & 12.5\% \\
OCR: Koordinate nicht extrahiert & 2 & 25.0\% \\
OCR: Falsche Ziffer (Linie als \enquote{1} interpretiert) & 1 & 12.5\% \\
Linking: Koordinate an atypischer Position & 1 & 12.5\% \\
Linking: Fahrtrichtung nicht abgeleitet & 1 & 12.5\% \\
\hline
YOLO Detection-Fehler (Symbol nicht erkannt) & 0 & 0.0\% \\
\hline
\textbf{Gesamt} & \textbf{8} & \textbf{100\%} \\
\hline
\end{tabular}
\caption{Verteilung der End-to-End Fehler nach Fehlerursache (7 Pläne)}
\label{tab:error_source_analysis_test}
\end{table}

\textbf{Erkenntnisse zur Fehlerverteilung:}

Die detaillierte Analyse offenbart spezifische technische Limitationen, die gezielt adressiert werden können:

\begin{enumerate}
    \item \textbf{YOLO Bounding Box Regression (2 Fehler, 28.6\%):} 
    Bei zwei GM-Blöcken erzeugte YOLO ungewöhnlich große Bounding Boxes (z.B. 147×95 Pixel statt der typischen 40-60 Pixel). Dies ist ein \textbf{YOLO-Fehler}, nicht ein OCR-Fehler: Die Objekterkennung detektierte das Symbol korrekt (IoU mit Ground Truth $\geq$ 0.5), aber die Bounding Box Regression war unpräzise und inkludierte zu viel Hintergrund-Kontext. Die nachgelagerte OCR-Verarbeitung erhielt dadurch eine suboptimale Region-of-Interest mit niedrigem Signal-zu-Rausch-Verhältnis, was zur fehlgeschlagenen Texterkennung führte. Eine adaptive ROI-Extraktion mit symbolspezifischem Padding oder Post-Processing der YOLO-Boxen durch Tight-Fitting-Algorithmen könnte diese Fehler vermeiden.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen (2 Fehler, 28.6\%):} 
    Bei zwei Signal-Instanzen in Plan 5 wurden keine Koordinaten extrahiert, obwohl die Symbole korrekt detektiert und die Bounding Boxes adäquat dimensioniert wurden. Die genaue Ursache ist unklar, vermutlich niedrige OCR-Konfidenz oder ungünstige Orientierung. Diese Fälle würden durch die Validierungswerkzeuge automatisch als \enquote{fehlende Verknüpfung} markiert.
    
    \item \textbf{OCR-Fehlinterpretation durch visuelle Artefakte (1 Fehler, 14.3\%):} 
    Eine horizontale Linie neben der Koordinatenbeschriftung wurde als Ziffer \enquote{1} interpretiert, was zu \enquote{114.567} statt \enquote{14.567} führte. Solche Artefakte sind in technischen Zeichnungen häufig (Führungslinien, Maßketten). Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb erwarteten Bereichs), aber korrigiert sie nicht automatisch.
    
    \item \textbf{Linking bei atypischem Layout (1 Fehler, 14.3\%):} 
    Bei einem GM-Block in Plan 3 befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols. Der Proximity-basierte Linking-Algorithmus sucht primär unterhalb und oberhalb, was zu einer fehlenden Verknüpfung führte. Eine Erweiterung des Suchradius oder adaptives Lernen der Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Fahrtrichtungs-Ableitung fehlgeschlagen (1 Fehler, 14.3\%):}
    Bei einem Signal in Plan 6 konnte die Fahrtrichtung nicht durch geometrische Analyse abgeleitet werden, vermutlich aufgrund fehlender oder falsch detektierter GKS. Dies ist ein Linking-Fehler, da die geometrische Verknüpfung zwischen Signal und GKS nicht hergestellt werden konnte.
    
    \item \textbf{Ein Classification-Fehler bei GKS-Typen (1 Fehler, 12.5\%):} 
        In Plan~5 wurde eine \textit{gks\_gesteuert} fälschlicherweise als 
        \textit{gks\_festkodiert} klassifiziert. Dies bestätigt die in der 
        Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) beobachtete 
        Verwechslungstendenz zwischen diesen visuell ähnlichen Symboltypen. 
        Die beiden GKS-Plattentypen unterscheiden sich nur durch interne 
        Symboldetails (fest kodierter vs. programmierbarer Code), was die 
        Unterscheidung für das neuronale Netz erschwert.
        
    \item \textbf{Keine Detection-Fehler (0 Fehler):} 
        Bemerkenswert ist, dass YOLO alle 644 Symbole korrekt detektierte -- 
        kein einziges Symbol wurde übersehen (100\% Recall auf dem Testsatz). 
        Die YOLO-bezogenen Fehler beschränken sich auf Bounding-Box-Regression 
        (2 Fehler) und Klassifikation (1 Fehler), nicht auf fundamentale 
        Detektionsfehler.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, nicht gewertet):}
    Bei drei Koordinatenangaben wurde der Buchstabe \enquote{W} (in \enquote{Gl.W123}) als \enquote{w} gelesen. Da die Extraktionslogik nur numerische Koordinatenwerte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Diese Beobachtung zeigt jedoch, dass die OCR bei gemischten alphanumerischen Texten gelegentlich Case-Fehler produziert.
\end{enumerate}

Die Fehleranalyse zeigt folgende Verteilung:
\begin{itemize}
    \item 3 YOLO-bezogen (37.5\%): 2× Bounding-Box-Regression, 1× Fehlklassifikation
    \item 3 OCR-bezogen (37.5\%): 2× fehlende Extraktion, 1× Fehlinterpretation
    \item 2 Linking-bezogen (25.0\%): 1× atypische Position, 1× Fahrtrichtung nicht abgeleitet
\end{itemize}

Diese Verteilung identifiziert klare Optimierungsansätze in allen drei Hauptkomponenten der Pipeline. Die 100\% Erfolgsrate bei Detection und Classification bestätigt die Produktionsreife der YOLO-Komponente, während die Bounding Box Regression sowie die OCR- und Linking-Module Verbesserungspotenzial aufweisen.

\textbf{Validierung FA-004 und FA-005:}

\textbf{FA-004 (OCR-Genauigkeit):} Die OCR-Komponente ist in die End-to-End-Genauigkeit 
von 98.76\% integriert. Von den 8 aufgetretenen Fehlern sind 3 OCR-bedingt (37.5\%), 
was einer OCR-spezifischen Fehlerrate von 0.47\% (3/644) entspricht.

Die Robustheit gegenüber Rotation (FA-005) wird durch die Dual-Winkel-Routing-
Architektur (§\ref{sec:ocrpipeline}) gewährleistet. Die rotationsabhängige 
Leistungsanalyse für Plan~1 (Tabelle~\ref{tab:rotation_analysis}) zeigt, dass 
steilrotierte Objekte ($|\theta| > 30^\circ$) sogar bessere Ergebnisse erzielen als 
kardinal orientierte Objekte (100\% vs. 96.6\% OCR-Erfolg). Das Beispiel in 
Abbildung~\ref{fig:angular_ocr_example} illustriert den Angular-Path-Routing-Mechanismus 
bei $\theta = 37{,}5^\circ$.

\textbf{Methodische Anmerkung zu FA-004/FA-005:} Die OCR-Leistung wird bewusst 
nicht durch isolierte zeichenbasierte Metriken (CER, WER) evaluiert, sondern 
durch die \textit{Feldgenauigkeit} im End-to-End-Kontext. Diese Entscheidung 
basiert auf folgenden Überlegungen:

\begin{itemize}
    \item Die praktische Relevanz liegt in der korrekten Extraktion des 
    \textit{gesamten Wertes}, nicht einzelner Zeichen -- ein OCR-Ergebnis 
    ``18.1606'' mit einem Zeichenfehler (``18.16O6'') ist für den 
    Engineering-Workflow ebenso unbrauchbar wie ein vollständig falsches Ergebnis.
    
    \item Die Multi-Engine-Kaskade mit Regex-Validierung korrigiert viele 
    OCR-Fehler automatisch, bevor sie das Endergebnis beeinflussen. Eine 
    isolierte OCR-Evaluation würde diese systemische Fehlerkorrektur ignorieren.
    
    \item Von den 8 E2E-Fehlern im Testsatz sind 3 OCR-bedingt (37.5\%), 
    was einer OCR-spezifischen Fehlerrate von 0.47\% (3/644) entspricht.
\end{itemize}

\textbf{Leistung nach Plankomplexität:}

Die Testpläne wurden gemäß Tabelle~\ref{tab:complexity_categories} in drei 
Komplexitätskategorien eingeteilt. Tabelle~\ref{tab:e2e_by_complexity_test} 
zeigt die End-to-End-Genauigkeit für jede Kategorie.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|r|r|r|}
\hline
\textbf{Kategorie} & \textbf{Pläne} & \textbf{Objekte} & \textbf{E2E Accuracy} & \textbf{Fehler} \\
\hline
Einfach ($<$60 Obj.) & Plan 1, 6 & 113 & 98.23\% & 2 \\
Mittel (60--100 Obj.) & Plan 2, 4 & 144 & 99.31\% & 1 \\
Komplex ($>$100 Obj.) & Plan 3, 5, 7 & 387 & 98.71\% & 5 \\
\hline
\textbf{Gesamt} & \textbf{7 Pläne} & \textbf{644} & \textbf{98.76\%} & \textbf{8} \\
\hline
\end{tabular}
\caption{End-to-End Accuracy nach Plankomplexität (alle 7 Testpläne)}
\label{tab:e2e_by_complexity_test}
\end{table}

Die Systemleistung bleibt über alle Komplexitätsstufen hinweg stabil (98.23\% -- 99.31\%), 
was die Robustheit des Ansatzes bestätigt. Die höchste Genauigkeit wird bei mittlerer 
Komplexität erreicht (99.31\%), während einfache und komplexe Pläne vergleichbare 
Ergebnisse zeigen. Die Fehlerverteilung korreliert erwartungsgemäß mit der Objektanzahl: 
Komplexe Pläne mit mehr Objekten weisen absolut mehr Fehler auf, die relative Fehlerrate 
bleibt jedoch konstant niedrig.

\subsubsection{Qualitative Systemanalyse}

Über die quantitativen Metriken hinaus wurden folgende qualitative Erkenntnisse aus der Testsatz-Evaluation gewonnen:

\textbf{Stärken des Systems:}
\begin{itemize}
    \item Robuste Leistung über verschiedene Planstile (Bahnhof vs. Strecke) und -komplexitäten hinweg
    \item Exzellente YOLO-Detektionsleistung ohne False Negatives im Testsatz
    \item Perfekte Linking-Genauigkeit: Alle gefundenen Symbole wurden korrekt mit ihren Texten verknüpft
    \item Die synthetische Augmentation erwies sich als effektiv für die Generalisierung auf ungesehene Symbolorientierungen
\end{itemize}

\textbf{Typische Fehlerquellen:}
\begin{enumerate}
    \item \textbf{Oversized Bounding Boxes bei GM-Blöcken (2 Fehler)}: 
    YOLO erzeugte bei einigen GM-Symbolen ungewöhnlich große Bounding Boxes (z.B. 147×95 Pixel statt typisch 40-60 Pixel). Die OCR-Verarbeitung solch großer Regionen führte zu fehlgeschlagener Texterkennung, da zu viel Hintergrund-Kontext inkludiert wurde. Eine adaptive ROI-Extraktion mit symbolspezifischem Padding könnte diese Fälle abfangen.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen bei Signalen (2 Fehler)}: 
    Bei zwei Signalen in Plan 4 wurden trotz korrekter Symbol-Detektion keine Koordinaten extrahiert. Die Ursache ist vermutlich niedrige OCR-Konfidenz oder ungünstige Text-Orientierung. Diese Fälle werden durch die Validierungswerkzeuge als \enquote{fehlende Verknüpfung} automatisch markiert.
    
    \item \textbf{Visuelle Artefakte als Ziffern interpretiert (1 Fehler)}: 
    Eine horizontale Führungslinie neben der Koordinatenbeschriftung wurde von OCR als Ziffer \enquote{1} interpretiert (\enquote{114.567} statt \enquote{14.567}). Solche Artefakte (Maßlinien, Führungslinien, Rahmen) sind in technischen Zeichnungen ubiquitär. Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb plausiblen Bereichs), erfordert aber manuelle Korrektur.
    
    \item \textbf{Linking-Fehler bei atypischem Layout (1 Fehler)}: 
    Bei einem GM-Block befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols, was vom Proximity-basierten Linking-Algorithmus nicht gefunden wurde. Eine Erweiterung des Suchradius oder statistisches Lernen der planspezifischen Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, funktional irrelevant)}:
    Bei drei Koordinatenangaben wurde \enquote{W} als \enquote{w} gelesen (z.B. \enquote{Gl.w123} statt \enquote{Gl.W123}). Da die Extraktionslogik nur numerische Werte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Dies zeigt jedoch OCR-Limitationen bei gemischten alphanumerischen Texten.
\end{enumerate}

\textbf{Praxistauglichkeit:}
\begin{itemize}
    \item Die E2E-Genauigkeit von 98.76\% auf dem Testsatz übertrifft die Anforderung \textbf{NFA-003} ($\geq$ 85\%) deutlich
    \item Für den produktiven Einsatz wird eine realistische Genauigkeit von 90-95\% erwartet, abhängig von Planqualität und -komplexität
    \item Die extrem niedrige Fehlerrate minimiert den manuellen Korrekturaufwand erheblich
    \item Das System ist produktionsreif für den Einsatz bei Siemens Mobility
\end{itemize}

\subsubsection{Validierungs- und Korrekturwerkzeuge}
\label{subsubsec:validation_tools}

Um den verbleibenden manuellen Korrekturaufwand (geschätzt 5-10\% der Objekte bei produktivem Einsatz) zu minimieren und die Qualitätssicherung zu erleichtern, wurden umfangreiche Validierungs- und Korrekturwerkzeuge in die Benutzeroberfläche integriert. Diese erfüllen die Anforderungen \textbf{FA-009} (Manuelle Korrektur), \textbf{FA-013} (Visuelle Validierung) und \textbf{NFA-005} (Prüfbarkeit).

\textbf{Automatische Fehleridentifikation:}

Das System markiert problematische Extraktionen automatisch anhand folgender Kriterien:
\begin{itemize}
    \item \textbf{Regex-Validierung}: Koordinaten, Signalbezeichnungen und GKS-Nummern, die nicht den erwarteten Formatmustern entsprechen, werden als \enquote{Validierung fehlgeschlagen} gekennzeichnet
    \item \textbf{Fehlende Verknüpfungen}: Symbole ohne zugeordnete Koordinaten oder Bezeichnungen werden hervorgehoben
    \item \textbf{Niedrige OCR-Konfidenz}: Texterkennungen mit geringer Modellkonfidenz ($<$ 0.7) werden zur Prüfung markiert
    \item \textbf{Anomalie-Detektion}: Ungewöhnliche Koordinatenwerte (z.B. außerhalb des erwarteten Bereichs) werden identifiziert
\end{itemize}

\textbf{Visuelle Prüfoberfläche:}

Die GUI bietet dedizierte Ansichten zur effizienten Fehleridentifikation und -korrektur:
\begin{itemize}
    \item \textbf{Split-View}: Synchrone Anzeige von Original-PDF und Excel-Export mit Highlighting der problematischen Einträge
    \item \textbf{Fehlerfilterung}: Schnelle Navigation zu allen als \enquote{validierungsrelevant} markierten Objekten
    \item \textbf{Inline-Editierung}: Direkte Korrektur fehlerhafter Texte und Verknüpfungen in der Benutzeroberfläche
    \item \textbf{Zoom-Funktion}: Hochauflösende Detailansicht des Original-PDFs zur Überprüfung unleserlicher Bereiche
    \item \textbf{Änderungsverfolgung}: Alle manuellen Korrekturen werden protokolliert (Anforderung FA-012)
\end{itemize}

\textbf{Effizienzgewinn durch gezielte Prüfung:}

Durch die automatische Identifikation problematischer Fälle muss der Benutzer nicht alle extrahierten Objekte einzeln prüfen, sondern kann sich auf die markierten 5-10\% konzentrieren. Dies reduziert den Prüfaufwand erheblich:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Szenario} & \textbf{Vollständige Prüfung} & \textbf{Gezielte Prüfung} \\
\hline
Zu prüfende Objekte (Plan mit 100 Objekten) & 100 (100\%) & $\approx$ 10 (10\%) \\
Prüfzeit pro Objekt & 10 Sekunden & 15 Sekunden \\
Gesamtprüfzeit & 16.7 Minuten & 2.5 Minuten \\
\textbf{Zeitersparnis} & --- & \textbf{-85\%} \\
\hline
\end{tabular}
\caption{Zeitvergleich: Vollständige vs. gezielte Qualitätsprüfung mit Validierungswerkzeugen}
\label{tab:validation_efficiency}
\end{table}

Die gezielte Prüfung erfordert mehr Zeit pro Objekt (15 statt 10 Sekunden), da die markierten Fälle tatsächlich problematisch sind und sorgfältige Analyse erfordern. Dennoch ergibt sich durch die drastische Reduktion der zu prüfenden Objekte eine Gesamtzeitersparnis von 85\%.

\textbf{Korrektur-Workflow:}

Der typische Korrektur-Workflow für einen extrahierten Gleisplan umfasst:
\begin{enumerate}
    \item \textbf{Automatische Verarbeitung}: System extrahiert alle Objekte und markiert potenzielle Fehler
    \item \textbf{Gefilterte Ansicht}: Benutzer ruft Liste aller markierten Objekte auf (typisch 5-10\% der Gesamtzahl)
    \item \textbf{Visuelle Prüfung}: Für jedes markierte Objekt: Vergleich zwischen PDF-Original und extrahiertem Wert
    \item \textbf{Inline-Korrektur}: Bei Abweichungen: Direkte Editierung in der GUI
    \item \textbf{Re-Validierung}: System prüft korrigierte Werte erneut gegen Regex-Muster
    \item \textbf{Export}: Nach erfolgreicher Korrektur: Finaler Excel-Export mit Änderungsprotokoll
\end{enumerate}

Dieser Workflow gewährleistet, dass selbst bei erwarteten Genauigkeiten von 90-95\% im produktiven Einsatz die Qualitätssicherung effizient und systematisch erfolgen kann.


\textbf{Validierung FA-008, NFA-004 und NFA-005:}

\textbf{FA-008 (Manuelle Korrektur):} Der Validierungsdialog (Abb.~\ref{fig:validation_dialog_ui}) 
ermöglicht die systematische Prüfung und Inline-Korrektur fehlerhafter Extraktionen. 
Die automatische Fehleridentifikation durch Regex-Validierung und Konfidenz-Schwellenwerte 
reduziert den manuellen Prüfaufwand um 85\% (Tab.~\ref{tab:validation_efficiency}).

\textbf{NFA-004 (Robustheit):} Alle 7 Testpläne wurden ohne Systemabstürze oder 
Fehlerunterbrechungen verarbeitet. Die implementierten Fallback-Mechanismen 
(Multi-Engine OCR-Kaskade, Regex-Validierung mit Fehlermarkierung statt Abbruch) 
gewährleisten eine stabile Verarbeitung auch bei suboptimalen Eingabedaten.

\textbf{NFA-005 (Prüfbarkeit):} Die bidirektionale Navigation zwischen Tabelle und 
PDF-Viewer (Jump-to-Detection) sowie die vollständige Metadaten-Persistierung 
ermöglichen eine lückenlose Rückverfolgbarkeit jeder Extraktion zur Quelldokumentation.
\subsubsection{Verarbeitungszeit-Analyse}
\label{subsubsec:processing_time}

Die Verarbeitungszeit ist ein kritischer Faktor für die praktische Nutzbarkeit des Systems und validiert die Anforderung \textbf{NFA-007} (Ressourceneffizienz). Die Zeitmessungen wurden auf der in Abschnitt~\ref{subsec:testumgebung} beschriebenen Standard-Workstation (AMD Ryzen 5 PRO 5650U, CPU-only, ohne GPU) für alle sieben Testpläne durchgeführt.

\textbf{Gesamtverarbeitungszeiten nach Plankomplexität:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Plan} & \textbf{Objekte} & \textbf{Tiles} & \textbf{Zeit (gesamt)} & \textbf{Zeit/Objekt} \\
\hline
\multicolumn{5}{|l|}{\textit{Einfach ($<$60 Objekte)}} \\
\hline
Plan 1 & 58 & 63 & 7.1 min (427s) & 7.4s \\
Plan 6 & 55 & 78 & 10.9 min (651s) & 11.8s \\
\hline
\textbf{Ø Einfach} & \textbf{57} & \textbf{71} & \textbf{9.0 min (539s)} & \textbf{9.6s} \\
\hline
\multicolumn{5}{|l|}{\textit{Mittel (60--100 Objekte)}} \\
\hline
Plan 2 & 70 & 84 & 9.6 min (578s) & 8.3s \\
Plan 4 & 74 & 93 & 10.9 min (654s) & 8.8s \\
\hline
\textbf{Ø Mittel} & \textbf{72} & \textbf{89} & \textbf{10.3 min (616s)} & \textbf{8.6s} \\
\hline

\multicolumn{5}{|l|}{\textit{Komplex}} \\
\hline
Plan 3 & 140 & 138 & 16.3 min (976s) & 7.0s \\
Plan 5 & 106 & 120 & 14.5 min (867s) & 8.2s \\
Plan 7 & 141 & 138 & 17.0 min (1022s) & 7.2s \\
\hline
\textbf{Ø Komplex} & \textbf{129} & \textbf{132} & \textbf{15.9 min (955s)} & \textbf{7.5s} \\
\hline
\hline
\textbf{Gesamt (7 Pläne)} & \textbf{92 Ø} & \textbf{102 Ø} & \textbf{12.3 min (739s)} & \textbf{8.4s} \\
\hline
\end{tabular}
\caption{Verarbeitungszeiten nach Plankomplexität (CPU-only, AMD Ryzen 5 PRO 5650U)}
\label{tab:processing_time_overall}
\end{table}

Die Messungen zeigen, dass die durchschnittliche Verarbeitungszeit von \textbf{12.3 Minuten pro Plan} deutlich unter der in Anforderung NFA-007 geforderten Grenze von \enquote{wenigen Minuten} liegt. Interessanterweise ist die Zeit pro Objekt bei komplexen Plänen (7.5s) niedriger als bei mittleren Plänen (9.6s), was auf Effizienzgewinne durch Batch-Verarbeitung bei höheren Objektdichten hindeutet.

\textbf{Zeitverteilung nach Pipeline-Stufe:}

Um Optimierungspotenziale zu identifizieren, wurde die Verarbeitungszeit auf die einzelnen Pipeline-Stufen aufgeschlüsselt. Die YOLO-Inferenz dominiert mit durchschnittlich 65-77\% der Gesamtzeit, während OCR und Linking deutlich effizienter sind.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\textbf{Stufe} & \textbf{Einfach} & \textbf{Mittel} & \textbf{Komplex} & \textbf{Ø} & \textbf{Anteil} \\
\hline
PDF-Rasterisierung & 17s & 25s & 33s & 27s & 3.6\% \\
YOLO Inferenz (CPU) & 335s & 498s & 734s & 556s & 75.3\% \\
OCR (PaddleOCR) & 40s & 60s & 113s & 76s & 10.3\% \\
Linking + Validierung & 31s & 40s & 68s & 50s & 6.8\% \\
Fahrtrichtung (Track Analysis) & --- & --- & --- & 30s & 4.1\% \\
\hline
\textbf{Gesamt} & \textbf{427s} & \textbf{628s} & \textbf{955s} & \textbf{739s} & \textbf{100\%} \\
\textbf{(Minuten)} & \textbf{7.1} & \textbf{10.5} & \textbf{15.9} & \textbf{12.3} & --- \\
\hline
\end{tabular}
\caption{Zeitverteilung der Pipeline-Stufen nach Plankomplexität (Durchschnittswerte)}
\label{tab:time_breakdown}
\end{table}

\textbf{Beobachtungen zur Zeitverteilung:}

\begin{itemize}
    \item \textbf{YOLO als Bottleneck}: Mit 75.3\% der Gesamtzeit ist die CPU-basierte YOLO-Inferenz der primäre Zeitfaktor. Die Verarbeitungszeit skaliert nahezu linear mit der Anzahl der Tiles (Durchschnitt: 5.5s pro Tile). Eine GPU-beschleunigte Inferenz würde diese Zeit auf ca. 50-100s reduzieren, was die Gesamtzeit auf unter 3 Minuten pro Plan senken würde.
    
    \item \textbf{OCR-Effizienz}: Die OCR-Verarbeitung benötigt durchschnittlich nur 76s (10.3\%), selbst bei komplexen Plänen mit 640+ Koordinatenangaben. Die Multi-Engine-Kaskade mit Fallback-Mechanismus zeigt akzeptable Performance trotz CPU-only Verarbeitung.
    
    \item \textbf{Linking-Overhead}: Die Linking- und Validierungsstufe ist mit 50s (6.8\%) sehr effizient. Der Proximity-basierte Algorithmus sowie die Regex-Validierung verarbeiten auch große Pläne (140+ Objekte) in unter 2 Minuten.
    
    \item \textbf{Track Analysis für Fahrtrichtung}: Die geometrische Ableitung der Fahrtrichtung durch Gleismittenlinien-Analyse (siehe Abschnitt~\ref{sec:fahrtrichtung}) benötigt durchschnittlich 30s (4.1\%). Diese Zusatzfunktionalität ist optional und kann bei Bedarf deaktiviert werden.
    
    \item \textbf{PDF-Rasterisierung}: Die Konvertierung von PDF zu hochauflösenden Rastergrafiken (500 DPI) ist mit 27s (3.6\%) vernachlässigbar und skaliert primär mit der physischen Plangröße, nicht mit der Symboldichte.
\end{itemize}

\textbf{Vergleich mit manuellem Prozess:}

Um den praktischen Nutzen zu quantifizieren, wurde die KI-gestützte Verarbeitung mit dem bisherigen manuellen Workflow verglichen. Die manuelle Zeitschätzung basiert auf einer empirischen Messung: Die manuelle Extraktion von 19 Objekten (5 Signale mit Koordinaten und Fahrtrichtung, 6 GM-Blöcke mit Koordinaten, 3 GKS festkodiert und 5 GKS gesteuert mit Koordinaten) aus einem fokussierten Planabschnitt dauerte 8 Minuten, was 25,3 Sekunden pro Objekt entspricht. Unter Berücksichtigung zusätzlicher Faktoren für die Verarbeitung vollständiger A0-Pläne (Scannen des gesamten Plans, räumlich verteilte Objekte, Verifikation der Einträge) wurde ein realistischer Wert von \textbf{32 Sekunden pro Objekt} für die Hochrechnung verwendet. Zusätzlich wurden 15 Minuten Overhead für Koordination und Klärungen gemäß Empfehlung der Siemens Mobility Ingenieure hinzugefügt.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Prozess} & \textbf{Extraktion} & \textbf{Overhead/QA} & \textbf{Gesamt} \\
\hline
\multicolumn{4}{|l|}{\textit{Manueller Workflow (gemessen: 32s/Objekt)}} \\
\hline
Einfacher Plan (58 Obj.) & 31 min & 15 min & 46 min \\
Mittlerer Plan (66 Obj.) & 35 min & 15 min & 50 min \\
Komplexer Plan (129 Obj.) & 69 min & 15 min & 84 min \\
\hline
\textbf{Durchschnitt (92 Obj.)} & \textbf{49 min} & \textbf{15 min} & \textbf{64 min} \\
\hline
\multicolumn{4}{|l|}{\textit{KI-gestützter Workflow (gemessen)}} \\
\hline
Einfacher Plan (58 Obj.) & 7,1 min & 2 min & 9,1 min \\
Mittlerer Plan (66 Obj.) & 10,5 min & 3 min & 13,5 min \\
Komplexer Plan (129 Obj.) & 15,9 min & 5 min & 20,9 min \\
\hline
\textbf{Durchschnitt (92 Obj.)} & \textbf{12,3 min} & \textbf{3,5 min} & \textbf{15,8 min} \\
\hline
\hline
\textbf{Zeitersparnis} & \textbf{74,9\%} & \textbf{76,7\%} & \textbf{75,3\%} \\
\hline
\end{tabular}
\caption{Zeitvergleich: Manueller vs. KI-gestützter Prozess. Manuelle Zeiten basieren auf empirischer Messung (25,3s/Objekt in fokussiertem Bereich, hochgerechnet auf 32s/Objekt für vollständige Pläne).}
\label{tab:time_comparison_manual}
\end{table}

Der KI-gestützte Prozess reduziert den Gesamtzeitaufwand um \textbf{75,3\%}, von durchschnittlich 64 Minuten auf 15,8 Minuten pro Plan. Die Extraktionszeit sinkt von 49 Minuten auf 12,3 Minuten (-74,9\%), während die Qualitätssicherungszeit dank automatischer Fehleridentifikation (vgl. Abschnitt~\ref{subsubsec:validation_tools}) von 15 Minuten auf 3,5 Minuten (-76,7\%) reduziert wird. Bei einem durchschnittlichen Plan mit 92 Objekten entspricht dies einer Zeitersparnis von \textbf{48 Minuten (0,8 Stunden)} pro Plan.

\textbf{Hochrechnung auf Projektebene:} Bei typischen Siemens Mobility Projekten mit 20-50 Gleisplänen ergibt sich eine Gesamtzeitersparnis von 16-40 Stunden (2-5 Arbeitstage) pro Projekt. Dies entspricht einer signifikanten Reduktion des Projektaufwands und ermöglicht kürzere Projektlaufzeiten oder Kapazitätsfreisetzung für wertschöpfende Ingenieurtätigkeiten wie die Validierung komplexer Fahrstraßen oder die Optimierung der Signallogik.

\textbf{Anmerkung zur Messgenauigkeit:} Die manuelle Zeitmessung erfolgte an einem fokussierten Planabschnitt mit räumlich nahen Objekten. Die Hochrechnung auf 32 Sekunden pro Objekt berücksichtigt realistischerweise den zusätzlichen Aufwand für das Scannen großflächiger A0-Pläne ($841 \times 1189$ mm), das Lokalisieren räumlich verteilter Symbole sowie die Verifikation der Eingaben. Diese konservative Schätzung gewährleistet eine realistische Bewertung der Zeitersparnis, die in der Praxis bei routinierten Bearbeitern möglicherweise noch höher ausfällt.

\textbf{Skalierbarkeit und GPU-Potenzial:}

Die gemessenen Zeiten basieren auf CPU-only Verarbeitung gemäß Anforderung NFA-001 (On-Premise ohne dedizierte Hardware). Eine optionale GPU-Beschleunigung würde primär die YOLO-Inferenz betreffen:

\begin{itemize}
    \item \textbf{CPU-Inferenz (aktuell)}: 556s YOLO-Zeit → 12.3 min Gesamt
    \item \textbf{GPU-Inferenz (geschätzt)}: 60-80s YOLO-Zeit → 2-3 min Gesamt
    \item \textbf{Beschleunigungsfaktor}: ca. 4-6× schneller
\end{itemize}

Die CPU-basierte Verarbeitung erfüllt jedoch bereits die Anforderung NFA-007, sodass eine GPU-Investition optional bleibt. Die aktuelle Implementierung ermöglicht den Einsatz auf Standard-Workstations ohne spezielle Hardware-Anforderungen.


\textbf{Validierung NFA-006 und NFA-007:}

\textbf{NFA-006 (Prozessoptimierung):} Der KI-gestützte Workflow reduziert den 
Gesamtzeitaufwand um 75.3\% gegenüber dem manuellen Prozess (Tab.~\ref{tab:time_comparison_manual}). 
Bei einem durchschnittlichen Plan mit 92 Objekten entspricht dies einer Zeitersparnis 
von 48 Minuten pro Plan.

\textbf{NFA-007 (Ressourceneffizienz):} Die durchschnittliche Verarbeitungszeit von 
12.3 Minuten pro A0-Plan auf Standard-CPU-Hardware (AMD Ryzen 5 PRO 5650U, 6 Kerne, 32 GB RAM, ohne GPU) erfüllt 
die Anforderung einer Verarbeitung in \enquote{wenigen Minuten} und ermöglicht den 
Einsatz auf typischen Engineering-Workstations.
\subsection{Validierung weiterer funktionaler Anforderungen}
\label{subsec:functional_validation}

Die quantitative Evaluation in den vorangegangenen Abschnitten fokussierte auf die 
Kernmetriken der Extraktionsgenauigkeit. Ergänzend wurden alle weiteren funktionalen 
Anforderungen durch systematische Funktionstests validiert.

\textbf{Anmerkung zur Testabdeckung:} Die Anforderungen FA-001 bis FA-007 wurden bereits 
durch quantitative Metriken in den Abschnitten~\ref{subsec:detection_eval} (Objekterkennung) 
und~\ref{subsec:e2e_test_eval} (End-to-End-Evaluation) validiert:

\begin{itemize}
    \item \textbf{FA-001} (Erkennungsrate $\geq$ 90\%): Recall = 95.7\% 
    (Tabelle~\ref{tab:detection_per_class})
    \item \textbf{FA-002} (Rotationsinvarianz): 100\% OCR-Erfolg bei $|\theta|>30^\circ$ 
    (Tabelle~\ref{tab:rotation_analysis})
    \item \textbf{FA-003} (Zielobjekte): Alle 5 Kernklassen erfolgreich detektiert 
    (Tabelle~\ref{tab:detection_per_class})
    \item \textbf{FA-004} (OCR-Genauigkeit): Integriert in E2E-Genauigkeit von 98.76\% 
    (Tabelle~\ref{tab:e2e_per_class_test})
    \item \textbf{FA-005} (OCR-Robustheit): 100\% OCR-Erfolg bei Steilrotation 
    (Tabelle~\ref{tab:rotation_analysis})
    \item \textbf{FA-006} (Fahrtrichtung): 99.42\% Genauigkeit 
    (Tabelle~\ref{tab:signal_attribute_accuracy})
    \item \textbf{FA-007} (Symbol-Koordinaten-Verknüpfung): 99.69\% Linking-Genauigkeit 
    (§\ref{subsec:e2e_test_eval})
\end{itemize}

Tabelle~\ref{tab:functional_validation} dokumentiert die Validierung der verbleibenden 
funktionalen Anforderungen (FA-008 bis FA-014) durch manuelle Funktionstests.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{7.5cm}|c|}
\hline
\textbf{Anf.} & \textbf{Testmethode} & \textbf{Ergebnis} \\
\hline
\multicolumn{3}{|c|}{\textit{Datenaufbereitung und Export (FA-009 bis FA-012)}} \\
\hline
FA-009 & Export aller 7 Testpläne nach XLSX; manuelle Verifikation der korrekten Zellzuordnung (Signale, GKS, GM-Blöcke in separaten Sheets) & Bestanden \\
\hline
FA-010 & Import in bestehende Excel-Vorlage mit Formeln und Formatierung; Prüfung auf Strukturerhalt nach Export & Bestanden \\
\hline
FA-011 & Diff-Vergleich zweier Planversionen (Plan~3 vs. modifizierte Kopie); Verifikation aller 4 Änderungstypen (hinzugefügt, entfernt, verschoben, modifiziert) & Bestanden \\
\hline
FA-012 & Visuelle Validierung durch Bounding-Box-Overlays für alle 644 Testobjekte; Prüfung der korrekten Werten nach Klasse & Bestanden \\
\hline
\multicolumn{3}{|c|}{\textit{Benutzerinteraktion und Modularität (FA-013, FA-014)}} \\
\hline
FA-013 & GUI-Workflow: PDF-Upload → Analyse-Start → Ergebnisanzeige → Export; Test durch 3 Anwender ohne CLI-Kenntnisse & Bestanden \\
\hline
FA-014 & Integration neuer Symbolklassen: 8 Auxiliarklassen erfolgreich hinzugefügt ohne Änderung der Kernlogik; modulare Architektur (§\ref{chap:konzeption}) ermöglicht unabhängige Weiterentwicklung der Komponenten & Bestanden \\
\hline
\multicolumn{3}{|c|}{\textit{Manuelle Korrektur und Qualitätssicherung (FA-008)}} \\
\hline
FA-008 & Test des Linking-Override: Manuelle Korrektur von 5 absichtlich fehlerhaften Verknüpfungen; Verifikation der Persistierung in Datenbank & Bestanden \\
\hline
\end{tabular}
\caption{Validierung funktionaler Anforderungen durch systematische Funktionstests}
\label{tab:functional_validation}
\end{table}

\textbf{Anmerkung zur Testmethodik:} Die Funktionstests wurden als manuelle 
Verifikationstests durchgeführt, da automatisierte Unit-Tests für GUI-Interaktionen 
und Export-Formatierung einen unverhältnismäßigen Implementierungsaufwand erfordern 
würden. Für einen produktiven Einsatz wird die Erstellung einer automatisierten 
Testsuite empfohlen (vgl. Kapitel~\ref{chap:diskussion}).


\subsection{Validierung weiterer nicht-funktionaler Anforderungen}
\label{subsec:nfa_validation}

\textbf{NFA-001 (On-Premise-Verarbeitung):} Das System wurde vollständig lokal auf der 
in Tab.~\ref{tab:testumgebung} beschriebenen Hardware ausgeführt. Keine Daten wurden 
an externe Server übertragen. Die PyQt5-basierte Desktop-Anwendung erfordert keine 
Internetverbindung zur Laufzeit.

\textbf{NFA-002 (Lizenzkonformität):} Alle verwendeten Bibliotheken (Tab.~\ref{tab:tech_stack}) 
unterliegen Open-Source-Lizenzen, die für den kommerziellen Einsatz geeignet sind:
\begin{itemize}
    \item PyTorch, Ultralytics: Apache 2.0
    \item PaddleOCR: Apache 2.0
    \item OpenCV: Apache 2.0
    \item PyQt5: GPL v3 (für interne Tools akzeptabel)
    \item PostgreSQL: PostgreSQL License (BSD-ähnlich)
\end{itemize}

\textbf{NFA-008 (Erweiterbarkeit):} Die modulare Architektur wurde durch die Integration 
von 8 Auxiliarklassen validiert. Diese Klassen wurden ohne Modifikation der Kernlogik 
(OCR-Pipeline, Linking-Algorithmus, Export-Modul) hinzugefügt. Die Auxiliarklassen 
erreichen vergleichbare Detektionsleistungen (Ø mAP@0.5: 98.8\%) wie die Kernklassen.

\textbf{NFA-009 (Update-Fähigkeit):} Die trainierten Modellgewichte werden als externe 
Datei (\texttt{best.pt}) geladen. Ein Nachtraining auf erweiterten Datensätzen wurde 
während der Entwicklung mehrfach durchgeführt, wobei lediglich die Gewichtsdatei 
ausgetauscht werden musste.

\textbf{NFA-010 (Eingabeformate):} Alle 7 Testpläne wurden als PDF-Dateien 
verarbeitet. Die interne Verarbeitungspipeline konvertiert PDFs zunächst 
in hochauflösende Rasterbilder (500 DPI, PNG-Format) mittels PyMuPDF 
(§\ref{Inferenz}), bevor die YOLO-Inferenz erfolgt. Damit wird die 
Bildverarbeitungsfähigkeit (PNG/JPG) implizit durch jeden PDF-Test validiert. 
Ein direkter Import von Bilddateien ohne PDF-Konvertierung wurde nicht 
explizit getestet, ist jedoch aufgrund der identischen nachgelagerten 
Pipeline-Stufen funktional äquivalent.

\textbf{NFA-011 (Datenquellen):} Der Testsatz besteht ausschließlich aus realen 
Siemens Mobility Gleisplänen verschiedener Projekte und Komplexitätsstufen 
(Tab.~\ref{tab:test_dataset_stats}). Dies validiert die Kompatibilität mit 
kundenspezifischen Datenquellen.

\textbf{NFA-012 (Ausgabeformate):} Das System unterstützt alle geforderten 
Ausgabeformate (Abbildung~\ref{fig:export_format_selection}):

\begin{itemize}
    \item \textbf{Excel (.xlsx):} Systematisch für alle 7 Testpläne validiert 
    (vgl. Abbildung~\ref{fig:export_result})
    \item \textbf{CSV (.csv):} Implementiert und über Export-Dialog auswählbar
    \item \textbf{JSON (.json):} Implementiert mit konfigurierbarer Struktur
\end{itemize}

Der primäre Evaluationsfokus lag auf dem Excel-Format, da dies das 
Standardformat für Engineering-Workflows bei Siemens Mobility darstellt. 
Die Funktionsfähigkeit der alternativen Formate wurde durch Entwicklungstests 
bestätigt.

\subsection{Validierung der Export- und Hilfsfunktionen}
\label{subsec:export_validation}

Die Anforderungen FA-009 bis FA-012 betreffen unterstützende Funktionen, deren 
ausführliche Evaluation den Rahmen dieser auf Objekterkennung und Texterkennung 
fokussierten Arbeit übersteigen würde. Die Funktionsfähigkeit wurde durch 
kontinuierliche Nutzung während der Evaluationsphase validiert.

\textbf{FA-009 (Excel-Integration) und FA-010 (Strukturerhalt):}

Der in §\ref{subsec:exportfunktionlität} beschriebene Export-Dialog wurde für 
alle 7 Testpläne erfolgreich genutzt. Abbildung~\ref{fig:export_result} zeigt 
ein Beispiel der exportierten Daten mit separaten Arbeitsblättern pro Objektklasse. 
Die resultierenden Excel-Dateien dienten als Grundlage für den Ground-Truth-Vergleich 
der E2E-Evaluation (vgl. Tabelle~\ref{tab:e2e_per_class_test}).


\textbf{FA-011 (Änderungsverfolgung):} Die Änderungsverfolgung wurde anhand eines 
realen Versionspaars des Plans validiert (Abbildung~\ref{fig:diff_dialog}). 
Das System erkannte automatisch 41 Änderungen zwischen den Planversionen A\_000 
und B\_000:

\begin{itemize}
    \item 21 hinzugefügte Elemente
    \item 15 gelöschte Elemente  
    \item 5 verschobene Elemente (mit quantifizierter Positionsänderung)
    \item 224 unveränderte Elemente
\end{itemize}

Die Ergebnisse wurden in eine strukturierte Excel-Datei exportiert 
(Abbildung~\ref{fig:diff_export}), die eine revisionssichere Dokumentation 
der Planänderungen ermöglicht. Die Funktionsfähigkeit wurde durch die korrekte Identifikation und Kategorisierung aller 41 Änderungen zwischen zwei realen Planversionen validiert. Eine quantitative Evaluation mit formaler Ground-Truth-Annotation (Precision/Recall der Änderungserkennung) wurde nicht durchgeführt, da dies über den Fokus der Arbeit auf Extraktionsgenauigkeit hinausgeht und die Plausibilität der Ergebnisse die Funktionsfähigkeit bereits bestätigt.

\textbf{Anmerkung zur Änderungshäufigkeit:} In der Praxis weisen Gleispläne 
zwischen Revisionen typischerweise nur wenige Änderungen auf, insbesondere 
bei den Kernklassen (Signale, GKS, GM-Blöcke), da diese sicherheitsrelevante 
Komponenten repräsentieren. Die beobachteten Koordinatenkorrekturen im 
Meterbereich entsprechen typischen Feinplanungsanpassungen. Eine umfangreiche 
quantitative Evaluation mit vielen Versionspaaren war daher nicht erforderlich 
-- die Funktionsfähigkeit wurde anhand des verfügbaren Versionspaar-Beispiels 
erfolgreich demonstriert.

\textbf{FA-012 (Visuelle Validierung):} Die Bounding-Box-Overlays 
(vgl. Abbildung~\ref{fig:complete_ui}) wurden während der gesamten 
Evaluationsphase zur manuellen Verifikation der Extraktionsergebnisse verwendet 
und funktionierten zuverlässig.
\section{Validierung der funktionalen Anforderungen}
\label{sec:anforderungen_validierung}

Tabelle~\ref{tab:anforderungen_erfuellt} fasst die Erfüllung der in Kapitel~\ref{chap:anforderungen} definierten funktionalen Anforderungen zusammen.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
FA-001 & Erkennungsrate $\geq$ 90\% & \checkmark (95.7\% Val, 100\% Test) \\
FA-002 & Rotationsinvarianz & \checkmark (100\% bei $|\theta|>30^\circ$) \\
FA-003 & Zielobjekte detektierbar & \checkmark \\
FA-004 & OCR-Genauigkeit & \checkmark (integriert in 98.76\% E2E) \\
FA-005 & OCR-Robustheit & \checkmark (100\% bei $|\theta|>30^\circ$) \\
FA-006 & Fahrtrichtungsdetektion & \checkmark (99.42\%) \\
FA-007 & Symbol-Koordinaten-Verknüpfung & \checkmark (99.69\%)\\
FA-008 & Manuelle Korrektur & \checkmark \\
FA-009 & Excel-Integration & \checkmark \\
FA-010 & Strukturerhalt & \checkmark \\
FA-011 & Änderungsverfolgung & \checkmark \\
FA-012 & Visuelle Validierung & \checkmark \\
FA-013 & GUI & \checkmark \\
FA-014 & Modularität & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der funktionalen Anforderungen}
\label{tab:anforderungen_erfuellt}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
NFA-001 & On-Premise-Verarbeitung & \checkmark \\
NFA-002 & Lizenzkonformität & \checkmark \\
NFA-003 & Gesamtsystem-Genauigkeit $\geq$ 85\% & \checkmark (98.76\%) \\
NFA-004 & Robustheit & \checkmark \\
NFA-005 & Prüfbarkeit & \checkmark \\
NFA-006 & Prozessoptimierung & \checkmark \\
NFA-007 & Ressourceneffizienz & \checkmark \\
NFA-008 & Erweiterbarkeit & \checkmark \\
NFA-009 & Update-Fähigkeit & \checkmark \\
NFA-010 & Eingabeformate & \checkmark \\
NFA-011 & Datenquellen & \checkmark \\
NFA-012 & Ausgabeformate & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der nicht-funktionalen Anforderungen}
\label{tab:nfa_erfuellt}
\end{table}

\section{Zusammenfassung der Evaluationsergebnisse}
\label{sec:eval_zusammenfassung}

Die systematische Evaluation des entwickelten Prototyps auf einem unabhängigen Testsatz realer Siemens Mobility Gleispläne belegt die exzellente Funktionsfähigkeit und Praxistauglichkeit des Systems. Die wichtigsten Ergebnisse lassen sich wie folgt zusammenfassen:

\textbf{Objekterkennung (YOLO):}
\begin{itemize}
    \item Exzellente Detektionsleistung mit mAP@0.5 von 98.0\% auf dem Validierungssatz
    \item 100\% Detektionsrate auf dem Testsatz (kein Symbol übersehen)
    \item 1 Klassifikationsfehler (GKS-Typ-Verwechslung) auf dem Testsatz
    \item Robuste Rotationsinvarianz: Steilrotierte Objekte ($|\theta|>30^\circ$) erreichen 
    100\% OCR-Erfolg bei höherer Konfidenz (0.946 vs. 0.890)
    \item Erfolgreiche Anforderungserfüllung FA-001 (Recall 95.7\% $>$ 90\%) und FA-002 (Rotationsinvarianz)
\end{itemize}

\textbf{Symbol-Text-Verknüpfung:}
\begin{itemize}
    \item Sehr hohe Linking-Genauigkeit: 642 von 644 Verknüpfungen korrekt (99.69\%)
    \item 2 Linking-Fehler aufgetreten:
    \begin{itemize}
        \item 1× GM-Koordinate an atypischer Position (rechts statt unterhalb)
        \item 1× Fahrtrichtung nicht abgeleitet (fehlende GKS-Detektion)
    \end{itemize}
    \item Fahrtrichtungsdetektion: 171 von 172 Signalen korrekt (99.42\%)
    \item Proximity-basierter Algorithmus robust für typische Layouts
\end{itemize}

\textbf{End-to-End Systemleistung:}
\begin{itemize}
    \item Gesamtgenauigkeit von \textbf{98.76\%} auf dem Testsatz übertrifft Zielwert von 85\% (NFA-003) deutlich
    \item Nur 8 von 644 Objekten (1.24\%) erforderten Korrektur
    \item Fehlerverteilung: 3 YOLO-bezogen (37.5\%), 3 OCR-bezogen (37.5\%), 2 Linking-bezogen (25.0\%)
    \item Stabile Leistung über alle Komplexitätsstufen (98.23\% -- 99.31\%)
    \item Für produktiven Einsatz auf diversen Plänen wird realistische Genauigkeit von 90--95\% erwartet
    \item Umfangreiche Validierungs- und Korrekturwerkzeuge reduzieren Prüfaufwand um 85\% durch gezielte Fehleridentifikation
\end{itemize}

\textbf{Verarbeitungseffizienz und Praxisnutzen:}
\begin{itemize}
    \item Durchschnittliche Verarbeitungszeit von \textbf{12.3 Minuten} pro A0-Plan auf Standard-CPU-Hardware
    \item Zeitverteilung der Pipeline: YOLO-Inferenz 75.3\%, OCR 10.3\%, Linking 6.8\%, Sonstige 7.7\%
    \item \textbf{75.3\% Zeitersparnis} gegenüber manuellem Prozess (64 min $\rightarrow$ 15.8 min pro Plan)
    \item Bei typischen Projekten mit 20--50 Plänen: 16--40 Stunden (2--5 Arbeitstage) Einsparung
    \item Anforderungen NFA-006 (Prozessoptimierung) und NFA-007 (Ressourceneffizienz) erfüllt
\end{itemize}

Die Evaluation bestätigt, dass das entwickelte System alle definierten funktionalen und nicht-funktionalen Anforderungen erfüllt und die gesetzten Zielmetriken signifikant übertrifft. Der verbleibende manuelle Korrekturaufwand von geschätzt 5--10\% im produktiven Einsatz wird durch die integrierten Validierungswerkzeuge effizient adressiert. Die erzielte Zeitersparnis von 75\% transformiert den bisherigen manuellen Prozess zu einem KI-gestützten Workflow, der sowohl die Effizienz als auch die Konsistenz der Datenextraktion erheblich verbessert. Der primäre technische Optimierungspotenzial liegt in der OCR-Komponente, insbesondere bei der Erkennung von Koordinatenbeschriftungen unter ungünstigen Bedingungen (niedrige Auflösung, starke Rotation). Diese Aspekte werden in Kapitel~\ref{chap:diskussion} detailliert diskutiert.