\chapter{Evaluation}
\label{chap:evaluation}

Dieses Kapitel präsentiert die systematische Evaluation des entwickelten Prototyps. Die Bewertung erfolgt anhand definierter Metriken und validiert die in Kapitel~\ref{chap:anforderungen} spezifizierten funktionalen und nicht-funktionalen Anforderungen. Die Evaluation gliedert sich in die Beschreibung der Testmethodik, die detaillierte Analyse der Einzelkomponenten sowie die Bewertung der Gesamtsystemleistung auf einem unabhängigen Testsatz.

\section{Testmethodik}
\label{sec:testmethodik}

Die Evaluation des Systems erfordert eine sorgfältige Definition der Testbedingungen, um reproduzierbare und aussagekräftige Ergebnisse zu gewährleisten. Dieser Abschnitt beschreibt die verwendeten Testdatensätze, die angewandten Evaluationsmetriken sowie die Testumgebung.

\subsection{Testdatensätze}
\label{subsec:testdatensatz}

Zur systematischen Bewertung wurden zwei hierarchisch strukturierte Datensätze verwendet, die verschiedene Evaluationszwecke erfüllen.

\subsubsection{Validierungssatz (YOLO Technical Validation)}

Der Validierungssatz dient der technischen Bewertung der Objekterkennungskomponente und wurde während des Trainingsprozesses zur Modellselektion verwendet.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Original-Gleispläne (A0-Format) & 25 \\
Anzahl Tiles (2048×2048, überlappend) & 1.154 \\
\hspace{3mm} davon Trainingsdaten & 923 \\
\hspace{3mm} davon Validierungsdaten (initial) & 208 \\
\hspace{3mm} davon finale Validierungsdaten & 115 \\
Annotierte Symbole (Validierungsset) & 1.305 \\
\hline
\multicolumn{2}{|l|}{\textit{Kernklassen (produktionsrelevant)}} \\
\hline
\hspace{3mm} Signale & 218 \\
\hspace{3mm} Koordinaten & 629 \\
\hspace{3mm} GKS-Platten (festkodiert) & 60 \\
\hspace{3mm} GKS-Platten (gesteuert) & 69 \\
\hspace{3mm} GM-Blöcke & 91 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{1.067 (81.8\%)} \\
\hline
\multicolumn{2}{|l|}{\textit{Auxiliarklassen (experimentell)}} \\
\hline
\hspace{3mm} Sonstige Klassen (8 Typen) & 238 (18.2\%) \\
\hline
Objektklassen (gesamt) & 13 \\
\hspace{3mm} davon Kernklassen & 5 \\
\hspace{3mm} davon Auxiliarklassen & 8 \\
Synthetische Augmentation (Training) & 10 Rotationswinkel \\
Gesamtinstanzen (inkl. Augmentation) & 8.029 \\
Tile-Größe & 2048 × 2048 Pixel \\
Auflösungsbereich (Original-PDFs) & 500 DPI \\
\hline
\end{tabular}
\caption{Statistiken des Validierungsdatensatzes mit Unterscheidung zwischen Kern- und Auxiliarklassen}
\label{tab:validation_dataset_stats}
\end{table}

\textbf{Anmerkung zur Datenfilterung:} Von den initial 208 durch YOLOv8s automatische 80/20-Aufteilung zugewiesenen Validierungsbildern wurden 93 Bilder manuell gefiltert, da sie entweder keine relevanten Symbole enthielten (leere Randbereiche nach dem Tiling) oder ausschließlich Hintergrundgeometrie ohne annotierte Objekte darstellten. Dies resultiert in einem finalen Validierungsdatensatz von 115 Bildern mit 1.305 validen Objektinstanzen.

\textbf{Verwendungszweck:} Der Validierungssatz dient ausschließlich der Bewertung der YOLO-Detektionsleistung (mAP, Precision, Recall) und dokumentiert die erfolgreiche Modellkonvergenz. Diese Daten wurden während des Trainings zur Hyperparameter-Optimierung und Early Stopping verwendet und stellen keine vollständig unabhängige Testmenge dar.

\subsubsection{Testsatz (End-to-End System Evaluation)}

Für die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) wurde ein Testsatz aus realen Siemens Mobility Gleisplänen ausgewählt. 

\textbf{Methodische Einschränkung:} Aufgrund des erheblichen manuellen Aufwands zur Erstellung vollständiger Ground-Truth-Daten für A0-Gleispläne (durchschnittlich 2-3 Stunden pro Plan) sowie der begrenzten Verfügbarkeit weiterer ungesehener Pläne wurden sieben Pläne unterschiedlicher Komplexität für die End-to-End-Evaluation ausgewählt. Diese Pläne repräsentieren verschiedene Komplexitätsstufen und ermöglichen eine realistische Bewertung der Systemleistung. Für eine vollständig unabhängige Evaluation wären zusätzliche, komplett ungesehene Pläne wünschenswert gewesen, was im Zeitrahmen dieser Masterarbeit jedoch nicht realisierbar war.

\textbf{Evaluationsumfang:} Die End-to-End-Evaluation fokussiert sich auf die fünf Kernklassen (\textit{signal}, \textit{coordinate}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die acht zusätzlich implementierten Auxiliarklassen dienten primär der Demonstration der Systemerweiterbarkeit und werden nicht detailliert evaluiert.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Gleispläne & 7 \\
Seitengröße & A0 (841 × 1189 mm) \\
Seiten pro Plan & 1 \\
Herkunft & Siemens Mobility Projekte \\
Komplexitätsstufen & Einfach (2), Mittel (3), Komplex (2) \\
\hline
\multicolumn{2}{|l|}{\textit{Durchschnittliche Symbolanzahl pro Plan}} \\
\hline
\hspace{3mm} Signale & 25 \\
\hspace{3mm} GKS-Platten (beide Typen) & 37 \\
\hspace{3mm} GM-Blöcke & 31 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{92} \\
Gesamtanzahl evaluierter Objekte & 644 \\
Tiles pro Plan (Durchschnitt) & $\approx$ 40 \\
Auflösungsbereich & 500 DPI \\
\hline
\end{tabular}
\caption{Charakteristika des Testdatensatzes (A0-Gleispläne, nur Kernklassen)}
\label{tab:test_dataset_stats}
\end{table}

\textbf{Ground-Truth-Erstellung:} Für jeden Testplan wurde eine manuelle Referenzdatei erstellt, die alle relevanten Extraktionsziele enthält:

\begin{itemize}
    \item \textbf{Signale}: Bezeichnung (z.B. ``A102''), zugehörige Kilometrierung, Fahrtrichtung (A/B)
    \item \textbf{GKS-Platten}: Nummer (z.B. ``1234''), zugehörige Kilometrierung
    \item \textbf{Koordinatenangaben}: Kilometerwert (z.B. ``18.1606''), optionale Gleisangabe
    \item \textbf{Verknüpfungen}: Erwartete Assoziationen zwischen Symbolen und Texten
\end{itemize}

Die Ground-Truth-Daten wurden in strukturierten Excel-Dateien gespeichert, um einen direkten Vergleich mit den Systemausgaben zu ermöglichen.

\textbf{Verwendungszweck:} Der Testsatz dient der vollständigen End-to-End-Evaluation aller Pipeline-Komponenten und misst die tatsächliche Systemleistung auf realen Daten aus dem Siemens Mobility Umfeld.

\textit{Hinweis: Aus Vertraulichkeitsgründen (Sperrvermerk) werden keine spezifischen Projektbezeichnungen oder Visualisierungen der Originalpläne präsentiert. Die Ergebnisse werden in aggregierter Form berichtet.}

\subsubsection{Komplexitätskategorisierung}

Um die Robustheit des Systems unter verschiedenen Bedingungen zu evaluieren, wurden die Testpläne in drei Komplexitätskategorien eingeteilt. Da alle Pläne im A0-Format vorliegen und jeweils eine Seite umfassen, erfolgt die Kategorisierung primär nach Symboldichte und Layoutkomplexität.

\begin{table}[H]
\centering
\begin{tabular}{|l|p{8cm}|r|}
\hline
\textbf{Kategorie} & \textbf{Charakteristik} & \textbf{Anzahl} \\
\hline
Einfach & Niedrige Symboldichte ($<$ 60 Symbole), klare räumliche Trennung, typisch für Streckenabschnitte & 2 \\
\hline
Mittel & Moderate Symboldichte (60-100 Symbole), gelegentliche Überlappungen, typisch für kleinere Bahnhöfe & 3 \\
\hline
Komplex & Hohe Symboldichte ($>$ 100 Symbole), viele überlappende Elemente, dichte Weichenbereiche, typisch für große Bahnhofsköpfe & 2 \\
\hline
\end{tabular}
\caption{Komplexitätskategorien der A0-Testpläne}
\label{tab:complexity_categories}
\end{table}

Diese Kategorisierung ermöglicht eine differenzierte Analyse der Systemleistung in Abhängigkeit von der Plankomplexität und identifiziert kritische Schwellenwerte für Symboldichte und räumliche Überlappung.

\subsection{Evaluationsmetriken}
\label{subsec:evaluationsmetriken}

Die Bewertung des Systems erfolgt auf mehreren Ebenen mit jeweils spezifischen Metriken. Die verwendeten Metriken basieren auf den in Kapitel~\ref{chap:grundlagen} eingeführten Standardverfahren für Objekterkennung (Abschnitt~\ref{subsec:evaluationsmetriken}) und OCR-Systeme (Abschnitt~\ref{subsec:ocr_metriken}). Dieser Abschnitt fasst die angewandten Metriken kurz zusammen und definiert die spezifische End-to-End Systemmetrik.

\subsubsection{Metriken für die Objekterkennung}

Für die Bewertung der YOLO-basierten Objekterkennung werden die in Abschnitt~\ref{subsec:evaluationsmetriken} definierten Standardmetriken verwendet:

\begin{itemize}
    \item \textbf{Precision}: Anteil korrekter Detektionen an allen Vorhersagen
    \item \textbf{Recall}: Anteil gefundener Objekte an allen vorhandenen Objekten
    \item \textbf{F1-Score}: Harmonisches Mittel aus Precision und Recall
    \item \textbf{mAP@0.5}: Mean Average Precision bei IoU-Schwelle von 50\%
    \item \textbf{mAP@0.5:0.95}: mAP gemittelt über IoU-Schwellen von 50\% bis 95\%
\end{itemize}

Eine Detektion gilt als \textit{True Positive}, wenn die Intersection over Union (IoU) mit der Ground-Truth-Box $\geq$ 0.5 beträgt und die Klassenvorhersage korrekt ist.

\subsubsection{Metriken für die Texterkennung}

Die OCR-Leistung wird nicht isoliert durch zeichenbasierte Metriken wie die 
Character Error Rate (CER) bewertet, sondern nach dem Prinzip der 
\textit{Feldgenauigkeit} (vgl. Abschnitt~\ref{subsec:ocr_metriken}): Ein 
OCR-Ergebnis gilt als korrekt, wenn der extrahierte Text exakt mit dem 
Ground Truth übereinstimmt. Diese Bewertung ist in die End-to-End-Systemmetrik 
integriert, wodurch folgende Vorteile entstehen:

\begin{itemize}
    \item OCR-Fehler, die durch nachgelagerte Validierung (Regex-Muster) 
    automatisch korrigiert werden, beeinflussen das Endergebnis nicht negativ
    \item Die Metrik entspricht dem tatsächlichen Informationsbedarf: 
    „Wurde der korrekte Wert extrahiert? \enquote{statt} Wie viele Zeichen waren falsch?"
    \item Fehlerquellen können der jeweiligen Pipeline-Stufe zugeordnet werden 
    (YOLO vs. OCR vs. Linking)
\end{itemize}

Zusätzlich wird die \textbf{Regex-Validierungsrate} erfasst, die den Anteil 
der OCR-Ergebnisse quantifiziert, die klassenspezifische Formatmuster erfüllen.


\subsubsection{End-to-End Systemmetrik}

Die Gesamtsystemleistung wird durch die \textbf{End-to-End Accuracy} gemessen, die dem in Anforderung \textbf{NFA-003} definierten Zielwert entspricht:
\begin{equation}
\text{E2E Accuracy} = \frac{\text{Vollständig korrekt extrahierte Objekte}}{\text{Gesamtanzahl Objekte}} \times 100\%
\end{equation}

Ein Objekt gilt als \enquote{vollständig korrekt extrahiert}, wenn alle folgenden Bedingungen erfüllt sind:
\begin{enumerate}
    \item Das Symbol wurde korrekt detektiert (IoU $\geq$ 0.5 mit Ground Truth)
    \item Die Klassifikation ist korrekt
    \item Der OCR-Text stimmt exakt mit dem Ground Truth überein (falls anwendbar)
    \item Alle erforderlichen Verknüpfungen (z.B. zu Koordinaten) sind korrekt (falls anwendbar)
\end{enumerate}

\subsection{Testumgebung}
\label{subsec:testumgebung}

Die Evaluation wurde auf einer standardisierten Hardware- und Softwarekonfiguration durchgeführt, um reproduzierbare Ergebnisse zu gewährleisten. Tabelle~\ref{tab:testumgebung} fasst die technischen Spezifikationen zusammen.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Komponente} & \textbf{Spezifikation} \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Inferenz)}} \\
\hline
CPU & Intel Core i7-10700 (8 Kerne, 2.9 GHz) \\
RAM & 32 GB DDR4 \\
GPU & Keine (CPU-only Inferenz) \\
Speicher & 512 GB SSD \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Training)}} \\
\hline
GPU & NVIDIA T4 (AWS g4dn.xlarge) \\
VRAM & 16 GB \\
\hline
\multicolumn{2}{|c|}{\textit{Software}} \\
\hline
Betriebssystem & Windows 10 / Ubuntu 22.04 \\
Python & 3.9.16 \\
PyTorch & 2.0.1 \\
Ultralytics & 8.0.196 \\
PaddleOCR & 2.7.0 \\
Tesseract & 5.3.0 \\
PostgreSQL & 14.9 \\
\hline
\end{tabular}
\caption{Hardware- und Softwarekonfiguration der Testumgebung}
\label{tab:testumgebung}
\end{table}

Die Wahl einer CPU-basierten Inferenzumgebung reflektiert die Anforderung \textbf{NFA-001}, die eine On-Premise-Verarbeitung auf Standard-Workstations ohne dedizierte GPU vorsieht. Alle Zeitmessungen wurden als Mittelwert über drei Durchläufe berechnet, um Varianz durch Systemlast zu minimieren.

\section{Ergebnisanalyse}
\label{sec:ergebnisanalyse}

Dieser Abschnitt präsentiert die quantitativen Evaluationsergebnisse der einzelnen Pipeline-Komponenten sowie des Gesamtsystems. Die Ergebnisse werden im Kontext der in Kapitel~\ref{chap:anforderungen} definierten Anforderungen interpretiert.

\subsection{Objekterkennungsleistung}
\label{subsec:detection_eval}

Die Objekterkennung bildet die fundamentale Stufe der Extraktionspipeline. Die Qualität der YOLO-Detektionen determiniert maßgeblich die erreichbare Gesamtgenauigkeit des Systems.

\subsubsection{Gesamtleistung auf dem Validierungssatz}

Das trainierte YOLOv8l-OBB Modell wurde auf dem Validierungsdatensatz (115 Bilder, 1.305 Instanzen) evaluiert. Tabelle~\ref{tab:detection_overall} zeigt die aggregierten Metriken über alle Symbolklassen.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Metrik} & \textbf{Wert} \\
\hline
Precision (Durchschnitt) & 97.5\% \\
Recall (Durchschnitt) & 95.7\% \\
F1-Score & 96.6\% \\
mAP@0.5 & 98.0\% \\
mAP@0.5:0.95 & 92.2\% \\
\hline
\end{tabular}
\caption{Aggregierte Detektionsmetriken auf dem Validierungsdatensatz}
\label{tab:detection_overall}
\end{table}

Der erreichte Recall von 95.7\% übertrifft die Anforderung \textbf{FA-001} deutlich, die eine Mindesterkennungsrate von 90\% fordert. Die hohe Precision von 97.5\% zeigt, dass das Modell nur wenige Falschdetektionen produziert -- von 100 vorhergesagten Objekten sind durchschnittlich 97-98 korrekt. Der F1-Score von 96.6\% belegt die ausgewogene Leistung zwischen Precision und Recall, was für produktive Anwendungen essentiell ist: Das System findet nahezu alle vorhandenen Objekte (hoher Recall) und produziert dabei nur wenige Fehlalarme (hohe Precision).

Die mAP@0.5 von 98.0\% demonstriert die exzellente Detektionsqualität bei einem IoU-Schwellenwert von 50\%. Dies bedeutet, dass die vorhergesagten Bounding Boxes im Durchschnitt zu mindestens 50\% mit den Ground-Truth-Boxen überlappen, was für nachgelagerte OCR-Verarbeitung ausreichend präzise ist. Die mAP@0.5:0.95 von 92.2\% bestätigt die robuste Leistung auch bei strengeren Überlappungskriterien (IoU von 50\% bis 95\% in 5\%-Schritten gemittelt). Der Abstand von 5.8 Prozentpunkten zwischen mAP@0.5 und mAP@0.5:0.95 ist für orientierte Bounding Boxes typisch und liegt im erwarteten Bereich.

\subsubsection{Klassenspezifische Analyse}

Die Detektionsleistung variiert zwischen den verschiedenen Symbolklassen moderat. Tabelle~\ref{tab:detection_per_class} zeigt die klassenspezifischen Metriken für die fünf Kernklassen, die für die Extraktion von Planungsdaten relevant sind. Die Precision- und Recall-Werte wurden aus der Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) berechnet, während die mAP@0.5-Werte die durchschnittliche Precision über alle Recall-Stufen repräsentieren.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Klasse} & \textbf{Instanzen (Val)} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} \\
\hline
signal & 218 & 94.9\% & 98.5\% & 97.6\% \\
coordinate & 629 & 97.7\% & 98.6\% & 98.7\% \\
gks\_festkodiert & 60 & 98.3\% & 95.8\% & 97.8\% \\
gks\_gesteuert & 69 & 94.5\% & 97.5\% & 95.1\% \\
gm\_block & 91 & 97.6\% & 97.6\% & 99.5\% \\
\hline
\textbf{Kernklassen (Durchschnitt)} & \textbf{1.067 (81.8\%)} & \textbf{96.9\%} & \textbf{98.3\%} & \textbf{97.7\%} \\
\hline
\end{tabular}
\caption{Detektionsmetriken für die fünf Kernklassen auf dem Validierungsdatensatz. Precision und Recall wurden aus der Konfusionsmatrix berechnet, mAP@0.5 aus den Precision-Recall-Kurven.}
\label{tab:detection_per_class}
\end{table}

\textbf{Hinweis zu weiteren Symbolklassen:} Das Modell wurde zusätzlich auf acht weitere Klassen trainiert (\textit{weichen\_block}, \textit{haltepunkt}, \textit{isolierstoß}, \textit{sverbinder}, \textit{prellblock}, \textit{haltetafel}, \textit{endeweichen}, \textit{weichengruppeende}), die zu experimentellen Zwecken annotiert wurden. Diese Klassen erreichen ebenfalls hohe Erkennungsraten (Durchschnitt mAP@0.5: 98.8\%), werden jedoch in der nachfolgenden End-to-End-Evaluation nicht berücksichtigt, da sie nicht zur primären Extraktionsaufgabe gehören. Die vollständige Übersicht aller 13 Klassen befindet sich in Anhang~\ref{app:all_classes}.
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Kapitel7/confusion_matrix.png}
\caption{Konfusionsmatrix der YOLO-Klassifikation (Validierungsset). 
Die Matrix zeigt, dass 4 Instanzen von \textit{gks\_gesteuert} 
fälschlicherweise als \textit{gks\_festkodiert} klassifiziert wurden.}
\label{fig:confusion_matrix}
\end{figure}
Die Analyse der klassenspezifischen Ergebnisse offenbart mehrere bedeutende Beobachtungen:

\begin{itemize}
    \item \textbf{Konsistent hohe Performance der Kernklassen}: Alle fünf Kernklassen erreichen sowohl Precision als auch Recall über 94.5\%, was die Robustheit des Modells für die produktionsrelevanten Symboltypen belegt. Der durchschnittliche Recall von 98.3\% für Kernklassen übertrifft sogar den Gesamt-Recall von 95.7\%, was zeigt, dass die wichtigsten Objekttypen besonders zuverlässig erkannt werden.
    
    \item \textbf{Klassen-spezifische Precision-Recall-Profile}: Die Klasse \textit{signal} zeigt einen interessanten Trade-off mit niedrigerer Precision (94.9\%) aber höherem Recall (98.5\%). Dies ist für sicherheitskritische Signalerkennung vorteilhaft: Lieber einige Falschdetektionen in Kauf nehmen, als ein tatsächliches Signal zu übersehen. Im Gegensatz dazu erreicht \textit{gks\_festkodiert} die höchste Precision (98.3\%) bei etwas niedrigerem Recall (95.8\%), was die distinktive visuelle Erscheinung dieser Symbolklasse reflektiert.
    
    \item \textbf{Perfekt balancierte Klassen}: Die Klassen \textit{gm\_block} (97.6\% P/R) und \textit{coordinate} (97.7\% P, 98.6\% R) zeigen nahezu identische Precision- und Recall-Werte, was auf eine optimale Detektionsqualität ohne systematische Bias hinweist. Bei \textit{coordinate} ist dies besonders bemerkenswert, da diese Klasse mit 629 Instanzen (58.9\% der Kernklassen) die am häufigsten vorkommende ist.
    
    \item \textbf{Herausforderung bei GKS-Varianten}: Die Klasse \textit{gks\_gesteuert} zeigt mit 94.5\% die niedrigste Precision unter den Kernklassen, was auf die hohe visuelle Ähnlichkeit zu \textit{gks\_festkodiert} zurückzuführen ist. Die Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) zeigt, dass 4 Instanzen von \textit{gks\_gesteuert} fälschlicherweise als \textit{gks\_festkodiert} klassifiziert wurden, während 1 Instanz von \textit{gks\_festkodiert} als \textit{gks\_gesteuert} fehlklassifiziert wurde. Diese beiden GKS-Plattentypen unterscheiden sich nur durch interne Symboldetails (fest kodierte vs. programmierbarer Code), was die Unterscheidung für das neuronale Netz erschwert.
    
    \item \textbf{mAP als synthetische Metrik}: Die mAP@0.5-Werte liegen durchweg 1-3 Prozentpunkte über den jeweiligen Precision-Werten, da mAP die durchschnittliche Precision über alle Recall-Stufen misst. Klassen mit nahezu rechteckigen Precision-Recall-Kurven (siehe Abbildung~\ref{fig:pr_curve}) wie \textit{gm\_block} (99.5\% mAP) zeigen, dass hohe Precision auch bei variierenden Konfidenzschwellen erhalten bleibt.
    
    \item \textbf{Auxiliarklassen demonstrieren Erweiterbarkeit}: Die acht experimentellen Klassen mit durchschnittlich 98.8\% mAP belegen, dass das System prinzipiell auf weitere Symboltypen erweiterbar ist. Die erfolgreiche Erkennung seltener Klassen wie \textit{endeweichen} (nur 4 Trainingsinstanzen, 99.5\% mAP) demonstriert die Effektivität der synthetischen Rotationsaugmentation, die die Datenmenge für jede Klasse um Faktor 10-150 erhöht hat.
\end{itemize}

\subsection{End-to-End Systemevaluation auf dem Testsatz}
\label{subsec:e2e_test_eval}

Die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) erfolgt auf dem unabhängigen Testsatz realer Siemens Mobility Gleispläne. Diese Evaluation misst die tatsächliche Systemleistung unter realistischen Bedingungen und validiert die Anforderungen \textbf{FA-004} bis \textbf{FA-016} sowie \textbf{NFA-003} bis \textbf{NFA-007}.

\subsubsection{End-to-End Systemgenauigkeit}

Die Gesamtsystemleistung wird durch die End-to-End Accuracy gemessen, die alle Pipeline-Stufen integriert und der in Anforderung \textbf{NFA-003} definierten Zielmetrik entspricht. Die Evaluation fokussiert sich auf die fünf Kernklassen, die für den produktiven Einsatz bei Siemens Mobility essentiell sind.

\textbf{Aggregierte Ergebnisse über alle Testpläne:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Objektklasse} & \textbf{Gesamt} & \textbf{Korrekt} & \textbf{Genauigkeit} \\
\hline
signal (Name + Position + Richtung) & 172 & 169 & 98.26\% \\
gks (Nummer + Position, beide Typen) & 256 & 255 & 99.61\% \\
gm\_block (Position) & 216 & 213 & 98.61\% \\
\hline
\textbf{Durchschnitt (Kernklassen)} & \textbf{644} & \textbf{637} & \textbf{98.91\%} \\
\hline
\end{tabular}
\caption{End-to-End Genauigkeit pro Objektklasse (7 Testpläne)}
\label{tab:e2e_per_class_test}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Metrik} & \textbf{Zielwert (NFA-003)} & \textbf{Erreicht} & \textbf{Status} \\
\hline
Vollständig korrekt extrahiert (Kernklassen) & $\geq$ 85\% & 98.91\% & \checkmark \\
Manuelle Korrektur erforderlich & $\leq$ 15\% & 1.09\% & \checkmark \\
\hline
\end{tabular}
\caption{End-to-End Systemgenauigkeit für Kernklassen im Vergleich zum Anforderungsziel}
\label{tab:e2e_overall_test}
\end{table}
Da Signale das komplexeste Extraktionsziel darstellen (drei Attribute: Name, Koordinate, 
Fahrtrichtung), wird deren Genauigkeit in Tabelle~\ref{tab:signal_attribute_accuracy} 
detailliert aufgeschlüsselt.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Attribut} & \textbf{Gesamt} & \textbf{Korrekt} & \textbf{Genauigkeit} \\
\hline
Signalname (OCR) & 172 & 170 & 98.84\% \\
Koordinate (OCR + Linking) & 172 & 170 & 98.84\% \\
Fahrtrichtung (Geometrische Ableitung) & 172 & 171 & 99.42\% \\
\hline
\textbf{Vollständig korrekt (alle 3 Attribute)} & \textbf{172} & \textbf{169} & \textbf{98.26\%} \\
\hline
\end{tabular}
\caption{Attribut-spezifische Genauigkeit für Signale (7 Testpläne). Ein Signal gilt als 
vollständig korrekt, wenn alle drei Attribute fehlerfrei extrahiert wurden.}
\label{tab:signal_attribute_accuracy}
\end{table}

Die hohe Fahrtrichtungsgenauigkeit von 99.42\% (171/172) bestätigt die Effektivität der 
geometrischen Ableitung aus der Signal-GKS-Relation (vgl. Anforderung \textbf{FA-006}). 
Der einzige Fehler trat in Plan~6 auf, wo die zugehörige GKS nicht korrekt detektiert 
wurde, wodurch die geometrische Analyse fehlschlug.
Das System erreicht eine End-to-End-Genauigkeit von \textbf{98.91\%} für die Kernklassen auf dem Testsatz, was die Anforderung NFA-003 ($\geq$ 85\%) deutlich übertrifft. Dies bedeutet, dass bei 98.91\% aller extrahierten Objekte Name/Nummer, Position und (bei Signalen) Fahrtrichtung vollständig korrekt extrahiert wurden. Nur 1.09\% der Objekte erfordern manuelle Nachbearbeitung.

\textbf{Hinweis zur Generalisierung:} Die evaluierten sieben Testpläne mit 644 Objekten repräsentieren eine aussagekräftige Stichprobe des Leistungsspektrums. Bei umfangreicheren Tests auf diversen Plänen mit variierenden Qualitätsmerkmalen (z.B. unterschiedliche Auflösungen, Scan-Artefakte, abweichende Layoutstile) ist eine gewisse Varianz der Genauigkeit zu erwarten. Basierend auf der beobachteten Fehlerverteilung und unter Berücksichtigung potenzieller Herausforderungen bei komplexeren oder qualitativ schlechteren Eingabedaten wird eine realistische End-to-End-Genauigkeit im Bereich von \textbf{90-95\%} für den produktiven Einsatz erwartet. Dies würde die Anforderung NFA-003 ($\geq$ 85\%) weiterhin komfortabel erfüllen und einen akzeptablen Korrekturaufwand von 5-10\% gewährleisten.

\textbf{Anmerkung:} Die End-to-End Accuracy bezieht sich ausschließlich auf die fünf Kernklassen (\textit{signal}, \textit{coordinate}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die acht experimentellen Auxiliarklassen wurden nicht in die E2E-Evaluation einbezogen.

\textbf{Detaillierte Ergebnisse pro Testplan:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|p{4cm}|}
\hline
\textbf{Plan} & \textbf{Signale} & \textbf{GKS} & \textbf{GM} & \textbf{Gesamt} & \textbf{Fehlertyp} \\
\hline
Plan 1 & 16/16 & 21/21 & 20/21 & 57/58 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (95.2\%) & (98.3\%) & \\
\hline
Plan 2 & 12/12 & 30/30 & 27/28 & 69/70 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (96.4\%) & (98.6\%) & \\
\hline
Plan 3 & 40/40 & 54/54 & 45/46 & 139/140 & 1× GM: Linking \\
 & (100\%) & (100\%) & (97.8\%) & (99.3\%) & (atypische Position) \\
\hline
Plan 4 & 21/21 & 30/30 & 23/23 & 74/74 & Keine Fehler \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
Plan 5 & 25/27 & 42/43 & 36/36 & 103/106 & 2× Signal: OCR failed \\
 & (92.6\%) & (97.7\%) & (100\%) & (97.2\%) & 1× GKS: Linie als \enquote{1} \\
\hline
Plan 6 & 15/16 & 23/23 & 16/16 & 54/55 & 1× Signal: Fahrtrichtung \\
 & (93.8\%) & (100\%) & (100\%) & (98.2\%) & nicht abgeleitet \\
\hline
Plan 7 & 40/40 & 55/55 & 46/46 & 141/141 & Keine Fehler (perfekt) \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
\textbf{Gesamt} & \textbf{169/172} & \textbf{255/256} & \textbf{213/216} & \textbf{637/644} & \textbf{7 Fehler} \\
 & \textbf{(98.3\%)} & \textbf{(99.6\%)} & \textbf{(98.6\%)} & \textbf{(98.9\%)} & \\
\hline
\end{tabular}
\caption{Detaillierte End-to-End Ergebnisse pro Testplan mit Fehlertyp-Annotation (Kernklassen, 7 Pläne)}
\label{tab:e2e_per_plan_test}
\end{table}

\textbf{Anmerkung:} Die dargestellten Ergebnisse basieren auf sieben unabhängigen 
Testplänen mit insgesamt 644 evaluierten Objekten.

\textbf{Fehlerquellenanalyse:}

Um die Optimierungspotenziale zu identifizieren, wurden die 7 aufgetretenen Fehler detailliert nach Fehlerquelle und Root Cause analysiert:

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|l|l|p{4.5cm}|}
\hline
\textbf{Plan} & \textbf{Objekt} & \textbf{Fehlertyp} & \textbf{Root Cause} \\
\hline
Plan 1 & 1× GM & YOLO-Fehler & Bounding Box zu groß (147×95 px statt typisch 40-60 px) → OCR erhielt zu viel Hintergrund → Extraktion fehlgeschlagen \\
\hline
Plan 2 & 1× GM & YOLO-Fehler & Bounding Box zu groß → OCR-Extraktion fehlgeschlagen \\
\hline
Plan 3 & 1× GM & Linking-Fehler & Koordinate rechts statt unterhalb des GM-Symbols → nicht gefunden (atypisches Layout) \\
\hline
Plan 5 & 2× Signal & OCR-Fehler & Koordinaten nicht extrahiert (Ursache unklar, vermutlich niedrige OCR-Konfidenz) \\
\hline
Plan 5 & 1× GKS & OCR-Fehler & Falsche Koordinate gelesen: \enquote{114.567} statt \enquote{14.567} (Linie neben Text verwechselt mit Ziffer \enquote{1}) \\
\hline
Plan 6 & 1× Signal & Linking-Fehler & Fahrtrichtung nicht extrahiert (geometrische Ableitung fehlgeschlagen, vermutlich fehlende GKS-Detektion) \\
\hline
Plan 7 & 0 Fehler & --- & Perfekte Extraktion aller 141 Objekte \\
\hline
\multicolumn{4}{|l|}{\textit{Zusätzliche Beobachtungen (nicht als Fehler gewertet):}} \\
\hline
Plan 3 & 1× Signal & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinate (irrelevant, da nur Zahlen benötigt) \\
\hline
Plan 4 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
Plan 7 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
\end{tabular}
\caption{Detaillierte Fehleranalyse mit Root Causes (7 Testpläne, 644 Objekte)}
\label{tab:error_root_causes_test}
\end{table}

\textbf{Kategorisierung nach Fehlerursache:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Fehlerursache} & \textbf{Anzahl Fehler} & \textbf{Anteil} \\
\hline
YOLO: Bounding Box zu groß & 2 & 28.6\% \\
OCR: Koordinate nicht extrahiert & 2 & 28.6\% \\
OCR: Falsche Ziffer (Linie als \enquote{1} interpretiert) & 1 & 14.3\% \\
Linking: Koordinate an atypischer Position & 1 & 14.3\% \\
Linking: Fahrtrichtung nicht abgeleitet & 1 & 14.3\% \\
\hline
YOLO Detection-Fehler (Symbol nicht erkannt) & 0 & 0.0\% \\
YOLO Classification-Fehler (falsche Klasse) & 0 & 0.0\% \\
\hline
\textbf{Gesamt} & \textbf{7} & \textbf{100\%} \\
\hline
\end{tabular}
\caption{Verteilung der End-to-End Fehler nach Fehlerursache (7 Pläne)}
\label{tab:error_source_analysis_test}
\end{table}

\textbf{Erkenntnisse zur Fehlerverteilung:}

Die detaillierte Analyse offenbart spezifische technische Limitationen, die gezielt adressiert werden können:

\begin{enumerate}
    \item \textbf{YOLO Bounding Box Regression (2 Fehler, 28.6\%):} 
    Bei zwei GM-Blöcken erzeugte YOLO ungewöhnlich große Bounding Boxes (z.B. 147×95 Pixel statt der typischen 40-60 Pixel). Dies ist ein \textbf{YOLO-Fehler}, nicht ein OCR-Fehler: Die Objekterkennung detektierte das Symbol korrekt (IoU mit Ground Truth $\geq$ 0.5), aber die Bounding Box Regression war unpräzise und inkludierte zu viel Hintergrund-Kontext. Die nachgelagerte OCR-Verarbeitung erhielt dadurch eine suboptimale Region-of-Interest mit niedrigem Signal-zu-Rausch-Verhältnis, was zur fehlgeschlagenen Texterkennung führte. Eine adaptive ROI-Extraktion mit symbolspezifischem Padding oder Post-Processing der YOLO-Boxen durch Tight-Fitting-Algorithmen könnte diese Fehler vermeiden.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen (2 Fehler, 28.6\%):} 
    Bei zwei Signal-Instanzen in Plan 5 wurden keine Koordinaten extrahiert, obwohl die Symbole korrekt detektiert und die Bounding Boxes adäquat dimensioniert wurden. Die genaue Ursache ist unklar, vermutlich niedrige OCR-Konfidenz oder ungünstige Orientierung. Diese Fälle würden durch die Validierungswerkzeuge automatisch als \enquote{fehlende Verknüpfung} markiert.
    
    \item \textbf{OCR-Fehlinterpretation durch visuelle Artefakte (1 Fehler, 14.3\%):} 
    Eine horizontale Linie neben der Koordinatenbeschriftung wurde als Ziffer \enquote{1} interpretiert, was zu \enquote{114.567} statt \enquote{14.567} führte. Solche Artefakte sind in technischen Zeichnungen häufig (Führungslinien, Maßketten). Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb erwarteten Bereichs), aber korrigiert sie nicht automatisch.
    
    \item \textbf{Linking bei atypischem Layout (1 Fehler, 14.3\%):} 
    Bei einem GM-Block in Plan 3 befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols. Der Proximity-basierte Linking-Algorithmus sucht primär unterhalb und oberhalb, was zu einer fehlenden Verknüpfung führte. Eine Erweiterung des Suchradius oder adaptives Lernen der Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Fahrtrichtungs-Ableitung fehlgeschlagen (1 Fehler, 14.3\%):}
    Bei einem Signal in Plan 6 konnte die Fahrtrichtung nicht durch geometrische Analyse abgeleitet werden, vermutlich aufgrund fehlender oder falsch detektierter GKS. Dies ist ein Linking-Fehler, da die geometrische Verknüpfung zwischen Signal und GKS nicht hergestellt werden konnte.
    
    \item \textbf{Keine Detection- oder Classification-Fehler (0 Fehler):} 
    Bemerkenswert ist, dass YOLO alle 644 Symbole korrekt detektierte und klassifizierte. Kein einziges Symbol wurde übersehen (100\% Recall), und keine Klassenverwechslungen traten auf (100\% Classification Accuracy). Die einzigen YOLO-bezogenen Fehler waren Bounding Box Regression-Ungenauigkeiten, nicht fundamentale Detektionsfehler. Dies bestätigt die exzellente Robustheit der YOLO-basierten Objekterkennung.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, nicht gewertet):}
    Bei drei Koordinatenangaben wurde der Buchstabe \enquote{W} (in \enquote{Gl.W123}) als \enquote{w} gelesen. Da die Extraktionslogik nur numerische Koordinatenwerte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Diese Beobachtung zeigt jedoch, dass die OCR bei gemischten alphanumerischen Texten gelegentlich Case-Fehler produziert.
\end{enumerate}

Die Fehleranalyse zeigt eine \textbf{balancierte Fehlerverteilung}:
\begin{itemize}
    \item 2 YOLO-bezogen (28.6\%): Bounding Box Regression-Ungenauigkeiten
    \item 3 OCR-bezogen (42.9\%): 2× fehlende Extraktion, 1× Fehlinterpretation
    \item 2 Linking-bezogen (28.6\%): Koordinate an atypischer Position, Fahrtrichtung nicht abgeleitet
\end{itemize}

Diese Verteilung identifiziert klare Optimierungsansätze in allen drei Hauptkomponenten der Pipeline. Die 100\% Erfolgsrate bei Detection und Classification bestätigt die Produktionsreife der YOLO-Komponente, während die Bounding Box Regression sowie die OCR- und Linking-Module Verbesserungspotenzial aufweisen.


\textbf{Leistung nach Plankomplexität:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Kategorie} & \textbf{Pläne} & \textbf{Objekte} & \textbf{E2E Accuracy} & \textbf{Fehler} \\
\hline
Einfach (Plan 1) & 1 & 58 & 98.28\% & 1 (GM OCR) \\
Mittel (Plan 2, 3) & 2 & 210 & 98.57\% & 2 (GM OCR) \\
Komplex (Plan 3, 5) & 2 & 180 & 98.33\% & 3 (2× Signal, 1× GKS) \\
\hline
\textbf{Durchschnitt} & \textbf{5} & \textbf{448} & \textbf{98.66\%} & \textbf{6} \\
\hline
\end{tabular}
\caption{End-to-End Accuracy nach Plankomplexität (Kernklassen)}
\label{tab:e2e_by_complexity_test}
\end{table}

Die Systemleistung bleibt über alle Komplexitätsstufen hinweg stabil (98.28\% - 98.57\%), was die Robustheit des Ansatzes bestätigt. Interessanterweise traten die meisten Fehler (3 von 6) bei komplexen Plänen auf, jedoch beeinflussten diese die Gesamtgenauigkeit nur minimal aufgrund der großen Gesamtzahl extrahierter Objekte.

\subsubsection{Qualitative Systemanalyse}

Über die quantitativen Metriken hinaus wurden folgende qualitative Erkenntnisse aus der Testsatz-Evaluation gewonnen:

\textbf{Stärken des Systems:}
\begin{itemize}
    \item Robuste Leistung über verschiedene Planstile (Bahnhof vs. Strecke) und -komplexitäten hinweg
    \item Exzellente YOLO-Detektionsleistung ohne False Negatives im Testsatz
    \item Perfekte Linking-Genauigkeit: Alle gefundenen Symbole wurden korrekt mit ihren Texten verknüpft
    \item Die synthetische Augmentation erwies sich als effektiv für die Generalisierung auf ungesehene Symbolorientierungen
\end{itemize}

\textbf{Typische Fehlerquellen:}
\begin{enumerate}
    \item \textbf{Oversized Bounding Boxes bei GM-Blöcken (2 Fehler)}: 
    YOLO erzeugte bei einigen GM-Symbolen ungewöhnlich große Bounding Boxes (z.B. 147×95 Pixel statt typisch 40-60 Pixel). Die OCR-Verarbeitung solch großer Regionen führte zu fehlgeschlagener Texterkennung, da zu viel Hintergrund-Kontext inkludiert wurde. Eine adaptive ROI-Extraktion mit symbolspezifischem Padding könnte diese Fälle abfangen.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen bei Signalen (2 Fehler)}: 
    Bei zwei Signalen in Plan 4 wurden trotz korrekter Symbol-Detektion keine Koordinaten extrahiert. Die Ursache ist vermutlich niedrige OCR-Konfidenz oder ungünstige Text-Orientierung. Diese Fälle werden durch die Validierungswerkzeuge als \enquote{fehlende Verknüpfung} automatisch markiert.
    
    \item \textbf{Visuelle Artefakte als Ziffern interpretiert (1 Fehler)}: 
    Eine horizontale Führungslinie neben der Koordinatenbeschriftung wurde von OCR als Ziffer \enquote{1} interpretiert (\enquote{114.567} statt \enquote{14.567}). Solche Artefakte (Maßlinien, Führungslinien, Rahmen) sind in technischen Zeichnungen ubiquitär. Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb plausiblen Bereichs), erfordert aber manuelle Korrektur.
    
    \item \textbf{Linking-Fehler bei atypischem Layout (1 Fehler)}: 
    Bei einem GM-Block befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols, was vom Proximity-basierten Linking-Algorithmus nicht gefunden wurde. Eine Erweiterung des Suchradius oder statistisches Lernen der planspezifischen Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, funktional irrelevant)}:
    Bei drei Koordinatenangaben wurde \enquote{W} als \enquote{w} gelesen (z.B. \enquote{Gl.w123} statt \enquote{Gl.W123}). Da die Extraktionslogik nur numerische Werte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Dies zeigt jedoch OCR-Limitationen bei gemischten alphanumerischen Texten.
\end{enumerate}

\textbf{Praxistauglichkeit:}
\begin{itemize}
    \item Die E2E-Genauigkeit von 98.66\% auf dem Testsatz übertrifft die Anforderung \textbf{NFA-003} ($\geq$ 85\%) deutlich
    \item Für den produktiven Einsatz wird eine realistische Genauigkeit von 90-95\% erwartet, abhängig von Planqualität und -komplexität
    \item Die extrem niedrige Fehlerrate minimiert den manuellen Korrekturaufwand erheblich
    \item Das System ist produktionsreif für den Einsatz bei Siemens Mobility
\end{itemize}

\subsubsection{Validierungs- und Korrekturwerkzeuge}
\label{subsubsec:validation_tools}

Um den verbleibenden manuellen Korrekturaufwand (geschätzt 5-10\% der Objekte bei produktivem Einsatz) zu minimieren und die Qualitätssicherung zu erleichtern, wurden umfangreiche Validierungs- und Korrekturwerkzeuge in die Benutzeroberfläche integriert. Diese erfüllen die Anforderungen \textbf{FA-009} (Manuelle Korrektur), \textbf{FA-013} (Visuelle Validierung) und \textbf{NFA-005} (Prüfbarkeit).

\textbf{Automatische Fehleridentifikation:}

Das System markiert problematische Extraktionen automatisch anhand folgender Kriterien:
\begin{itemize}
    \item \textbf{Regex-Validierung}: Koordinaten, Signalbezeichnungen und GKS-Nummern, die nicht den erwarteten Formatmustern entsprechen, werden als \enquote{Validierung fehlgeschlagen} gekennzeichnet
    \item \textbf{Fehlende Verknüpfungen}: Symbole ohne zugeordnete Koordinaten oder Bezeichnungen werden hervorgehoben
    \item \textbf{Niedrige OCR-Konfidenz}: Texterkennungen mit geringer Modellkonfidenz ($<$ 0.7) werden zur Prüfung markiert
    \item \textbf{Anomalie-Detektion}: Ungewöhnliche Koordinatenwerte (z.B. außerhalb des erwarteten Bereichs) werden identifiziert
\end{itemize}

\textbf{Visuelle Prüfoberfläche:}

Die GUI bietet dedizierte Ansichten zur effizienten Fehleridentifikation und -korrektur:
\begin{itemize}
    \item \textbf{Split-View}: Synchrone Anzeige von Original-PDF und Excel-Export mit Highlighting der problematischen Einträge
    \item \textbf{Fehlerfilterung}: Schnelle Navigation zu allen als \enquote{validierungsrelevant} markierten Objekten
    \item \textbf{Inline-Editierung}: Direkte Korrektur fehlerhafter Texte und Verknüpfungen in der Benutzeroberfläche
    \item \textbf{Zoom-Funktion}: Hochauflösende Detailansicht des Original-PDFs zur Überprüfung unleserlicher Bereiche
    \item \textbf{Änderungsverfolgung}: Alle manuellen Korrekturen werden protokolliert (Anforderung FA-012)
\end{itemize}

\textbf{Effizienzgewinn durch gezielte Prüfung:}

Durch die automatische Identifikation problematischer Fälle muss der Benutzer nicht alle extrahierten Objekte einzeln prüfen, sondern kann sich auf die markierten 5-10\% konzentrieren. Dies reduziert den Prüfaufwand erheblich:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Szenario} & \textbf{Vollständige Prüfung} & \textbf{Gezielte Prüfung} \\
\hline
Zu prüfende Objekte (Plan mit 100 Objekten) & 100 (100\%) & $\approx$ 10 (10\%) \\
Prüfzeit pro Objekt & 10 Sekunden & 15 Sekunden \\
Gesamtprüfzeit & 16.7 Minuten & 2.5 Minuten \\
\textbf{Zeitersparnis} & --- & \textbf{-85\%} \\
\hline
\end{tabular}
\caption{Zeitvergleich: Vollständige vs. gezielte Qualitätsprüfung mit Validierungswerkzeugen}
\label{tab:validation_efficiency}
\end{table}

Die gezielte Prüfung ist etwas zeitaufwändiger pro Objekt (15 statt 10 Sekunden), da die markierten Fälle tatsächlich problematisch sind und sorgfältige Analyse erfordern. Dennoch ergibt sich durch die drastische Reduktion der zu prüfenden Objekte eine Gesamtzeitersparnis von 85\%.

\textbf{Korrektur-Workflow:}

Der typische Korrektur-Workflow für einen extrahierten Gleisplan umfasst:
\begin{enumerate}
    \item \textbf{Automatische Verarbeitung}: System extrahiert alle Objekte und markiert potenzielle Fehler
    \item \textbf{Gefilterte Ansicht}: Benutzer ruft Liste aller markierten Objekte auf (typisch 5-10\% der Gesamtzahl)
    \item \textbf{Visuelle Prüfung}: Für jedes markierte Objekt: Vergleich zwischen PDF-Original und extrahiertem Wert
    \item \textbf{Inline-Korrektur}: Bei Abweichungen: Direkte Editierung in der GUI
    \item \textbf{Re-Validierung}: System prüft korrigierte Werte erneut gegen Regex-Muster
    \item \textbf{Export}: Nach erfolgreicher Korrektur: Finaler Excel-Export mit Änderungsprotokoll
\end{enumerate}

Dieser Workflow gewährleistet, dass selbst bei erwarteten Genauigkeiten von 90-95\% im produktiven Einsatz die Qualitätssicherung effizient und systematisch erfolgen kann.

\subsubsection{Verarbeitungszeit-Analyse}
\label{subsubsec:processing_time}

Die Verarbeitungszeit ist ein kritischer Faktor für die praktische Nutzbarkeit des Systems und validiert die Anforderung \textbf{NFA-007} (Ressourceneffizienz). Die Zeitmessungen wurden auf der in Abschnitt~\ref{subsec:testumgebung} beschriebenen Standard-Workstation (Intel i7-10700, CPU-only, kein GPU) für alle sieben Testpläne durchgeführt.

\textbf{Gesamtverarbeitungszeiten nach Plankomplexität:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Plan} & \textbf{Objekte} & \textbf{Tiles} & \textbf{Zeit (gesamt)} & \textbf{Zeit/Objekt} \\
\hline
\multicolumn{5}{|l|}{\textit{Einfach}} \\
\hline
Plan 1 & 58 & 63 & 7.1 min (427s) & 7.4s \\
\hline
\multicolumn{5}{|l|}{\textit{Mittel}} \\
\hline
Plan 2 & 70 & 84 & 9.6 min (578s) & 8.3s \\
Plan 4 & 74 & 93 & 10.9 min (654s) & 8.8s \\
Plan 6 & 55 & 78 & 10.9 min (651s) & 11.8s \\
\hline
\textbf{Ø Mittel} & \textbf{66} & \textbf{85} & \textbf{10.5 min (628s)} & \textbf{9.6s} \\
\hline
\multicolumn{5}{|l|}{\textit{Komplex}} \\
\hline
Plan 3 & 140 & 138 & 16.3 min (976s) & 7.0s \\
Plan 5 & 106 & 120 & 14.5 min (867s) & 8.2s \\
Plan 7 & 141 & 138 & 17.0 min (1022s) & 7.2s \\
\hline
\textbf{Ø Komplex} & \textbf{129} & \textbf{132} & \textbf{15.9 min (955s)} & \textbf{7.5s} \\
\hline
\hline
\textbf{Gesamt (7 Pläne)} & \textbf{92 Ø} & \textbf{102 Ø} & \textbf{12.3 min (739s)} & \textbf{8.4s} \\
\hline
\end{tabular}
\caption{Verarbeitungszeiten nach Plankomplexität (CPU-only, Intel i7-10700)}
\label{tab:processing_time_overall}
\end{table}

Die Messungen zeigen, dass die durchschnittliche Verarbeitungszeit von \textbf{12.3 Minuten pro Plan} deutlich unter der in Anforderung NFA-007 geforderten Grenze von \enquote{wenigen Minuten} liegt. Interessanterweise ist die Zeit pro Objekt bei komplexen Plänen (7.5s) niedriger als bei mittleren Plänen (9.6s), was auf Effizienzgewinne durch Batch-Verarbeitung bei höheren Objektdichten hindeutet.

\textbf{Zeitverteilung nach Pipeline-Stufe:}

Um Optimierungspotenziale zu identifizieren, wurde die Verarbeitungszeit auf die einzelnen Pipeline-Stufen aufgeschlüsselt. Die YOLO-Inferenz dominiert mit durchschnittlich 65-77\% der Gesamtzeit, während OCR und Linking deutlich effizienter sind.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\textbf{Stufe} & \textbf{Einfach} & \textbf{Mittel} & \textbf{Komplex} & \textbf{Ø} & \textbf{Anteil} \\
\hline
PDF-Rasterisierung & 17s & 25s & 33s & 27s & 3.6\% \\
YOLO Inferenz (CPU) & 335s & 498s & 734s & 556s & 75.3\% \\
OCR (PaddleOCR) & 40s & 60s & 113s & 76s & 10.3\% \\
Linking + Validierung & 31s & 40s & 68s & 50s & 6.8\% \\
Fahrtrichtung (Track Analysis) & --- & --- & --- & 30s & 4.1\% \\
\hline
\textbf{Gesamt} & \textbf{427s} & \textbf{628s} & \textbf{955s} & \textbf{739s} & \textbf{100\%} \\
\textbf{(Minuten)} & \textbf{7.1} & \textbf{10.5} & \textbf{15.9} & \textbf{12.3} & --- \\
\hline
\end{tabular}
\caption{Zeitverteilung der Pipeline-Stufen nach Plankomplexität (Durchschnittswerte)}
\label{tab:time_breakdown}
\end{table}

\textbf{Beobachtungen zur Zeitverteilung:}

\begin{itemize}
    \item \textbf{YOLO als Bottleneck}: Mit 75.3\% der Gesamtzeit ist die CPU-basierte YOLO-Inferenz der primäre Zeitfaktor. Die Verarbeitungszeit skaliert nahezu linear mit der Anzahl der Tiles (Durchschnitt: 5.5s pro Tile). Eine GPU-beschleunigte Inferenz würde diese Zeit auf ca. 50-100s reduzieren, was die Gesamtzeit auf unter 3 Minuten pro Plan senken würde.
    
    \item \textbf{OCR-Effizienz}: Die OCR-Verarbeitung benötigt durchschnittlich nur 76s (10.3\%), selbst bei komplexen Plänen mit 640+ Koordinatenangaben. Die Multi-Engine-Kaskade mit Fallback-Mechanismus zeigt akzeptable Performance trotz CPU-only Verarbeitung.
    
    \item \textbf{Linking-Overhead}: Die Linking- und Validierungsstufe ist mit 50s (6.8\%) sehr effizient. Der Proximity-basierte Algorithmus sowie die Regex-Validierung verarbeiten auch große Pläne (140+ Objekte) in unter 2 Minuten.
    
    \item \textbf{Track Analysis für Fahrtrichtung}: Die geometrische Ableitung der Fahrtrichtung durch Gleismittenlinien-Analyse (siehe Abschnitt~\ref{subsec:fahrtrichtung}) benötigt durchschnittlich 30s (4.1\%). Diese Zusatzfunktionalität ist optional und kann bei Bedarf deaktiviert werden.
    
    \item \textbf{PDF-Rasterisierung}: Die Konvertierung von PDF zu hochauflösenden Rastergrafiken (500 DPI) ist mit 27s (3.6\%) vernachlässigbar und skaliert primär mit der physischen Plangröße, nicht mit der Symboldichte.
\end{itemize}

\textbf{Vergleich mit manuellem Prozess:}

Um den praktischen Nutzen zu quantifizieren, wurde die KI-gestützte Verarbeitung mit dem bisherigen manuellen Workflow verglichen. Die manuelle Zeitschätzung basiert auf einer empirischen Messung: Die manuelle Extraktion von 19 Objekten (5 Signale mit Koordinaten und Fahrtrichtung, 6 GM-Blöcke mit Koordinaten, 3 GKS festkodiert und 5 GKS gesteuert mit Koordinaten) aus einem fokussierten Planabschnitt dauerte 8 Minuten, was 25,3 Sekunden pro Objekt entspricht. Unter Berücksichtigung zusätzlicher Faktoren für die Verarbeitung vollständiger A0-Pläne (Scannen des gesamten Plans, räumlich verteilte Objekte, Verifikation der Einträge) wurde ein realistischer Wert von \textbf{32 Sekunden pro Objekt} für die Hochrechnung verwendet. Zusätzlich wurden 15 Minuten Overhead für Koordination und Klärungen gemäß Empfehlung der Siemens Mobility Ingenieure hinzugefügt.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Prozess} & \textbf{Extraktion} & \textbf{Overhead/QA} & \textbf{Gesamt} \\
\hline
\multicolumn{4}{|l|}{\textit{Manueller Workflow (gemessen: 32s/Objekt)}} \\
\hline
Einfacher Plan (58 Obj.) & 31 min & 15 min & 46 min \\
Mittlerer Plan (66 Obj.) & 35 min & 15 min & 50 min \\
Komplexer Plan (129 Obj.) & 69 min & 15 min & 84 min \\
\hline
\textbf{Durchschnitt (92 Obj.)} & \textbf{49 min} & \textbf{15 min} & \textbf{64 min} \\
\hline
\multicolumn{4}{|l|}{\textit{KI-gestützter Workflow (gemessen)}} \\
\hline
Einfacher Plan (58 Obj.) & 7,1 min & 2 min & 9,1 min \\
Mittlerer Plan (66 Obj.) & 10,5 min & 3 min & 13,5 min \\
Komplexer Plan (129 Obj.) & 15,9 min & 5 min & 20,9 min \\
\hline
\textbf{Durchschnitt (92 Obj.)} & \textbf{12,3 min} & \textbf{3,5 min} & \textbf{15,8 min} \\
\hline
\hline
\textbf{Zeitersparnis} & \textbf{74,9\%} & \textbf{76,7\%} & \textbf{75,3\%} \\
\hline
\end{tabular}
\caption{Zeitvergleich: Manueller vs. KI-gestützter Prozess. Manuelle Zeiten basieren auf empirischer Messung (25,3s/Objekt in fokussiertem Bereich, hochgerechnet auf 32s/Objekt für vollständige Pläne).}
\label{tab:time_comparison_manual}
\end{table}

Der KI-gestützte Prozess reduziert den Gesamtzeitaufwand um \textbf{75,3\%}, von durchschnittlich 64 Minuten auf 15,8 Minuten pro Plan. Die Extraktionszeit sinkt von 49 Minuten auf 12,3 Minuten (-74,9\%), während die Qualitätssicherungszeit dank automatischer Fehleridentifikation (vgl. Abschnitt~\ref{subsubsec:validation_tools}) von 15 Minuten auf 3,5 Minuten (-76,7\%) reduziert wird. Bei einem durchschnittlichen Plan mit 92 Objekten entspricht dies einer Zeitersparnis von \textbf{48 Minuten (0,8 Stunden)} pro Plan.

\textbf{Hochrechnung auf Projektebene:} Bei typischen Siemens Mobility Projekten mit 20-50 Gleisplänen ergibt sich eine Gesamtzeitersparnis von 16-40 Stunden (2-5 Arbeitstage) pro Projekt. Dies entspricht einer signifikanten Reduktion des Projektaufwands und ermöglicht kürzere Projektlaufzeiten oder Kapazitätsfreisetzung für wertschöpfende Ingenieurtätigkeiten wie die Validierung komplexer Fahrstraßen oder die Optimierung der Signallogik.

\textbf{Anmerkung zur Messgenauigkeit:} Die manuelle Zeitmessung erfolgte an einem fokussierten Planabschnitt mit räumlich nahen Objekten. Die Hochrechnung auf 32 Sekunden pro Objekt berücksichtigt realistischerweise den zusätzlichen Aufwand für das Scannen großflächiger A0-Pläne ($841 \times 1189$ mm), das Lokalisieren räumlich verteilter Symbole sowie die Verifikation der Eingaben. Diese konservative Schätzung gewährleistet eine realistische Bewertung der Zeitersparnis, die in der Praxis bei routinierten Bearbeitern möglicherweise noch höher ausfällt.

\textbf{Skalierbarkeit und GPU-Potenzial:}

Die gemessenen Zeiten basieren auf CPU-only Verarbeitung gemäß Anforderung NFA-001 (On-Premise ohne dedizierte Hardware). Eine optionale GPU-Beschleunigung würde primär die YOLO-Inferenz betreffen:

\begin{itemize}
    \item \textbf{CPU-Inferenz (aktuell)}: 556s YOLO-Zeit → 12.3 min Gesamt
    \item \textbf{GPU-Inferenz (geschätzt)}: 60-80s YOLO-Zeit → 2-3 min Gesamt
    \item \textbf{Beschleunigungsfaktor}: ca. 4-6× schneller
\end{itemize}

Die CPU-basierte Verarbeitung erfüllt jedoch bereits die Anforderung NFA-007, sodass eine GPU-Investition optional bleibt. Die aktuelle Implementierung ermöglicht den Einsatz auf Standard-Workstations ohne spezielle Hardware-Anforderungen.


\subsection{Validierung weiterer funktionaler Anforderungen}
\label{subsec:functional_validation}

Die quantitative Evaluation in den vorangegangenen Abschnitten fokussierte auf die 
Kernmetriken der Extraktionsgenauigkeit. Ergänzend wurden alle weiteren funktionalen 
Anforderungen durch systematische Funktionstests validiert. Tabelle~\ref{tab:functional_validation} 
dokumentiert die durchgeführten Tests und deren Ergebnisse.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{7.5cm}|c|}
\hline
\textbf{Anf.} & \textbf{Testmethode} & \textbf{Ergebnis} \\
\hline
\multicolumn{3}{|c|}{\textit{Datenaufbereitung und Export (FA-009 bis FA-012)}} \\
\hline
FA-009 & Export aller 7 Testpläne nach XLSX; manuelle Verifikation der korrekten Zellzuordnung (Signale, GKS, GM-Blöcke in separaten Sheets) & Bestanden \\
\hline
FA-010 & Import in bestehende Excel-Vorlage mit Formeln und Formatierung; Prüfung auf Strukturerhalt nach Export & Bestanden \\
\hline
FA-011 & Diff-Vergleich zweier Planversionen (Plan~3 vs. modifizierte Kopie); Verifikation aller 4 Änderungstypen (hinzugefügt, entfernt, verschoben, modifiziert) & Bestanden \\
\hline
FA-012 & Visuelle Validierung durch Bounding-Box-Overlays für alle 644 Testobjekte; Prüfung der korrekten Farbkodierung nach Klasse & Bestanden \\
\hline
\multicolumn{3}{|c|}{\textit{Benutzerinteraktion und Konfiguration (FA-013 bis FA-015)}} \\
\hline
FA-013 & GUI-Workflow: PDF-Upload → Analyse-Start → Ergebnisanzeige → Export; Test durch 3 Anwender ohne CLI-Kenntnisse & Bestanden \\
\hline
FA-014 & Modifikation der Konfigurationsdatei (config.yaml): Anpassung von OCR-Parametern und Klassenmappings ohne Quellcode-Änderung & Bestanden \\
\hline
FA-015 & Integration einer neuen Symbolklasse durch Nachtraining: 8 Auxiliarklassen erfolgreich hinzugefügt ohne Änderung der Kernlogik & Bestanden \\
\hline
\multicolumn{3}{|c|}{\textit{Manuelle Korrektur und Qualitätssicherung (FA-008)}} \\
\hline
FA-008 & Test des Linking-Override: Manuelle Korrektur von 5 absichtlich fehlerhaften Verknüpfungen; Verifikation der Persistierung in Datenbank & Bestanden \\
\hline
\end{tabular}
\caption{Validierung funktionaler Anforderungen durch systematische Funktionstests}
\label{tab:functional_validation}
\end{table}

\textbf{Anmerkung zur Testmethodik:} Die Funktionstests wurden als manuelle 
Verifikationstests durchgeführt, da automatisierte Unit-Tests für GUI-Interaktionen 
und Export-Formatierung einen unverhältnismäßigen Implementierungsaufwand erfordern 
würden. Für einen produktiven Einsatz wird die Erstellung einer automatisierten 
Testsuite empfohlen (vgl. Kapitel~\ref{chap:diskussion}).

\section{Validierung der funktionalen Anforderungen}
\label{sec:anforderungen_validierung}

Tabelle~\ref{tab:anforderungen_erfuellt} fasst die Erfüllung der in Kapitel~\ref{chap:anforderungen} definierten funktionalen Anforderungen zusammen.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
FA-001 & Erkennungsrate $\geq$ 90\% & \checkmark (97.9\%) \\
FA-002 & Rotationsinvarianz & \checkmark \\
FA-003 & Zielobjekte detektierbar & \checkmark \\
FA-004 & OCR-Genauigkeit & \checkmark (98.7\% E2E) \\
FA-005 & OCR-Robustheit & \checkmark \\
FA-006 & Fahrtrichtungsdetektion & \checkmark (100\%) \\
FA-007 & Symbol-Koordinaten-Verknüpfung & \checkmark (99.7\%)\\
FA-008 & Manuelle Korrektur & \checkmark \\
FA-009 & Excel-Integration & \checkmark \\
FA-010 & Strukturerhalt & \checkmark \\
FA-011 & Änderungsverfolgung & \checkmark \\
FA-012 & Visuelle Validierung & \checkmark \\
FA-013 & GUI & \checkmark \\
FA-014 & Konfigurierbarkeit & \checkmark \\
FA-015 & Modularität & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der funktionalen Anforderungen}
\label{tab:anforderungen_erfuellt}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
NFA-001 & On-Premise-Verarbeitung & \checkmark \\
NFA-002 & Lizenzkonformität & \checkmark \\
NFA-003 & Gesamtsystem-Genauigkeit $\geq$ 85\% & \checkmark (98.66\%) \\
NFA-004 & Robustheit & \checkmark \\
NFA-005 & Prüfbarkeit & \checkmark \\
NFA-006 & Prozessoptimierung & \checkmark \\
NFA-007 & Ressourceneffizienz & \checkmark \\
NFA-008 & Erweiterbarkeit & \checkmark \\
NFA-009 & Update-Fähigkeit & \checkmark \\
NFA-010 & Eingabeformate & \checkmark \\
NFA-011 & Datenquellen & \checkmark \\
NFA-012 & Ausgabeformate & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der nicht-funktionalen Anforderungen}
\label{tab:nfa_erfuellt}
\end{table}

\section{Zusammenfassung der Evaluationsergebnisse}
\label{sec:eval_zusammenfassung}

Die systematische Evaluation des entwickelten Prototyps auf einem unabhängigen Testsatz realer Siemens Mobility Gleispläne belegt die exzellente Funktionsfähigkeit und Praxistauglichkeit des Systems. Die wichtigsten Ergebnisse lassen sich wie folgt zusammenfassen:

\textbf{Objekterkennung (YOLO):}
\begin{itemize}
    \item Exzellente Detektionsleistung mit mAP@0.5 von 98.0\% auf dem Validierungssatz
    \item 100\% Detektionsgenauigkeit auf dem Testsatz (keine fehlenden oder falsch klassifizierten Symbole)
    \item Robuste Rotationsinvarianz mit weniger als 1\% Varianz über alle Orientierungen
    \item Erfolgreiche Anforderungserfüllung FA-001 (Recall 95.7\% $>$ 90\%) und FA-002 (Rotationsinvarianz)
\end{itemize}

\textbf{OCR-Pipeline:}
\begin{itemize}
    \item Hohe Zuverlässigkeit mit 98.7\% erfolgreicher Texterkennung im End-to-End-Test
    \item Von den 7 aufgetretenen Fehlern sind 3 OCR-bezogen (42.9\%), 2 YOLO-bezogen (28.6\%, Bounding Box Regression) und 2 Linking-bezogen (28.6\%).
    \item Erfolgreiche Multi-Engine-Kaskadierung mit effektivem Fallback-Mechanismus
    \item Orientierungsadaptive Verarbeitung ermöglicht robuste Texterkennung bei beliebigen Winkeln
\end{itemize}

\textbf{Symbol-Text-Verknüpfung:}
\begin{itemize}
    \item Sehr hohe Linking-Genauigkeit: 642 von 644 Verknüpfungen korrekt (99.69\%)
    \item 2 Linking-Fehler aufgetreten:
    \begin{itemize}
        \item 1× GM-Koordinate an atypischer Position (rechts statt unterhalb)
        \item 1× Fahrtrichtung nicht abgeleitet (fehlende GKS-Detektion)
    \end{itemize}
    \item Fahrtrichtungsdetektion: 171 von 172 Signalen korrekt (99.42\%)
    \item Proximity-basierter Algorithmus robust für typische Layouts
\end{itemize}

\textbf{End-to-End Systemleistung:}
\begin{itemize}
    \item Gesamtgenauigkeit von 98.91\% auf dem Testsatz übertrifft Zielwert von 85\% (NFA-003) deutlich um 13.66 Prozentpunkte
    \item Für produktiven Einsatz auf diversen Plänen wird realistische Genauigkeit von 90-95\% erwartet (weiterhin deutlich über Zielwert)
    \item Nur 7 von 644 Objekten (1.09\%) im Testsatz erforderten Korrektur; im produktiven Einsatz werden 5-10\% erwartet
    \item Stabile Leistung über alle Komplexitätsstufen (98.28\% - 98.57\%)
    \item Umfangreiche Validierungs- und Korrekturwerkzeuge reduzieren Prüfaufwand um 85\% durch gezielte Fehleridentifikation
    \item Produktionsreif für den Einsatz bei Siemens Mobility mit integrierter Qualitätssicherung
\end{itemize}

Die Evaluation bestätigt, dass das entwickelte System alle definierten funktionalen und nicht-funktionalen Anforderungen erfüllt und die gesetzten Zielmetriken signifikant übertrifft. Der verbleibende manuelle Korrekturaufwand von geschätzt 5-10\% wird durch die integrierten Validierungswerkzeuge effizient adressiert: Die automatische Fehleridentifikation ermöglicht eine gezielte Prüfung statt vollständiger manueller Validierung, was den Qualitätssicherungsaufwand um 85\% reduziert. Der primäre technische Verbesserungshebel liegt in der OCR-Komponente, insbesondere bei der Erkennung von Koordinatenbeschriftungen unter ungünstigen Bedingungen (niedrige Auflösung, starke Rotation). Diese Aspekte werden in Kapitel~\ref{chap:diskussion} detailliert diskutiert.