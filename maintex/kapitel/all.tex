\chapter{Anforderungsanalyse}
\label{chap:anforderungen}
Die in Kapitel 3 dargelegten theoretischen und technischen Grundlagen bilden das Fundament für die praktische Umsetzung des Systems. In den nachfolgenden Schritten erfolgt die Konkretisierung der Anforderungen an den zu entwickelnden Prototyp. Die vorliegende Analyse leitet sich aus den funktionalen Zielen des Projektes sowie den nicht-funktionalen Rahmenbedingungen im industriellen Umfeld der Siemens Mobility GmbH ab. Zu diesem Zweck werden die Systemgrenzen, Datenformate sowie potenzielle technische Herausforderungen strukturiert analysiert.

\section{Funktionale Anforderungen}
\label{sec:funktionaleanforderungen}
Im Rahmen dieser Arbeit wird ein Prototyp entwickelt, der die automatisierte Extraktion und Interpretation von Informationen aus technischen Gleisplänen ermöglicht. Die funktionalen Anforderungen ergeben sich aus den spezifischen Aufgabenstellungen der Signaltechnik-Planung. Das System soll folgende Kernfunktionen erfüllen:

\subsection{Symbolerkennung und Objektklassifizierung}
\label{subsec:req_symbol}
Eine zentrale Anforderung ist die zuverlässige Detektion bahntechnischer Symbole in Vektor- oder Rastergrafiken.

\begin{itemize}
    \item \textbf{FA-001}\label{req:FA-001} \textbf{Erkennungsrate:} Das System muss mindestens 90\,\% der definierten Symbolklassen automatisiert erkennen (gemessen als Recall des YOLO-Modells).

    \item \textbf{FA-002}\label{req:FA-002} \textbf{Rotationsinvarianz:} Da Symbole in Gleisplänen der Topologie folgend in beliebigen Winkeln angeordnet sein können, muss die Erkennung rotationsinvariant erfolgen. Das System muss in der Lage sein, Objekte unabhängig von ihrer Orientierung ($0^\circ$ bis $360^\circ$) korrekt zu klassifizieren und deren Winkel (Rotation) präzise zu bestimmen.
    \item \textbf{FA-003}\label{req:FA-003} \textbf{Zielobjekte:} Basierend auf der Domänenanalyse wurden die in den Tabellen \ref{tab:kernklassen} und \ref{tab:auxiliarklassen} dargestellten Objekte als Detektionsziele definiert. Diese gliedern sich in \textbf{Kernklassen}, die für die Planungsaufgaben bei Siemens Mobility essentiell sind und vollständig evaluiert werden, sowie \textbf{Auxiliarklassen}, die zur Demonstration der Systemerweiterbarkeit implementiert werden.
\end{itemize} 
\subsubsection{Kernklassen (produktionsrelevant)}

Die folgenden fünf Objektklassen bilden den Kern der Extraktionsaufgabe und werden 
in Kapitel~\ref{chap:evaluation} quantitativ evaluiert:

\begin{longtable}{|p{5cm}|p{7cm}|}
    \hline
    \textbf{Objektklasse} & \textbf{Symbol} \\
    \hline
    \endhead

    Signal & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/Signal2.png}
        \vspace{0.2cm}
    } \\
    \hline 

    Koordinate (Positionsangabe) & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/Position.png}
        \vspace{0.2cm}
    } \\
    \hline

    GKS-Platte (festkodiert) & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/GKS1.png}
        \vspace{0.2cm}
    } \\
    \hline

    GKS-Platte (gesteuert) & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/GKS2.png}
        \vspace{0.2cm}
    } \\
    \hline

    GM-Block & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/gleismagnet.png}
        \vspace{0.2cm}
    } \\
    \hline

    \caption{Kernklassen für die Datenextraktion (vollständig evaluiert)}
    \label{tab:kernklassen}
\end{longtable}

\subsubsection{Auxiliarklassen (Erweiterbarkeit)}

Zur Demonstration der Modularität und Erweiterbarkeit des Systems (vgl. Anforderung FA-013) werden zusätzlich acht weitere Objektklassen implementiert. Diese Klassen 
sind für spezifische Anwendungsfälle relevant, gehören jedoch nicht zum Kernumfang 
der aktuellen Planungsaufgabe und werden daher nicht detailliert evaluiert.

\begin{longtable}{|p{5cm}|p{7.5cm}|}
    \hline
    \textbf{Objektklasse} & \textbf{Symbol} \\
    \hline
    \endhead

    Weichenblock & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/Weiche2.png}
        \vspace{0.2cm}
    } \\
    \hline 

    Haltepunkt & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/Haltepunkt2.png}
        \vspace{0.2cm}
    } \\
    \hline

    Isolierstoß & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/Isolierstoß.png}
        \vspace{0.2cm}
    } \\
    \hline

    S-Verbinder & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/Weiche2.png}
        \vspace{0.2cm}
    } \\
    \hline

    Prellbock & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=2cm]{images/symbols/prellblock.png}
        \vspace{0.2cm}
    } \\
    \hline

    Haltetafel & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=3cm]{images/symbols/Haltetafel.png}
        \vspace{0.2cm}
    } \\
    \hline

    Ende Weichen & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=3cm]{images/symbols/weichenende.png}
        \vspace{0.2cm}
    } \\
    \hline

    Weichengruppenende & 
    \parbox[c]{7cm}{\centering
        \vspace{0.2cm}
        \includegraphics[width=3cm]{images/symbols/weichengruppeende.png}
        \vspace{0.2cm}
    } \\
    \hline

    \caption{Auxiliarklassen zur Demonstration der Erweiterbarkeit (nicht evaluiert)}
    \label{tab:auxiliarklassen}
\end{longtable}

\textbf{Hinweis:} Die Auxiliarklassen werden im Rahmen des YOLOv8-Trainings 
(Kapitel~\ref{sec:objekterkennungmityolov8obb}) mit annotiert und erreichen 
vergleichbare Detektionsleistungen wie die Kernklassen. Die Fokussierung der End-to-End-Evaluation auf 
die Kernklassen erfolgt aufgrund ihrer direkten Relevanz für die 
Projektierungsaufgaben bei Siemens Mobility.

\subsection{Texterkennung und OCR-Integration}
\begin{itemize}
    \item \textbf{FA-004}\label{req:FA-004} \textbf{OCR-Genauigkeit:} Integration von OCR 
    zur Extraktion von Textinformationen (Signalbezeichnungen, Kilometrierungen). Die 
    Bewertung der Texterkennungsqualität erfolgt nach dem Prinzip der 
    \textit{Feldgenauigkeit} (vgl. Abschnitt~\ref{subsec:ocr_metriken}): Ein Textfeld 
    gilt als korrekt, wenn der extrahierte Wert exakt mit dem Ground Truth 
    übereinstimmt. Diese feldbasierte Bewertung ist in die End-to-End-Systemgenauigkeit 
    (NFA-003) integriert, da sie die praktische Relevanz für den Engineering-Workflow 
    besser abbildet als zeichenbasierte Metriken wie die Character Error Rate. 
    Typische OCR-Fehler (Verwechslungen von O/0, I/1, etc.) werden durch 
    domänenspezifische Validierung (Regex-Muster) automatisch erkannt.
    
    \item \textbf{FA-005}\label{req:FA-005} \textbf{Robustheit:} Die Texterkennung muss auch unter erschwerten Bedingungen zuverlässig funktionieren. Dazu zählen:
    \begin{itemize}
        \item Niedriger Kontrast oder Artefakte durch Scan-Vorgänge (Bildrauschen).
        \item Rotierte Beschriftungen in kardinalen Orientierungen ($0^\circ, 90^\circ, 180^\circ, 270^\circ$) sowie Abweichungen bis $\pm 15^\circ$ durch manuelle Platzierung oder Export-Toleranzen.
        \item Überlagerungen durch Führungslinien oder andere grafische Elemente.
    \end{itemize}
    \begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{images/Kapitel4/OCRsample.png} 
    \caption{Beispielhafte Extraktion von Text und Position am Symbol GKS}
    \label{fig:Ocrsample}
    \end{figure}
\end{itemize}

\subsection{Semantische Verknüpfung und Logik}
Über die reine Detektion hinaus muss das System logische Zusammenhänge zwischen den erkannten Elementen herstellen (\enquote{Intelligent Linking}):

\begin{itemize}
    \item \textbf{FA-006}\label{req:FA-006} \textbf{Fahrtrichtungsdetektion:} Basierend auf der geometrischen Orientierung (Rotationswinkel) von Signalen und Weichen muss automatisch die Wirkrichtung (z.\,B. in Richtung steigender oder fallender Kilometrierung) abgeleitet werden.
    
    \item \textbf{FA-007}\label{req:FA-007} \textbf{Symbol-Koordinaten-Verknüpfung:} 
    Jedes erkannte Gleisplanelement (Signal, GKS-Platte, GM-Block) muss automatisch 
    mit der räumlich nächstgelegenen Kilometrierungsangabe verknüpft werden. 
    Die Verknüpfung erfolgt durch geometrische Proximity-Analyse unter 
    Berücksichtigung der Symbolorientierung. Bei komplexen Objekten wie Haltepunkten müssen zusätzlich mehrere zusammengehörige Elemente (Haltepunkt-Symbol, zugehöriges Signal, Koordinate) zu einer logischen Einheit gruppiert werden.

    \item \textbf{FA-008}\label{req:FA-008} \textbf{Manuelle Korrektur (Human-in-the-Loop):} Da automatische Heuristiken fehleranfällig sein können, muss das System eine Möglichkeit bieten, automatisch erstellte Verknüpfungen manuell zu überschreiben oder zu korrigieren (\enquote{Linking Override}).
\end{itemize}

\subsection{Datenaufbereitung und Export}
\begin{itemize}
    \item \textbf{FA-009}\label{req:FA-009} \textbf{Excel-Integration:} Die extrahierten Daten müssen vollautomatisch in vordefinierte Projektierungs- oder Prüftabellen (Excel .xlsx) übertragen werden. Dabei ist essenziell, dass die Daten in die semantisch korrekten Zellen (Zeilen/Spalten) geschrieben werden.
    \begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{images/Kapitel4/excelübertragung.png} 
    \caption{Schematische Übertragung von erkannten Objektdaten in die Ziel-Tabelle}
    \label{fig:excelübertragung}
    \end{figure}

    \item \textbf{FA-010}\label{req:FA-010} \textbf{Strukturerhalt:} Beim Import in bestehende Dateien darf die vorhandene Struktur (Formatierung, Formeln, Makros) nicht beschädigt werden. Das System darf lediglich Werte (Values) in definierte Bereiche einfügen (Non-destructive Update).

    \item \textbf{FA-011}\label{req:FA-011} \textbf{Änderungsverfolgung (Diff-Funktion):} Automatischer Vergleich zweier Planversionen mit Kategorisierung:
        \begin{itemize}
            \item Hinzugefügt: Neue Objekte in Version B
            \item Entfernt: Objekte aus Version A fehlen in Version B
            \item Verschoben: Identische Kennung, unterschiedliche Position
            \item Modifiziert: Gleiche Position, geänderter Text
        \end{itemize}
    Export als separates Excel-Sheet mit Spalten: Objektklasse, Kennung, Änderungstyp, Alt-Wert, Neu-Wert, Koordinaten.
    \begin{figure}[H]
    \centering
    \includegraphics[width=9cm]{images/Kapitel4/änderungsample.png} 
    \caption{Visualisierung der Änderungsverfolgung zwischen zwei Planversionen}
    \label{fig:änderungsample}
    \end{figure}

    \item \textbf{FA-012}\label{req:FA-012} \textbf{Visuelle Validierung:} Zur manuellen Überprüfung (\enquote{Ground Truthing}) sollen im ausgegebenen Gleisplan (oder einer Overlay-Ansicht) alle erkannten Objekte durch farbige Bounding Boxes markiert werden.
    \begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/Kapitel4/gleisplanmarkierung.png} 
    \caption{Visuelle Validierung durch Bounding-Box-Overlays im Gleisplan \cite{railroadstationdrawing}}
    \label{fig:gleisplanmarkierung}
    \end{figure}
\end{itemize}

\subsection{Benutzerinteraktion und Konfiguration}
\begin{itemize}
    \item \textbf{FA-013}\label{req:FA-013} \textbf{Grafische Benutzeroberfläche (GUI):} Die Bedienung soll über eine intuitive Oberfläche erfolgen, die den Upload von PDF-Dateien, den Start der Analyse sowie den Export ermöglicht, ohne dass Kommandozeilen-Kenntnisse erforderlich sind.
    
    
    \item \textbf{FA-014}\label{req:FA-014} \textbf{Modularität:} Der Prototyp muss eine 
    modulare Architektur aufweisen, die eine klare Trennung der Verantwortlichkeiten zwischen den Verarbeitungsstufen (Objekterkennung, Texterkennung, UI, Export) gewährleistet. Dies ermöglicht die unabhängige Weiterentwicklung einzelner Komponenten sowie die Integration neuer Funktionalitäten (z.B. zusätzliche Symbolklassen) ohne Modifikation der Kernlogik. Die Architekturkonzeption wird in Kapitel~\ref{chap:konzeption} detailliert beschrieben.
\end{itemize}

\section{Nicht-funktionale Anforderungen}
\label{sec:nichtfunktionaleanforderungen}
Ergänzend zu den funktionalen Zielen definieren die nicht-funktionalen Anforderungen die Qualitätsmerkmale und Rahmenbedingungen des Systems.

\subsection{Sicherheit und Datenschutz}
\begin{itemize}
    \item \textbf{NFA-001}\label{req:NFA-001} \textbf{On-Premise-Verarbeitung:} Keine Datenübertragung an externe Cloud-Services. Vollständig lokale Ausführung auf Siemens-Systemen. Offline-Betrieb gewährleistet.
    
    \item \textbf{NFA-002}\label{req:NFA-002} \textbf{Lizenzkonformität:} Ausschließlich Open-Source-Bibliotheken mit genehmigten Lizenzen (Apache 2.0, MIT, BSD). Dokumentation aller Abhängigkeiten.
\end{itemize}

\subsection{Qualität und Zuverlässigkeit}
\begin{itemize}
    \item \textbf{NFA-003}\label{req:NFA-003} \textbf{Gesamtsystem-Genauigkeit:} Mindestens 85\,\% aller Objekte müssen vollständig korrekt extrahiert werden (Detektion + OCR + Linking + Export korrekt). Verbleibende 15\,\% erfordern manuelle Korrektur. Messung in Kapitel 7 auf Ground-Truth-Datensatz.
    
    \item \textbf{NFA-004}\label{req:NFA-004} \textbf{Robustheit:} Resilient gegenüber fehlerhaften Eingaben (korrupte PDFs, Rauschen). Definierte Fehlermeldungen statt Absturz.
    
    \item \textbf{NFA-005}\label{req:NFA-005} \textbf{Prüfbarkeit:} Visuelle Validierung aller Ergebnisse (Bounding-Box-Overlay). Rückverfolgbarkeit: Klick auf Tabellenzeile zeigt Position im Plan.
\end{itemize}

\subsection{Effizienz und Wirtschaftlichkeit}
\begin{itemize}
    \item \textbf{NFA-006}\label{req:NFA-006} \textbf{Prozessoptimierung:} Das Hauptziel ist die Reduktion des manuellen Prüfaufwands. Das Tool soll den Prozess vom zeitintensiven \enquote{4-Augen-Prinzip} (zwei Menschen prüfen manuell) hin zu einem KI-gestützten Prozess (\enquote{Mensch prüft KI}) transformieren.
    
    \item \textbf{NFA-007}\label{req:NFA-007} \textbf{Ressourceneffizienz:} Die Analyse eines durchschnittlichen Bahnhofsplans sollte auf Standard-Hardware in akzeptabler Zeit (wenige Minuten) durchführbar sein.
\end{itemize}

\subsection{Wartbarkeit und Erweiterbarkeit}
\begin{itemize}
    \item \textbf{NFA-008}\label{req:NFA-008} \textbf{Erweiterbarkeit:} Der Code ist modular zu strukturieren, um zukünftige Erweiterungen (z.\,B. Anbindung an CAD-Systeme) zu erleichtern.
    
    \item \textbf{NFA-009}\label{req:NFA-009} \textbf{Update-Fähigkeit:} Neue Symbolvarianten müssen über Konfigurations-Updates oder neue Modell-Gewichte (Weights) einspielbar sein.
\end{itemize}

\subsection{Datenformate und Schnittstellen}
\label{subsec:datenformate}
Die Datengrundlage bilden reale Gleispläne aus dem Bereich \textit{Trainguard MT ZUB} der Siemens Mobility GmbH. Das System muss mit verschiedenen Eingabeformaten und Datenquellen kompatibel sein.

\begin{itemize}
    \item \textbf{NFA-010}\label{req:NFA-010} \textbf{Eingabeformate:} Das System muss folgende Eingabeformate verarbeiten können:
    \begin{itemize}
        \item PDF-Dateien (Vektor- und Rastergrafiken)
        \item Bilddateien (PNG, JPG) mit Mindestauflösung $\geq 300$ DPI
    \end{itemize}
    
    \item \textbf{NFA-011}\label{req:NFA-011} \textbf{Datenquellen:} Das System muss mit folgenden Datenquellen kompatibel sein:
    \begin{itemize}
        \item \textbf{Primäre Quelle:} Kundenspezifische Gleispläne im PDF-/Bildformat. Diese enthalten Vektor- oder Rastergrafiken, exportiert aus technischen Zeichnungs- und Planungssystemen (z.\,B. AutoCAD, LCAD).
        \item \textbf{Sekundäre Quellen:} Historische Bestandspläne (Scans mit potentiell niedrigerer Qualität) sowie synthetisch erzeugte Testdaten zur Überprüfung der Robustheit.
    \end{itemize}
    
    \item \textbf{NFA-012}\label{req:NFA-012} \textbf{Ausgabeformate:} Das System muss Ergebnisse in folgenden Formaten exportieren können:
    \begin{itemize}
        \item Excel-Arbeitsmappe (.xlsx) für Engineering-Workflows
        \item CSV für einfachen Datenaustausch
        \item JSON für API-Schnittstellen
    \end{itemize}
\end{itemize}

Tabelle \ref{tab:formate-prototyp} gibt einen detaillierten Überblick über die im System verwendeten Dateiformate und deren Einsatz in der Verarbeitungspipeline.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabularx}{\textwidth}{|p{2.5cm}|p{2cm}|X|X|}
\hline
\textbf{Kategorie} & \textbf{Format} & \textbf{Beschreibung} & \textbf{Einsatz im Prototyp} \\
\hline
\textbf{Eingabe} & PDF & Standardformat für Pläne & Primäre Datenquelle \\
 & PNG/JPG & Rasterisierte Ausschnitte & Input für CNN/YOLO \\
\hline
\textbf{Verarbeitung} & JSON & Strukturierte Metadaten & Interner Datenaustausch \\
 & PostgreSQL & Relationale Datenbank & Persistenz \& Versionierung \\
\hline
\textbf{Ausgabe} & XLSX & Excel-Arbeitsmappe & Engineering-Workflow \\
 & CSV & Textbasiertes Format & Einfacher Datenaustausch \\
 & JSON & API-Response & Schnittstellenanbindung \\
\hline
\end{tabularx}
\caption{Übersicht der unterstützten Datenformate (siehe \ref{req:NFA-010} bis \ref{req:NFA-012})}
\label{tab:formate-prototyp}
\end{table}

\section{Herausforderungen bei der Umsetzung}
Die Realisierung der in den Abschnitten \ref{subsec:req_symbol} bis \ref{subsec:datenformate} definierten Anforderungen sieht sich folgenden technischen Herausforderungen gegenüber:

\begin{enumerate}
    \item \textbf{Daten-Heterogenität:} Die Varianz in den Eingabedaten (unterschiedliche Export-Einstellungen, Linienstärken, Skalierungen) erschwert eine universelle Regelbildung.
    \item \textbf{Visuelle Ambiguität:} Einige Symbole (z.\,B. unterschiedliche Gleiskoppelspulen-Typen) unterscheiden sich visuell nur in wenigen Pixeln oder sind nur durch den Kontext (Begleittext) differenzierbar.
    \item \textbf{OCR-Komplexität:} Technischer Text in Plänen ist oft extrem klein, rotiert und durch Führungslinien durchgestrichen, was klassische OCR-Engines (wie Tesseract) an ihre Grenzen bringt.
    \item \textbf{Mangel an Trainingsdaten:} Es existiert kein öffentlicher Datensatz für bahntechnische Symbolik. Ein \enquote{Cold Start} ist notwendig, bei dem Trainingsdaten zunächst manuell (z.\,B. via CVAT) annotiert werden müssen.
    \item \textbf{Semantische Lücke:} Der Schritt von der Erkennung (\enquote{Da ist eine Box}) zur Bedeutung (\enquote{Das ist Weiche 12 in Rechtslage}) erfordert komplexe Heuristiken, insbesondere beim Mapping von Textboxen zu den geometrisch nächsten Symbolen.
\end{enumerate}

\section{Anforderungs-Rückverfolgbarkeit}
\label{sec:traceability}

Zur Sicherstellung der vollständigen Umsetzung aller definierten Anforderungen 
wird eine Rückverfolgbarkeitsmatrix (Traceability Matrix) eingeführt. Diese 
dokumentiert die Zuordnung jeder Anforderung zu den entsprechenden 
Implementierungskomponenten (Kapitel~\ref{chap:implementierung}) sowie den 
Evaluationsmetriken (Kapitel~\ref{chap:evaluation}).

\subsection{Funktionale Anforderungen}

\begin{longtable}{|l|p{4cm}|p{4.5cm}|p{4.5cm}|}
\hline
\textbf{ID} & \textbf{Anforderung} & \textbf{Implementierung} & \textbf{Evaluation} \\
\hline
\endfirsthead
\hline
\textbf{ID} & \textbf{Anforderung} & \textbf{Implementierung} & \textbf{Evaluation} \\
\hline
\endhead

FA-001 & Erkennungsrate $\geq$ 90\% & 
§\ref{sec:objekterkennungmityolov8obb}: YOLOv8-OBB Training mit 13 Klassen & 
§\ref{subsec:detection_eval}: Recall = 95.7\% (Val), 100\% (Test) \\
\hline

FA-002 & Rotationsinvarianz ($0^\circ$--$360^\circ$) & 
§\ref{sec:objekterkennungmityolov8obb}: OBB-Annotation, synthetische Rotation (10 Winkel) & 
§\ref{subsec:e2e_test_eval}, Tab.~\ref{tab:rotation_analysis}: 38 Objekte mit $|\theta|>30°$ bei höherer Konfidenz (0.946 vs. 0.890) \\
\hline

FA-003 & Zielobjekte (5 Kernklassen) & 
§\ref{sec:objekterkennungmityolov8obb}: Signal, Koordinate, GKS (2 Typen), GM-Block & 
§\ref{subsec:e2e_test_eval}: 636/644 Objekte korrekt (98.76\%)\\
\hline

FA-004 & OCR-Genauigkeit (in E2E integriert) & 
§\ref{sec:ocrpipeline}: Multi-Engine Kaskade (PaddleOCR, Tesseract, EasyOCR) & 
§\ref{subsec:e2e_test_eval}: 3 OCR-bedingte Fehler (0.47\% Fehlerrate) \\
\hline

FA-005 & OCR-Robustheit (Rauschen, Rotation) & 
§\ref{sec:ocrpipeline}: Dual-Winkel-Routing, CLAHE, Linienentfernung & 
§\ref{subsec:e2e_test_eval}, Tab.~\ref{tab:rotation_analysis}: 100\% OCR-Erfolg bei $|\theta|>30°$ (38/38) \\
\hline

FA-006 & Fahrtrichtungsdetektion & 
§\ref{sec:fahrtrichtung}: Geometrische Ableitung aus Signal-GKS-Relation & 
§\ref{subsec:e2e_test_eval}: 171/172 korrekt (99.42\%), Tab.~\ref{tab:signal_attribute_accuracy} \\
\hline

FA-007 & Symbol-Koordinaten-Verknüpfung & 
§\ref{sec:intelligentesymboltextverknüpfung}: Proximity-basiertes Linking & 
§\ref{subsec:e2e_test_eval}: 642/644 Verknüpfungen korrekt (99.69\%) \\
\hline

FA-008 & Manuelle Korrektur (Human-in-the-Loop) & 
§\ref{sec:validierungundsicherung}: Validierungsdialog mit Inline-Editierung & 
§\ref{subsubsec:validation_tools}: Prüfaufwand um 85\% reduziert \\
\hline

FA-009 & Excel-Integration & 
§\ref{subsec:exportfunktionlität}: XLSX-Export mit Formatierung & 
§\ref{subsec:functional_validation}, Tab.~\ref{tab:functional_validation} \\
\hline

FA-010 & Strukturerhalt (Non-destructive Update) & 
§\ref{subsec:exportfunktionlität}: Werte-Insertion ohne Formatänderung & 
§\ref{subsec:functional_validation}, Tab.~\ref{tab:functional_validation} \\
\hline

FA-011 & Änderungsverfolgung (Diff) & 
§\ref{subsec:vergleichundänderung}: UID-basierter Versionsvergleich & 
§\ref{subsec:functional_validation}, Tab.~\ref{tab:functional_validation} \\
\hline

FA-012 & Visuelle Validierung (Bounding Boxes) & 
§\ref{sec:benutzeroberfläche}: PDF-Viewer mit Overlay-System & 
§\ref{subsec:functional_validation}, Tab.~\ref{tab:functional_validation} \\
\hline

FA-013 & Grafische Benutzeroberfläche & 
§\ref{sec:benutzeroberfläche}: PyQt5-basierte Desktop-Anwendung & 
§\ref{subsec:functional_validation}, Tab.~\ref{tab:functional_validation} \\
\hline

FA-014 & Modularität (Architektur) & 
§\ref{chap:konzeption}: Schichtenarchitektur; §\ref{sec:objekterkennungmityolov8obb}: 8 Auxiliarklassen & 
§\ref{subsec:nfa_validation}: Erweiterung ohne Kernlogik-Änderung \\
\hline

\caption{Rückverfolgbarkeitsmatrix: Funktionale Anforderungen}
\label{tab:traceability_fa}
\end{longtable}

\subsection{Nicht-funktionale Anforderungen}

\begin{longtable}{|l|p{3.5cm}|p{4cm}|p{4cm}|}
\hline
\textbf{ID} & \textbf{Anforderung} & \textbf{Implementierung} & \textbf{Evaluation} \\
\hline
\endfirsthead
\hline
\textbf{ID} & \textbf{Anforderung} & \textbf{Implementierung} & \textbf{Evaluation} \\
\hline
\endhead

NFA-001 & On-Premise-Verarbeitung & 
Vollständig lokale Ausführung, keine Cloud-APIs & 
§\ref{subsec:testumgebung}: CPU-only Inferenz; §\ref{subsec:nfa_validation} \\
\hline

NFA-002 & Lizenzkonformität (Apache/MIT/BSD) & 
Tab.~\ref{tab:tech_stack}: Alle Bibliotheken Open Source & 
§\ref{subsec:nfa_validation}: Lizenzprüfung dokumentiert \\
\hline

NFA-003 & Gesamtgenauigkeit $\geq$ 85\% & 
Gesamte Pipeline (Kap.~\ref{chap:implementierung}) & 
§\ref{subsec:e2e_test_eval}: 98.76\% erreicht \\
\hline

NFA-004 & Robustheit (fehlerhafte Eingaben) & 
§\ref{sec:validierungundsicherung}: Fallback-Mechanismen & 
§\ref{subsubsec:validation_tools}: 7 Pläne ohne Abstürze \\
\hline

NFA-005 & Prüfbarkeit (Rückverfolgbarkeit) & 
§\ref{sec:unterstützendekomponente}: Metadaten, Jump-to-Detection & 
§\ref{subsubsec:validation_tools}: Bidirektionale Navigation \\
\hline

NFA-006 & Prozessoptimierung & 
Automatisierung des manuellen Prozesses & 
§\ref{subsubsec:processing_time}: 75.3\% Zeitersparnis \\
\hline

NFA-007 & Ressourceneffizienz & 
CPU-kompatible Inferenz & 
§\ref{subsubsec:processing_time}: Ø 12.3 min/Plan \\
\hline

NFA-008 & Erweiterbarkeit & 
Modulare Architektur (§\ref{chap:konzeption}) & 
§\ref{subsec:nfa_validation}: 8 Auxiliarklassen integriert \\
\hline

NFA-009 & Update-Fähigkeit & 
Externe Modell-Gewichte (\texttt{best.pt}) & 
§\ref{subsec:nfa_validation}: Nachtraining demonstriert \\
\hline

NFA-010 & Eingabeformate (PDF, PNG, JPG) & 
§\ref{Inferenz}: PyMuPDF, OpenCV & 
§\ref{subsec:testdatensatz}: 7 PDFs verarbeitet \\
\hline

NFA-011 & Datenquellen (Kundenpläne) & 
§\ref{Inferenz}: 500 DPI Rasterisierung & 
§\ref{subsec:testdatensatz}: Siemens Mobility Pläne \\
\hline

NFA-012 & Ausgabeformate (XLSX, CSV, JSON) & 
§\ref{subsec:exportfunktionlität}: Drei Export-Optionen & 
§\ref{subsec:functional_validation}: Funktionstest bestanden \\
\hline

\caption{Rückverfolgbarkeitsmatrix: Nicht-funktionale Anforderungen}
\label{tab:traceability_nfa}
\end{longtable}

\chapter{Konzeption des Prototyps}
\label{chap:konzeption}
In diesem Kapitel wird der Entwurf und die technische Konzeption des Prototyps aufbauend auf der im vorherigen Kapitel definierten Anforderungsanalyse dargelegt. Das Ziel besteht in der Entwicklung einer modularen und skalierbaren Systemarchitektur, die den gesamten Workflow von der rohen PDF-Datei bis zum strukturierten und validierten Datenexport automatisiert. Die konkrete Implementierungsdetails werden anschließend in Kapitel 6 behandelt.

\section{Systemarchitektur}
Die Architektur des Prototyps folgt dem Prinzip der Modularität und Separation of Concerns. 
Das System ist in funktional abgegrenzte Module unterteilt, die über definierte Schnittstellen miteinander kommunizieren. Diese Architektur ermöglicht eine schrittweise Entwicklung, einfache Wartbarkeit und die Möglichkeit zur späteren Erweiterung um zusätzliche Funktionalitäten \textbf{(Anforderungen FA-014, NFA-008)}. Abbildung \ref{fig:Systemarchitektur} zeigt die Gesamtarchitektur des Systems mit ihren Hauptkomponenten.

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{images/Kapitel5/Moduldiagrammv2.png} 
    \caption{Modulare Systemarchitektur des Gleisplanextraktors}
    \label{fig:Systemarchitektur}
\end{figure}

\subsection{Dateneingabe}

Das System akzeptiert Gleispläne in PDF Format als primäre Eingabe. PDF wurde als Standardformat gewählt, da es in der Praxis neben Bildformaten am häufigsten für den Export technischer Zeichnungen verwendet wird. Funktionale Anforderungen an das Eingabemodul:
\begin{itemize}
    \item Hochladen einzelner oder mehrerer PDF Dateien
    \item Validierung des Dateiformats und der Dateiintegrität
    \item Extraktion grundlegender Metadaten (Seitenanzahl,Auflösung,Name des Plans)
\end{itemize}



\subsection{Vorverarbeitung}
Die Vorverarbeitung transformiert die PDF Dokumente in ein für die nachfolgenden KI Module geeignetes Format. Dieser Schritt ist essentiell, weil Deep Learning basierte Objekterkennungsmodelle pixelbasierte Eingaben benötigen. Kernaufgaben des Vorverarbeitungsmoduls: 
\begin{enumerate}
    \item \textbf{Rasterisierung}: Konvertierung der PDF Seiten in hochauflösende Bilddateien (z.B. PNG, JPEG). Die Auflösung muss ausreichend hoch gewählt werden, um auch kleine Symbole und Beschriftungen erkennbar zu machen, typischerweise $\>300$ DPI.
    \item \textbf{Normalisierung}: Einheitliche Farbkanalbehandlung (z.B. Konvertierung zu Graustufen) zur Reduktion der Datenkomplexität bei gleichzeitiger Beibehaltung der relevanten visuellen Information.
    \item \textbf{Tiling (Kachelbildung)}: Große Gleispläne überschreiten häufig die maximale Eingabegröße moderner Objekterkennung (typischerweise 640x640 bis 1024x1024 Pixel). Daher wird das Gesamtbild in überlappende Ausschnitte (Tiles) unterteilt. Die Überlappung stellt sicher, dass Symbole an Kachelgrenzen nicht unvollständig erfasst werden.  
\end{enumerate}
Designentscheidungen: Die Größe der Tiles und der Überlappungsgrad sind konfigurierbare Parameter, die je nach Plandichte und Symbolgrößen angepasst werden können.


\subsection{Detection \& OCR}

Dieses Modul bildet das technologische Herzstück des Systems und kombiniert zwei komplementäre Erkennungsverfahren:
\begin{enumerate}
    \item \textbf{Objekterkennung} - Ein trainiertes Deep Learning Modell der YOLO Familie analysiert die vorverarbeiteten Bildausschnitte und identifiziert relevante Symbole (Signale, Weichen, GKS, etc.). Für jedes erkannte Objekt liefert das Modell:
    \begin{itemize}
        \item Die Klasse (z.B. \enquote{Signal},\enquote{Weiche})
        \item Die Bounding Box (räumliche Position im Bild)
        \item Einen Konfidenzwert (0-1), der die Sicherheit der Erkennung ergibt.
    \end{itemize}
    Die Verwendung von Oriented Bounding Boxes (OBB) ist konzeptionell vorgesehen, um rotierte Symbole präziser zu erfassen, was eine häufige Herausforderung in technischen Zeichnungen ist. 
    \item \textbf{Texterkennung} - OCR Verfahren extrahieren textuelle Informationen aus den erkannten Symbolbereichen. Die Texterkennung erfolgt räumlich fokussiert innerhalb oder in definierter Nähe der durch die Objekterkennung identifizierten Bounding Boxes. Dieser zielgerichtete Ansatz reduziert Fehler durch irrelevante Textfragmente im Gleisplan.\\
    \textbf{Konzeptionelle Herausforderung:} Text in technischen Zeichnungen kann in verschiedenen Orientierungen vorliegen. Das Konzept sieht daher eine orientierungsbewusste OCR Strategie vor, bei der Textregionen vor Erkennung normalisiert werden. 

\end{enumerate}

\subsection{Nachbearbeitung und Mapping}
Die Rohausgaben der Erkennungsmodule erfordern weitere Verarbeitungsschritte, um strukturierte und semantisch korrekte Daten zu erzeugen:
\begin{enumerate}
    \item \textbf{Datenbereinigung:} Rohe OCR Ergebnisse enthalten häufig Artefakte, Formatierungsfehler oder unerwünschte Zeichen. Regelbasierte Filter (z.B. mittels parametrischer Beziehungen) normalisieren die extrahierten Texte gemäß erwarteten Mustern.
    \item \textbf{Zusammenführung:} Die im Schritt erzeugten überlappenden Kacheln müssen wieder zu einem Gesamtbild zusammengefügt werden. Dabei werden Duplikate in den Überlappungsbereichen durch Non Maximum Suppression(NMS) eliminiert. 
    \item \textbf{Semantisches Mapping:} Die erkannten Symbole und Texte werden in einen semantischen Kontext überführt. Beispielsweise wird ein detektiertes Symbol der Klasse \enquote{Signal} mit dem dazugehörigen OCR Text zu einer logischen Einheit verknüpft.
\end{enumerate}
\subsection{Speicherung}
Alle extrahierten und verarbeiteten Daten werden in einem strukturierten Persistenzsystem gespeichert:\\
\textbf{Konzeptionelle Anforderungen:}
\begin{itemize}
    \item \textbf{Versionierung:} Jede Analyse eines Gleisplans erhält eine eindeutige Versions ID, um spätere Vergleiche zu ermöglichen.
    \item \textbf{Nachverfolgbarkeit:} Jedem Datensatz werden Metadaten zugeordnet(Quelldokument, Seitennummer, Koordinaten), um die Rückverfolgbarkeit zur Originalquelle zu gewährleisten.
    \item \textbf{Flexibles Schema:} Verwendung eines Datenbanksystems, das sowohl strukturierte als auch semi strukturierte Daten effizient speichern kann(z.B. PostgreSQL mit JSONB-Support)
\end{itemize}

\subsection{Benutzeroberfläche}
Die UI dient als Kontrollzentrum für alle Systemfunktionen und ermöglicht die Interaktion ohne Programmierkenntnisse:\\
\textbf{Konzeptionelle Anforderungen:}
\begin{itemize}
    \item \textbf{Prozessvisualisierung:} Anzeige des Verarbeitungsfortschritts in Echtzeit.
    \item \textbf{Ergebnisvalidierung:} Visuelle Darstellung der erkannten Objekte überlagert auf dem Original Gleisplan.
    \item \textbf{Konfigurationszugriff:} Bearbeitung von Systemparametern über intuitive Eingabemasken
    \item \textbf{Fehlerbehandlung:} Klare Fehlermeldungen und Hilfestellungen bei Problemen
\end{itemize}

\subsection{Export}
Das Exportmodul transformiert die interne Datenrepräsentation in kundengerechte Ausgabeformate:\\
\textbf{Unterstützte Formate:}
\begin{itemize}
    \item Excel(.xlsx): Strukturierte Tabellen für direkte Integration in bestehende Engineering Workflows
    \item CSV: Austauschformat für einfache Datenweiterverarbeitung
    \item JSON: Maschinenlesbares Format für API Integration in nachgelagerte Systeme
\end{itemize}

\textbf{Konzeptionelle Anforderung:} Das Exportmodul muss konfigurierbare Templates unterstützen, um unterschiedliche Kundenvorgaben für Tabellenstrukturen abzubilden.




\section{Workflow: Von PDF zu Excel}
Der zu entwickelnde Prototyp basiert auf einer klaren Struktur eines Swimlane-Diagramms mit drei Ebenen, wodurch eine saubere Trennung zwischen Benutzeroberfläche, Datenverarbeitung und Datenspeicherung gewährleistet wird. Das folgende Flussdiagramm \ref{fig:Workflowdiagramm} veranschaulicht den gesamten Prozess von der ersten Benutzereingabe bis zum endgültigen Datenexport. 
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/Kapitel5/Prototyp flowchar v02.png} 
    \caption{Workflowdiagramm des Prototyps}
    \label{fig:Workflowdiagramm}
\end{figure}

\subsection{1. Ebene - UI/Frontend (Benutzeroberfläche)}

Die Frontend-Ebene bildet die Schnittstelle zwischen Anwender und System. Der konzeptionelle Ablauf umfasst:
\begin{enumerate}
    \item \textbf{Dateiauswahl:} Der Benutzer wählt eine PDF-Datei sowie optional eine Konfigurationsdatei aus
    \item \textbf{Parametereinstellung:} Konfiguration von Verarbeitungsoptionen (z.B. zu analysierende Seiten, Erkennungsmodell, OCR Engine)
    \item \textbf{Prozessinitiierung:} Start der automatisierten Analyse durch Betätigung einer \enquote{Ausführen}-Schaltfläche
    \item \textbf{Ergebnisanzeige:} Darstellung der Erkennungsergebnisse zur manuellen Überprüfung
    \item \textbf{Export:} Finalisierung und Download der aufbereiteten Daten im gewünschten Format
\end{enumerate}
Designprinzip: Der Workflow folgt einem linearen Assistenten-Modell (Wizard Pattern), das den Benutzer schrittweise durch den Prozess führt.

\subsection{2. Ebene - Backend/Verarbeitung (Kernlogik)}
Das Backend orchestriert die automatisierte Analysepipeline. Die Prozessschritte sind:

\begin{enumerate}
    \item \textbf{Rasterisierung:} Konvertierung der PDF-Seiten in Pixelbilder hoher Auflösung
    
    \item \textbf{Tiling:} Segmentierung in überlappende Bildausschnitte für effiziente Verarbeitung
    
    \item \textbf{Objekterkennung:} Detektion relevanter Symbole mittels Deep Learning Modell
    
    \item \textbf{OCR:} Extraktion von Textinformationen aus identifizierten Symbolbereichen
    
    \item \textbf{Postprocessing \& Merge:} Zusammenführung der Tile-Ergebnisse zu einem Gesamtbild und Eliminierung von Duplikaten in Überlappungsbereichen durch Non-Maximum-Suppression (NMS). Rohe OCR-Ergebnisse werden durch regelbasierte Filter bereinigt.
    
    \item \textbf{Linking \& Semantisches Mapping:} Verknüpfung erkannter Symbole mit zugehörigen Texten (IDs, Koordinaten) zu logischen Einheiten. Beispielsweise wird ein detektiertes Symbol der Klasse \enquote{Signal} mit dem dazugehörigen OCR-Text zu einer semantisch interpretierbaren Entität verbunden. Die konzeptionellen Strategien hierfür werden in Abschnitt \ref{sec:linkingassociationmodul} detailliert beschrieben.
    
    \item \textbf{Validierung \& Qualitätskontrolle:} Automatisierte dreistufige Plausibilitätsprüfung der Erkennungsergebnisse auf Symbol-, OCR- und Linking-Ebene. Fehlgeschlagene Validierungen werden zur manuellen Überprüfung markiert (Human-in-the-Loop). Das vollständige Validierungskonzept ist in Abschnitt \ref{sec:validierungundqualitätssicherung} dargelegt.
    
    \item \textbf{Versionsspeicherung:} Persistierung der Ergebnisse mit Versionsmetadaten in der Datenbank. Jede Analyse erhält eine eindeutige Versions-ID, um spätere Vergleiche zu ermöglichen.
    
    \item \textbf{Exportvorbereitung:} Transformation der internen Datenrepräsentation in das gewünschte Ausgabeformat
\end{enumerate}

Designprinzip: Die Pipeline folgt dem Pipes-and-Filters Architekturmuster, bei dem jede Komponente eine spezifische Transformation durchführt und das Ergebnis an die nächste Stufe weiterreicht.

\textbf{Optionaler Versionsvergleich:} Unabhängig von der automatischen Verarbeitungspipeline kann der Benutzer über die Benutzeroberfläche einen manuellen Vergleich zwischen zwei Versionen desselben Gleisplans initiieren. Hierzu werden beide Planversionen in separaten Ansichten (Tabs) geladen. Durch Betätigung einer Vergleichsfunktion analysiert das System beide Datensätze und identifiziert Änderungen (hinzugefügte, entfernte oder modifizierte Objekte). Die konzeptionelle Vergleichsstrategie basiert auf dem in Abschnitt \ref{sec:änderungsverfolgung} beschriebenen hybriden Matching-Ansatz. Die Ergebnisse werden visuell hervorgehoben dargestellt, sodass der Benutzer Planänderungen nachvollziehen kann. Die technische Implementierung wird in Kapitel 6 beschrieben.

\subsection{3. Ebene - Speicher und Export (Datenverwaltung)}
Die unterste Ebene verwaltet die Daten des gesamten Systems, die in statischen und dynamischen Daten unterteilt sind:

\begin{itemize}
    \item \textbf{Statische Daten} umfassen die Basiskonfigurationsdaten, die die Regeln für das Mapping und die Verarbeitungsparameter definieren, sowie die trainierten Gewichte des Erkennungsmodells. Diese Daten dienen als feste Eingabeparameter für die Verarbeitung im Backend und ändern sich während der Laufzeit nicht.
    
    \item \textbf{Dynamische Daten} werden während des Verarbeitungsprozesses generiert. Zu diesem Zweck ist eine zentrale Datenbank für die dauerhafte Speicherung der Analyseergebnisse und Versionsstände erforderlich. Darüber hinaus werden temporäre Dateien wie Anzeigebilder und Overlays zwischengespeichert. Die aufbereiteten Daten werden schließlich in Form von Exportdateien bereitgestellt, die dem Nutzer zum Download zur Verfügung stehen.
\end{itemize}

Designprinzip: Die Trennung zwischen statischen und dynamischen Daten folgt dem Konzept der Immutability für Konfigurationen, während Analyseergebnisse in einem transaktionalen Datenbanksystem versioniert gespeichert werden.

\textit{Hinweis:} Die konkrete technologische Umsetzung (Datenbankschema, Dateiformate, Speicherorganisation) wird in Kapitel 6 detailliert beschrieben.

\section{Designentscheidungen}
Die Entwicklung des Prototyps erforderte fundamentale Entscheidungen bezüglich der eingesetzten Technologien und Architekturansätze. Die nachfolgenden Abschnitte dokumentieren diese Designentscheidungen mit technischer Begründung und Abgrenzung zu alternativen Lösungsansätzen.

\subsection{Datenaufbereitung: PDF-Rasterisierung mit pdf2image}
Da moderne Objekterkennungsmodelle (wie YOLO) auf Pixeldaten operieren, müssen die vektorbasierten PDF-Gleispläne in einem ersten Schritt rasterisiert werden. Hierfür wurde die Bibliothek \texttt{pdf2image} gewählt, die als Python-Wrapper für die C++-Bibliothek \textit{Poppler} fungiert.\cite{Belval2024}

Diese Komponente bildet den kritischen Eingangskanal der Pipeline. Die Entscheidung für \texttt{pdf2image} begründet sich durch folgende technische Eigenschaften:

\begin{itemize}
    \item \textbf{Rendering-Qualität (Cairo Engine):} 
    Poppler nutzt intern die \textit{Cairo}-Rendering-Engine, die eine pixelgenaue Rasterisierung von Vektorlinien und eingebetteten Textelementen gewährleistet. Für die vorliegende Arbeit wurde eine Auflösung von \textbf{500 DPI} gewählt. Diese hohe Pixeldichte ist notwendig, um filigrane Symbole (z.\,B. Weichenzungen oder Sperrsignale) auch nach der Kachelung (Tiling) noch differenzierbar für das neuronale Netz darzustellen \cite{Poppler2024}.
    
    \item \textbf{Speichereffizientes Ressourcenmanagement:} 
    Gleispläne weisen extreme Dimensionen auf (bis zu $67.000 \times 7.000$ Pixel). Ein naives Laden solcher Dateien würde den Arbeitsspeicher (RAM) handelsüblicher Workstations überlasten. \texttt{pdf2image} unterstützt eine Streaming-Verarbeitung, bei der Seiten einzeln in den Speicher geladen und sofort verarbeitet werden, ohne das Gesamtdokument im RAM zu halten.
    
    \item \textbf{Nahtlose Python-Integration:} 
    Die Bibliothek liefert die gerenderten Daten direkt als \textit{PIL}-Objekte (Python Imaging Library) zurück. Dies ermöglicht eine In-Memory-Weitergabe an den \textit{NumPy}/\textit{OpenCV}-Stack für das Preprocessing, ohne den I/O-Flaschenhals des Zwischenspeicherns auf der Festplatte \cite{Belval2024}.
\end{itemize}

\subsubsection{Abgrenzung zu alternativen Bibliotheken}
Die Auswahl wurde gegen populäre Alternativen validiert. Die folgende Tabelle fasst die Ausschlusskriterien zusammen, wobei insbesondere lizenzrechtliche Aspekte im Unternehmenskontext eine Rolle spielten:

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabularx}{\textwidth}{|p{3.5cm}|X|}
    \hline
    \textbf{Alternative} & \textbf{Nachteil / Ausschlussgrund}\\
    \hline
    \textbf{PyMuPDF (fitz)} & \textit{Lizenzrisiko:} Obwohl PyMuPDF performant ist, unterliegt es der \textbf{AGPL-Lizenz}. Für eine kommerzielle Nutzung innerhalb von Siemens Mobility würde dies eine Offenlegung des gesamten Quellcodes erzwingen (Copyleft-Effekt), was für interne Tools oft ausgeschlossen ist.\\
    \hline
    \textbf{PDFminer} & \textit{Falscher Fokus:} PDFminer extrahiert die Dokumentenstruktur (Text, Metadaten), ist aber nicht in der Lage, komplexe Vektorgrafiken visuell korrekt zu rasterisieren. Für visuelle Objekterkennung ist es daher ungeeignet \cite{Shinyama2022}.\\
    \hline
    \textbf{Poppler CLI} & \textit{Systembruch:} Die direkte Nutzung der Kommandozeilen-Tools (\texttt{pdftoppm}) erfordert Systemaufrufe via \texttt{subprocess}. Dies erschwert das Exception-Handling und verhindert den direkten Datenaustausch im RAM (erzwungener File-I/O), was die Pipeline verlangsamt.\\
    \hline
    \end{tabularx}
    \caption{Vergleich verschiedener PDF-Verarbeitungsbibliotheken}
    \label{tab:Alternative_PDF}
\end{table}

Die konkrete Wahl der Rendering-Parameter (DPI-Auflösung, Farbtiefe, 
Anti-Aliasing) und die Implementierung der Pipeline wird in Abschnitt \ref{Inferenz}
(Preprocessing und Rasterisierung) beschrieben.
\subsection{Objekterkennung: Einsatz von YOLOv8 OBB}
Zur Identifizierung der schematischen Symbole (Signale, Weichen, Gleiselemente) wurde das Modell \textbf{YOLOv8} (You Only Look Once) in der spezialisierten Konfiguration für rotierte Bounding Boxes (OBB) implementiert. Diese Wahl stellt eine Abkehr von klassischen Detektoren dar und wird durch die spezifische Topologie von Gleisplänen begründet.

Die Entscheidung für YOLOv8 OBB basiert auf drei technischen Hauptfaktoren\cite{yolov8_ultralytics}:

\begin{enumerate}
    \item \textbf{Rotationsinvarianz durch OBB-Regression:} 
    Technische Gleispläne zeichnen sich durch eine hohe Dichte an Symbolen aus, die entlang der Gleisachsen in beliebigen Winkeln ($\theta \in [-90^\circ, +90^\circ]$) angeordnet sind. Herkömmliche Detektoren mit achsenparallelen Boxen (Horizontal Bounding Boxes, HBB) sind hier ungeeignet, da sie:
    \begin{itemize}
        \item Bei diagonalen Objekten (z.\,B. $45^\circ$) einen hohen Anteil an irrelevanter Hintergrundfläche einschließen (geringe Signal-to-Noise Ratio).
        \item Bei dichter Symbolfolge zu signifikanten Überlappungen (Intersection over Union, IoU) führen, was fälschlicherweise die Non-Maximum-Suppression (NMS) auslöst und benachbarte Objekte unterdrückt.
    \end{itemize}
    
    YOLOv8 OBB löst dieses Problem durch die Vorhersage eines 5-dimensionalen Vektors $(c_x, c_y, w, h, \theta)$, wobei $\theta$ den Rotationswinkel beschreibt. Dies ermöglicht eine präzise Umhüllung der Symbole unabhängig von ihrer Orientierung im Plan \cite{UltralyticsOBB}.

    \item \textbf{Echtzeitfähigkeit und Tiling-Effizienz:} 
    Da Gleispläne oft Auflösungen von über $50.000$ Pixeln Breite aufweisen, müssen sie in Hunderte kleinerer Kacheln (Tiles) zerlegt werden. Die Inferenzgeschwindigkeit ist daher kritisch für die Gesamtlaufzeit. YOLOv8 erreicht auf Standard-GPU-Instanzen (z.\,B. AWS g4dn.xlarge) eine Framerate von $\sim 40$ FPS bei einer Auflösung von $1024 \times 1024$. Dies ermöglicht die Verarbeitung eines gesamten Bahnhofsplans in wenigen Sekunden, was für eine interaktive Nutzung der Anwendung essenziell ist \cite{yolov8_ultralytics}.

    \item \textbf{Architektonische Modularität (Anchor-Free):} 
    Im Gegensatz zu älteren YOLO-Versionen nutzt v8 einen \textit{Anchor-Free}-Ansatz. Dies eliminiert die Notwendigkeit, vor dem Training manuelle Anker-Boxen basierend auf der Verteilung der Symbolgrößen zu berechnen (K-Means Clustering). Dies reduziert das Hyperparameter-Tuning erheblich und verbessert die Generalisierung auf neue, bisher unbekannte Symbolklassen \cite{10204762}.
\end{enumerate}

\subsubsection{Abgrenzung zu alternativen Detektionsverfahren}
Im Auswahlprozess wurden verschiedene Architekturen evaluiert. Die folgende Tabelle verdeutlicht, warum diese trotz spezifischer Stärken für den Anwendungsfall \enquote{Gleisplan} verworfen wurden:

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabularx}{\textwidth}{|p{3cm}|p{4.5cm}|X|} 
    \hline
    \textbf{Alternative} & \textbf{Evaluationsergebnis} & \textbf{Grund für die Ablehnung} \\ \hline
    \textbf{RTMDet} & Hohe Präzision (mAP: 91.2\%), vergleichbar mit YOLOv8 & Obwohl die Rotation-Aware-Features vielversprechend sind, zeigte RTMDet eine instabilere Konvergenz bei kleinen Datensätzen ($<2000$ Samples) und verfügt über ein kleineres Entwickler-Ökosystem im Vergleich zu YOLO \cite{lyu2022rtmdetempiricalstudydesigning}. \\ \hline
    \textbf{Template Matching} (OpenCV) & Geringer Recall ($<60\%$) bei leichten Rotationen ($\pm 10^\circ$) & Klassische Computer-Vision-Verfahren sind nicht robust gegenüber Skalierung und Artefakten. Zudem ist der Ansatz bei $>13$ Symbolklassen rechnerisch nicht skalierbar \cite{Brunelli2009}. \\ \hline
    \textbf{DETR / DINOv2} & Trainingszeit ca. $3\times$ länger als bei CNNs & Transformer-basierte Ansätze benötigen enorme Datenmengen, um die Inductive Biases von CNNs zu lernen. Für die strukturierte Domäne technischer Zeichnungen stellt dies einen unverhältnismäßigen Ressourcenaufwand dar \cite{Carion2020_DETR}. \\ \hline
    \textbf{Faster R-CNN} & Inferenz: $\sim 8$ FPS (vs. 40 FPS bei YOLO) & Die Two-Stage-Architektur (Region Proposal Network + Classifier) ist zwar präzise, aber für das Feedback in einer User-Interface-Anwendung zu langsam \cite{Ren2015_FasterRCNN}. \\ \hline
    \end{tabularx}
    \caption{Vergleich und Bewertung von Alternativen zu YOLOv8}
    \label{tab:Alternative_Detection}
\end{table}
Die Entscheidung für YOLOv8-OBB adressiert direkt die Anforderungen \textbf{FA-001} 
(Erkennungsrate $\geq$ 90\%), \textbf{FA-002} (Rotationsinvarianz durch OBB-Regression) 
und \textbf{NFA-007} (Ressourceneffizienz durch Echtzeitfähigkeit).
\subsection{Texterkennung: Multi-Engine-Strategie und Dual-Angle-Routing}
Die Extraktion von Textinformationen aus technischen Gleisplänen stellt aufgrund variierender Schriftarten, unterschiedlicher Orientierungen ($0^\circ, 90^\circ, 270^\circ$) und der Überlagerung durch grafische Elemente eine besondere Herausforderung dar. Um eine maximale Robustheit zu gewährleisten, wurde keine einfache OCR-Lösung gewählt, sondern eine \textbf{kaskadierte Multi-Engine-Strategie} implementiert.

Das System kombiniert die Deep-Learning-basierte Bibliothek \textbf{PaddleOCR} (als Primärinstanz) mit der klassischen Engine \textbf{Tesseract} (als Fallback). Ergänzt wird dies durch einen eigens entwickelten \textbf{Dual-Angle-Routing-Algorithmus}.

\subsubsection{Primäre Engine: PaddleOCR (PP-OCRv3)}
Als Hauptkomponente kommt PaddleOCR zum Einsatz. Die Entscheidung für dieses Framework basiert auf der Architektur des PP-OCRv3-Modells, welches auf einem ultra-leichten CRNN (Convolutional Recurrent Neural Network) basiert.\cite{paddleocr} 

Die technischen Vorzüge gegenüber alternativen Engines zeigten sich in der empirischen Evaluation:
\begin{itemize}
    \item \textbf{Rotation-Robustheit:} Standard-OCR-Engines versagen oft bei vertikalem Text. PaddleOCR unterstützt zwar eine interne Klassifizierung (\texttt{use\_angle\_cls}), diese erwies sich jedoch bei kurzen technischen Bezeichnern als fehleranfällig. Daher wurde das System so konfiguriert, dass die Rotation extern durch die OBB-Koordinaten (aus dem YOLO-Schritt) vorgegeben wird, was die Stabilität signifikant erhöht \cite{du2020ppocrpracticalultralightweight}.
    
    \item \textbf{Genauigkeit (Character Error Rate - CER):} Auf einem validierten Testset von Gleisplan-Ausschnitten (Crops) erzielte PaddleOCR die niedrigsten Fehlerraten:
    \begin{itemize}
        \item \textbf{PaddleOCR:} 7.2\,\% (horizontal), 12.1\,\% (vertikal/rotiert)
        \item \textbf{Tesseract:} 11.4\,\% (horizontal), 23.8\,\% (vertikal/rotiert)
        \item \textbf{EasyOCR:} 9.1\,\% (horizontal), 18.3\,\% (vertikal/rotiert)
    \end{itemize}
    
    \item \textbf{Inferenzgeschwindigkeit:} Mit durchschnittlich \textbf{52 ms pro Crop} (CPU, 12 Threads) ist PaddleOCR deutlich effizienter als Tesseract (78 ms), was bei Plänen mit tausenden Textobjekten essentiell für die Gesamtlaufzeit ist.
\end{itemize}

\subsubsection{Algorithmus: Dual Angle Routing}
Ein neuartiger Ansatz der Pipeline ist das \textit{Dual Angle Routing}. Da die Orientierung eines Textes im CAD-Plan nicht immer eindeutig aus der Bounding Box hervorgeht (z.\,B. bei quadratischen Boxen), wird jeder Textausschnitt spekulativ in zwei Orientierungen verarbeitet:

\begin{enumerate}
    \item \textbf{Pfad A (Original):} Der Crop wird unverändert an die OCR übergeben.
    \item \textbf{Pfad B (Orthogonal):} Der Crop wird um $90^\circ$ rotiert an die OCR übergeben.
\end{enumerate}
\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
    % LEFT: Cardinal Path
    \begin{scope}
        \node[above, font=\large\bfseries] at (1.5, 3.2) {Cardinal Path};
        \node[below, font=\small] at (1.5, 2.8) {$|\theta| \leq 15^\circ$};
        
        % Slightly rotated text box (12 degrees)
        \draw[thick, blue, rotate around={12:(1.5,1.5)}] (0.5,1) rectangle (2.5,2);
        \node[rotate=12, font=\small] at (1.5,1.5) {Signal A12};
        
        % Arrow indicating transformation
        \draw[->, very thick, green!70!black] (1.5, 0.5) -- (1.5, 0.8);
        \node[below, font=\footnotesize, align=center] at (1.5, 0.5) {
            Diskrete\\Rotation
        };
        
        % Result (horizontal)
        \draw[thick, green!70!black, fill=green!10] (0.5,-1.5) rectangle (2.5,-0.5);
        \node[font=\small] at (1.5,-1) {Signal A12};
        
        \node[below, font=\footnotesize] at (1.5, -1.8) {OCR-bereit};
    \end{scope}
    
    % RIGHT: Angular Path
    \begin{scope}[xshift=6cm]
        \node[above, font=\large\bfseries] at (1.5, 3.2) {Angular Path};
        \node[below, font=\small] at (1.5, 2.8) {$|\theta| > 15^\circ$};
        
        % Heavily rotated text box (45 degrees)
        \draw[thick, orange, rotate around={45:(1.5,1.5)}] (0.5,1) rectangle (2.5,2);
        \node[rotate=45, font=\small] at (1.5,1.5) {Signal A12};
        
        % Arrow indicating transformation
        \draw[->, very thick, green!70!black] (1.5, 0.5) -- (1.5, 0.8);
        \node[below, font=\footnotesize, align=center] at (1.5, 0.5) {
            Perspektiv-\\transformation
        };
        
        % Result (horizontal)
        \draw[thick, green!70!black, fill=green!10] (0.5,-1.5) rectangle (2.5,-0.5);
        \node[font=\small] at (1.5,-1) {Signal A12};
        
        \node[below, font=\footnotesize] at (1.5, -1.8) {OCR-bereit};
    \end{scope}
\end{tikzpicture}
\caption{Konzeptioneller Vergleich der Dual-Winkel-Verarbeitungsstrategien: Cardinal Path für nahezu ausgerichtete Texte ($|\theta| \leq 15^\circ$) mit diskreter Rotation; Angular Path für stark geneigte Texte ($|\theta| > 15^\circ$) mit Perspektiventransformation}
\label{fig:dual_angle_comparison}
\end{figure}

Die Entscheidung, welches Ergebnis übernommen wird, erfolgt durch eine \textbf{Textnormierungs-Heuristik}. Beide Ergebnisse werden gegen reguläre Ausdrücke (Regex) für typische Signalbezeichnungen (z.\,B. \texttt{\^{}[A-Z]\{1,3\}[0-9]+}) geprüft. Das Ergebnis mit dem höheren Konfidenz-Score, das gleichzeitig dem Schema entspricht, wird akzeptiert. Dies eliminiert Rauschen, das entsteht, wenn vertikaler Text fälschlicherweise horizontal gelesen wird.

\subsubsection{Fallback-Ebene und Vorverarbeitung}
Für Fälle, in denen PaddleOCR keine plausiblen Ergebnisse liefert (Konfidenz $< 0.5$), greift das System auf \textbf{Tesseract 5} zurück.
\begin{itemize}
    \item \textbf{Spezialisierung:} Tesseract zeigt insbesondere bei stark pixelierten, aber hochkontrastigen Signalnamen Stärken.
    \item \textbf{Page Segmentation Modes (PSM):} Durch die explizite Setzung von \texttt{--psm 7} (Single Line) für Koordinaten und \texttt{--psm 8} (Single Word) für IDs wird die Engine gezwungen, Layout-Analysen zu überspringen, was Fehlinterpretationen reduziert \cite{Smith2007}.
    \item \textbf{Preprocessing:} Vor der OCR-Anwendung durchlaufen die Bildausschnitte eine adaptive Binarisierung nach Otsu \cite{Otsu1979gxi}, um Artefakte der PDF-Rasterung zu entfernen und den Zeichenkontrast zu maximieren.
\end{itemize}

\subsubsection{Abgrenzung zu Alternativen}
Die folgende Tabelle fasst zusammen, warum aktuelle Transformer-basierte Ansätze (\enquote{State of the Art} in der Forschung) für diesen spezifischen Anwendungsfall nicht geeignet waren:

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.4}
    \begin{tabularx}{\textwidth}{|p{3.5cm}|X|}
    \hline
    \textbf{Alternative} & \textbf{Grund für den Ausschluss}\\
    \hline
    \textbf{LayoutLMv3} & \textit{Ressourcen-Overhead:} Als Transformer-Modell benötigt es GPU-Beschleunigung und ca. 2.1 GB VRAM. Für die Erkennung einzelner kurzer Textzeilen (Single-Line OCR) stellt dies einen unverhältnismäßigen Ressourcenaufwand (\enquote{Overkill}) dar \cite{huang2022layoutlmv3pretrainingdocumentai}.\\
    \hline
    \textbf{Donut (End-to-End)} & \textit{Mangelnde Generalisierung:} Da Donut (Document Understanding Transformer) direkt Bild-zu-Token generiert, ohne explizite Bounding Boxes zu nutzen, scheitert es oft an den ungewöhnlichen Vektorschriftarten der CAD-Pläne, da kein spezifisches Vortraining existiert \cite{kim2022ocrfreedocumentunderstandingtransformer}.\\
    \hline
    \textbf{TrOCR} & \textit{Rotations-Schwäche:} TrOCR ist auf horizontale Textzeilen spezialisiert. Da Gleispläne Text in beliebigen Winkeln enthalten, wäre ein komplexes Pre-Alignment notwendig, was den Vorteil der Architektur negiert \cite{li2022trocrtransformerbasedopticalcharacter}.\\
    \hline
    \end{tabularx}
    \caption{Vergleich und Bewertung alternativer OCR-Ansätze}
    \label{tab:ocr_alternatives}
\end{table}
Die Multi-Engine-Strategie mit Dual-Angle-Routing erfüllt die Anforderungen \textbf{FA-004} 
(OCR-Genauigkeit) und \textbf{FA-005} (OCR-Robustheit bei Rotation und Rauschen).
\subsection{Datenexport und Reporting: Excel Schnittstelle}
Für den Export der extrahierten Informationen und die Übergabe an die Fachabteilung wird die Funktion \texttt{pandas.DataFrame.to\_excel()} in Kombination mit der \texttt{openpyxl}-Engine verwendet. Diese Architektur ermöglicht die direkte Serialisierung der internen Datenstrukturen in das weit verbreitete \texttt{.xlsx}-Format (Office Open XML).\cite{openpyxl}

Die Entscheidung für diesen Ansatz stützt sich auf drei zentrale Gründe:

\begin{enumerate}
    \item \textbf{Feature-Vollständigkeit und Usability}: 
    Die Lösung bietet native Unterstützung für semantische Formatierungen, die die manuelle Weiterverarbeitung erleichtern:
    \begin{itemize}
        \item \textbf{Visuelle Strukturierung:} Anpassung der Spaltenbreite, Festlegen von Zellenfarben und Fixierung von Kopfzeilen (Freeze Panes).
        \item \textbf{Rückverfolgbarkeit (Traceability):} Verwendung von Formeln wie \texttt{=HYPERLINK()}, um vom Datensatz direkt auf den Bildausschnitt im Plan zu verweisen.
        \item \textbf{Multi-Sheet-Architektur:} Erstellung mehrerer Arbeitsblätter (Tabs) je nach Datenklasse (z.\,B. separate Blätter für Signale und Weichen).
    \end{itemize}
    
    \item \textbf{Integration mit pandas}: 
    Es entsteht eine nahtlose Datenfluss-Pipeline ohne Medienbruch. Die im Analyse-Schritt erzeugten DataFrames werden direkt persistiert, wobei \texttt{openpyxl} als Backend dient, um auch bestehende Templates modifizieren zu können.

    \item \textbf{Unternehmenskonformität}:
    \begin{itemize}
        \item Das \texttt{.xlsx}-Format ist der Industriestandard bei Siemens Mobility.
        \item Die Dateien sind kompatibel mit Microsoft Excel ($\geq$ 2016) und LibreOffice Calc.
        \item Da keine Makros (\texttt{.xlsm}) benötigt werden, entfallen sicherheitstechnische Hürden beim Austausch.
    \end{itemize}


    Im Rahmen der Architekturentscheidung wurden folgende Alternativen evaluiert und verworfen:

    \begin{table}[H]
        \centering
        \renewcommand{\arraystretch}{1.4} % Etwas mehr Zeilenabstand für Lesbarkeit
        \begin{tabularx}{\textwidth}{|p{3.5cm}|X|}
        \hline
        \textbf{Alternative} & \textbf{Nachteil / Ausschlussgrund}\\
        \hline
        \textbf{XlsxWriter} & Reine \textit{Write-Only}-Bibliothek. Sie ist zwar performant, kann aber existierende \texttt{.xlsx}-Dateien nicht einlesen oder bearbeiten, was die Nutzung von Vorlagen (Templates) verhindert.\\
        \hline
        \textbf{CSV (Textdatei)} & Keine Unterstützung für Formatierungen, Formeln oder mehrere Arbeitsblätter. Zudem bestehen Risiken durch Encoding-Probleme (UTF-8 vs. ANSI) und Trennzeichen-Konflikte.\\
        \hline
        \textbf{LibreOffice API / COM} & Systemabhängig und langsam. Erfordert eine lokale Installation der Office-Suite auf dem Server, was die Portabilität der Anwendung einschränkt.\\
        \hline
        \end{tabularx}
        \caption{Vergleich der Export-Alternativen zu pandas + openpyxl}
        \label{tab:Alternative_Export}
    \end{table}

\end{enumerate}
Die Implementierung der Export-Pipeline, einschließlich der Tabellenformatierung, 
bedingten Formatierung und Formelintegration, wird in Abschnitt \ref{subsec:exportfunktionlität}
(Excel-Export-Modul) beschrieben. Diese Architekturentscheidung erfüllt die Anforderungen \textbf{FA-009} (Excel-Integration), 
\textbf{FA-010} (Strukturerhalt bei bestehendem Template) und \textbf{NFA-012} (Ausgabeformate).
\subsection{Rückverfolgbarkeit: Koordinaten + Bildausschnitte}
Die gewählte Lösung für die Rückverfolgbarkeit ist ein hybrider Ansatz mit Koordinaten in Excel und optionaler Miniaturbildgenerierung. Für die UI-Interaktion wurde eine einfache Methode verwendet: Wird die Tabellenzeile in der Benutzeroberfläche angeklickt, so wird dem Benutzer ein vergrößerter Bereich von $2048 \times 2048$ px im Layout angezeigt, wobei die entsprechenden Datenüberlagerungen in leuchtendem Rot hervorgehoben sind.\\
Die Gründe für diese gewählte Lösung sind:
\begin{itemize}
    \item Kompaktheit: Die Koordinaten benötigen nur 8 Byte und 50 KB pro Miniaturansicht, was einen sehr geringen Speicherplatzbedarf darstellt und eine sehr hohe Funktionalität bieten. 
    \item Flexibilität: Auf Wunsch des Benutzers kann die Benutzeroberfläche den entsprechenden vergrößerten Bereich für eine genauere und präzisere Analyse darstellen. 
    \item Normkonformität: Diese Methode erfüllt außerdem die Anforderungen der VDI 1000:2017-06 hinsichtlich Rückverfolgbarkeit.\cite{VDI1000} 
\end{itemize} 
Andere Alternative
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3} % More row height
\begin{tabularx}{\textwidth}{|p{4cm}|X|}
\hline
\textbf{Alternative} & \textbf{Nachteil}\\
\hline
\textbf{Nur Bildausschnitte} & Datenmenge: ~12 MB für 250 Symbole; keine maschinelle Weiterverarbeitung\\
\hline
\textbf{Nur Hyperlinks} & Externe PDF-Abhängigkeit; kein Inline-Preview in UI\\
\hline
\textbf{Bildoverlay in Excel} & Excel Dateigröße >100 MB; Leistungsprobleme
\\
\hline
\end{tabularx}
\caption{Alternativen zu pandas + openpyxl}
\label{tab:Alternative5}
\end{table}
\subsection{Änderungsverfolgung: Konzeptioneller Ansatz}
\label{sec:änderungsverfolgung}
Das System unterstützt die Versionierung von Gleisplänen, um Änderungen zwischen 
verschiedenen Planständen nachvollziehbar zu machen.

\subsubsection{Konzeptionelle Strategie}

Die Änderungserkennung basiert auf einem zweistufigen Vergleichsansatz:

\begin{enumerate}
  \item \textbf{Objektidentifikation}: Jedes extrahierte Symbol erhält eine eindeutige 
        Kennung (UID), die aus Objektklasse und Textinhalt gebildet wird. Dies ermöglicht 
        die eindeutige Zuordnung zwischen Planversionen.
        
        Beispiel: Ein Signal mit der Bezeichnung \enquote{AS102} erhält die UID \texttt{signal\_AS102}.
  
  \item \textbf{Differenzbildung}: Durch Mengenoperationen auf den UID-Mengen werden 
        Änderungen kategorisiert:
        \begin{itemize}
          \item \textbf{Hinzugefügt}: Objekte, die nur in der neuen Version existieren
          \item \textbf{Entfernt}: Objekte, die nur in der alten Version existieren
          \item \textbf{Modifiziert}: Objekte mit gleicher UID, aber unterschiedlichen 
                Attributen (Position, Text)
        \end{itemize}
\end{enumerate}

\subsubsection{Hybrid-Matching für Positionsänderungen}

Wenn ein Symbol räumlich verschoben wurde, kann die UID-basierte Zuordnung versagen. 
Daher wird ein Hybrid-Ansatz verwendet:

\begin{itemize}
  \item \textbf{Primär}: Räumlicher Abgleich über OBB-Überlappung (IoU)
  \item \textbf{Fallback}: Textbasierter Abgleich über Ähnlichkeitsmetriken
\end{itemize}

Die Ergebnisse werden als strukturierter Änderungsbericht exportiert, der die Art der 
Änderung, die betroffenen Attribute und die Positionen dokumentiert.

Die technische Implementierung dieser Strategie, einschließlich der mathematischen 
Formulierung und der Algorithmen, wird in Abschnitt \ref{subsec:vergleichundänderung} detailliert beschrieben.

\subsection{Benutzeroberfläche: PyQt5}
Für die Benutzeroberfläche wurde das Framework PyQt5 aus der Python-Bibliothek ausgewählt. Die Gründe für diese Wahl werden nachfolgend dargelegt\cite{pyqt5}:
\begin{enumerate}
    \item Leistung und Rendering Architektur - Die Kernaufgabe der Anwendung ist die Visualisierung von extrem hochauflösenden Gleisplänen (bis zu 67.000 x 7.000 Pixel).
    \begin{itemize}
        \item Hardwarebeschleunigung: PyQt5 nutzt über die QGraphicsView-Klasse direkt die OpenGL-Schnittstelle der Grafikkarte. Dies ermöglicht eine Hardwarebeschleunigung für Zoomen und Schwenken ohne Ruckeln.
        \item Multithreading: Um das Blockieren der Benutzeroberfläche (UI Freezing) während rechenintensiver Operationen (YOLO-Inferenz, OCR-Verarbeitung) zu verhindern, wurde eine asynchrone Architektur mittels QThread und dem Signal-Slot-Mechanismus implementiert. Dies garantiert, dass die GUI auch während der Analyse reaktionsfähig bleibt.
    \end{itemize}
    \item Unternehmenssicherheit und Datensouveränität: Im Zusammenhang mit kritischer Eisenbahninfrastruktur ist der Schutz vertrauenswürdiger Gleisplanungsdaten von höchster Priorität:
    \begin{itemize}
        \item On-Premise Deployment: Die Anwendung läuft vollständig lokal auf dem Rechner des Ingenieurs. Es findet keine Datenübertragung an externe Server oder Cloud-Dienste statt.
        \item Security by Design: Durch den Verzicht auf Browser-Technologien entfallen typische Web-Angriffsvektoren wie Cross-Site Scripting (XSS) oder unerwünschtes Data-Leakage durch Browser-Extensions. Die Datenhoheit bleibt im lokalen Dateisystem.
    \end{itemize}
    \item Ergonomie und Multi-Monitor-Workflows: PyQt5 zeichnet sich durch eine hohe Flexibilität in der Fensterverwaltung aus. Ingenieure nutzen in vielen Fällen mehrere Monitore, um den Gleisplan auf einem Bildschirm und die tabellarischen Daten auf dem anderen Bildschirm zur Überprüfung anzuzeigen. Mittels dieses Frameworks ist es möglich, die Fenster derart zu arrangieren, dass die Daten im Gleisplan effizient aufgefunden und überprüft werden können. Diese Lösung ermöglicht zudem das dynamische Ausklappen (Popping out) von Widgets. Dies trägt zur Ergonomie bei und reduziert den Zeit- und Arbeitsaufwand beim Wechseln zwischen Fenstern oder Registerkarten. 
    \item Vergleich mit alternativen Technologien: Im Vorfeld der Entwicklung wurden verschiedene UI-Frameworks evaluiert. Die folgende Tabelle fasst die Gründe für deren Ausschluss zusammen:
    \begin{table}[ht]
    \centering
    \begin{tabular}{|p{2.5cm}|p{4cm}|p{9cm}|} 
        \hline
        \textbf{Alternative} & \textbf{Technologie} & \textbf{Hauptgrund für den Ausschluss} \\ \hline
        Streamlit & Python Web-Wrapper & Kein Multi-Window Support- Statelessness erzwingt ständiges Neuladen (Rerun) bei Interaktionen, was bei großen Bildern zu inakzeptablen Wartezeiten führt.\cite{StreamlitSecurity} \\ \hline
        Flask / Django & Web Backend & Overhead \& Komplexität- Erfordert lokalen Webserver und Browser; keine native Integration in das Dateisystem (Drag \& Drop, native Dialoge).\\ \hline
        Electron & JS/Chromium & Ressourcenverbrauch- Hoher Speicherbedarf durch gebündelte Chromium Instanz; Sicherheitsrisiken durch JavaScript Bibliotheken; Performance bei 60k Pixel Bildern unterlegen.\\ \hline
        Tkinter & Python Native & Veraltete UX; Fehlende GPU-Beschleunigung für Canvas-Elemente; keine moderne HiDPI-Skalierung (\enquote{blurry} Text auf 4K-Monitoren).\\ \hline
    \end{tabular}
    \caption{Alternative zu PyQt5}
    \label{tab:Alternative6}
    \end{table}
Die Wahl von PyQt5 als Desktop-Framework adressiert die Anforderungen \textbf{FA-013} 
(GUI ohne CLI-Kenntnisse), \textbf{NFA-001} (On-Premise-Verarbeitung ohne Cloud) und 
\textbf{FA-012} (visuelle Validierung durch Overlay-System).
\subsection{Datenpersistenz und Speicherschicht: PostgreSQL}
Für die dauerhafte Speicherung der Analyseergebnisse und Gleisplandaten wurde das objekt-relationale Datenbanksystem (ORDBMS) PostgreSQL gewählt. Die Implementierung verfolgt einen hybriden Ansatz (\enquote{Relational + Document Store}), der die Transaktionssicherheit einer SQL-Datenbank mit der Flexibilität einer dokumentenorientierten Speicherung (via JSONB) kombiniert.\cite{postgresql_docs}
\begin{enumerate}
    \item Schema-Design und Datenspeicherung: Das zentrale Speicherelement ist die Tabelle \textit{workspaces}. Anstatt für jede erkannte Objektklasse (Signale, Weichen, Texte) separate relationale Tabellen anzulegen, werden die heterogenen Analyseergebnisse in einem semi-strukturierten Format aggregiert. Dieser Entwurf adressiert zwei spezifische Anforderungen der Gleisplananalyse:
    \begin{itemize}
        \item Semi strukturierte Metadaten(JSONB): Die Ergebnisse der Objekterkennung (Bounding Boxes, Klassenwahrscheinlichkeiten, OCR-Texte) variieren stark in ihrer Struktur. Das JSONB-Format (Binary JSON) erlaubt es, diese hierarchischen Daten effizient zu speichern, ohne das Datenbankschema bei jeder Änderung der YOLO-Klassen anpassen zu müssen (Schema Evolution). Der GIN-Index (Generalized Inverted Index) ermöglicht dabei Abfragen auf tief verschachtelte Attribute (z. B. \enquote{Finde alle Pläne mit Signalen vom Typ AH}) in logarithmischer Zeit, vergleichbar mit klassischen Spaltenindizes.
        \item Binäre Topologie-Daten (BYTEA): Das extrahierte Gleisskelett (der Graph der Fahrwege) liegt zur Laufzeit als speicheroptimiertes NumPy-Array vor. Die Konvertierung in Textform (JSON/CSV) wäre ineffizient und würde Präzision kosten. Daher wird dieses Array direkt als binäres Objekt (BYTEA) persistiert, was Ladezeiten minimiert und die exakte Repräsentation der Gleisgeometrie wahrt.
    \end{itemize}
    \item Versionierung und \enquote{Upsert}-Strategie: Um Dateninkonsistenzen bei mehrfacher Analyse desselben Plans zu vermeiden, implementiert die Anwendung eine UPSERT-Logik (Update or Insert)
    \item Vorbereitung für Active Learning (Human in the Loop): Ein wesentliches Ziel der Architektur ist die nachhaltige Verbesserung der KI Modelle durch Nutzer Feedback (siehe Kapitel 8.3.2). Hierfür wurde ein separates Schema entworfen, das Abweichungen zwischen KI-Vorhersage und Nutzerkorrektur protokolliert:
    \item Abgrenzung zu alternativen Speichertechnologien
        \begin{table}[ht]
    \centering
    \begin{tabular}{|p{2cm}|p{4cm}|p{8.5cm}|} 
        \hline
        \textbf{Alternative} & \textbf{Evaluationsergebnis} & \textbf{Begründung der Ablehnung} \\ \hline
        SQLite & Bedingt geeignet & Obwohl leichtgewichtig, fehlt SQLite eine robuste Multi-User-Unterstützung (Database Locking bei Schreibzugriffen) und die JSON-Abfragefunktionen sind weniger performant als die GIN-Indizierung von PostgreSQL.\cite{SQLiteVsPostgres} \\ \hline
        MongoDB & Nicht gewählt & Als native NoSQL-DB wäre sie für JSON gut geeignet, bietet jedoch schwächere Garantien für relationale Integrität (z. B. Foreign Keys für user\_corrections) und erhöht die Komplexität im Deployment (zusätzliche Infrastruktur-Komponente).\\ \hline
        JSON-Files & Ungenügend & Die Speicherung in reinen Textdateien bietet keine Transaktionssicherheit (ACID), keine Indizierung für schnelle Suchen und führt bei parallelen Zugriffen zu \enquote{Race Conditions}.\\ \hline
    \end{tabular}
    \caption{Alternativen zu PostgreSQL}
    \label{tab:Alternative7}
    \end{table}
\end{enumerate}





\end{enumerate}
Das Datenbankdesign unterstützt die Anforderungen \textbf{NFA-005} (Prüfbarkeit durch 
Metadaten-Persistierung), \textbf{FA-011} (Änderungsverfolgung durch Versionierung) und \textbf{NFA-002} (Lizenzkonformität durch Open-Source-Datenbank).
\section{Linking- \& Assoziationsmodul}
\label{sec:linkingassociationmodul}
Die semantische Interpretation der Gleispläne erfordert die korrekte Verknüpfung 
zwischen erkannten Symbolen (Ankerobjekten) und den zugehörigen Texten (IDs, Koordinaten). 
Dieses Modul adressiert die Anforderungen \textbf{FA-006} (Fahrtrichtungsdetektion) und 
\textbf{FA-007} (Symbol-Koordinaten-Verknüpfung).

\subsection{Konzeptioneller Ansatz}

Das Linking-Modul basiert auf drei Kernprinzipien:

\begin{enumerate}
  \item \textbf{Rotationsinvariante Geometrie}: 
  Räumliche Beziehungen (z.B. \enquote{Text befindet sich unterhalb des Symbols}) werden 
  im lokalen Koordinatensystem der OBB bewertet, nicht im globalen Bildkoordinatensystem. 
  Dies gewährleistet robuste Verknüpfungen unabhängig von der Symbolorientierung.
  
  \item \textbf{Klassenspezifische Regeln}: 
  Für jede Objektklasse sind typische Text-Positionen definiert (z.B. Signalbezeichnungen 
  oberhalb des Symbols, Koordinaten unterhalb). Diese Regeln basieren auf Domänenwissen 
  über Gleisplan-Konventionen.
  
  \item \textbf{Adaptive Mustererkennung}: 
  Das System lernt wiederkehrende Layoutmuster aus erfolgreichen Verknüpfungen und 
  nutzt diese zur Verbesserung zukünftiger Zuordnungen (vgl. Abschnitt \ref{subsec:adaptivelearningmechanism}).
\end{enumerate}

\subsection{Verknüpfungsstrategien}

\subsubsection{Proximity-basierte Suche}

Textkandidaten werden innerhalb klassenspezifischer Suchradien um das Ankersymbol 
gesucht. Die Auswahl erfolgt nach:

\begin{itemize}
  \item Minimaler euklidischer Distanz
  \item Richtungskonformität (z.B. \enquote{unterhalb} für Koordinaten)
  \item OCR-Konfidenz bei mehreren Kandidaten
\end{itemize}

\subsubsection{Speziallogiken}

Für komplexe Entitäten wurden spezialisierte Verknüpfungsalgorithmen entwickelt:

\begin{itemize}
  \item \textbf{Fahrtrichtungserkennung}: Geometrische Ableitung aus der relativen 
        Position von Signal und Gleiskoppelspule(festkodiert)
  \item \textbf{Haltepunkt-Gruppierung}: Kollinearitätstest zur Verknüpfung von 
        Haltepunkt-Symbol, Signal und Koordinate
\end{itemize}

Die mathematische Formulierung und algorithmischen Details werden im Abschnitt \ref{sec:intelligentesymboltextverknüpfung} dargelegt.

\section{Validierung und Qualitätssicherung}
\label{sec:validierungundqualitätssicherung}
Das Validierungskonzept adressiert die Anforderungen \textbf{FA-008} (manuelle Korrektur durch Human-in-the-Loop), \textbf{NFA-004} (Robustheit durch Fehlerbehandlung) und 
\textbf{NFA-005} (Prüfbarkeit durch strukturierte Validierungsberichte).Das Validierungskonzept gliedert sich in drei Ebenen:

\begin{enumerate}
    \item \textbf{Symbolvalidierung}: Prüfung erkannter Objekte gegen erwartete 
    Bounding-Box-Geometrien (Größe, Seitenverhältnis, Kompaktheit).
    
    \item \textbf{OCR-Validierung}: Regex-basierter Abgleich extrahierter Texte 
    mit domänenspezifischen Mustern (Signal: \texttt{[A-Z]\{1,4\}\textbackslash d\{1,4\}}, 
    Koordinate: \texttt{\textbackslash d+[.,]\textbackslash d+}).
    
    \item \textbf{Linking-Validierung}: Plausibilitätsprüfung räumlicher Assoziationen 
    (Distanzschwellenwerte, Richtungskonformität).
\end{enumerate}

Fehlgeschlagene Validierungen werden zur manuellen Prüfung markiert (Human-in-the-Loop). 
Implementierung in Kapitel 6.5.
\chapter{Implementierung}
\label{chap:implementierung}
Dieses Kapitel beschreibt die technische Realisierung des Prototyps. Es werden die Implementierungsdetails der in Kapitel \ref{chap:konzeption} konzipierten Architektur dargelegt, von der Datenaufbereitung über das Modelltraining bis zur finalen Datenaggregation.

\section{Technologiestack und Entwicklungsumgebung}

Die Implementierung des Prototyps basiert auf bewährten Open-Source-Technologien und -Bibliotheken. Die Auswahl des Technologiestacks erfolgte unter Berücksichtigung von Leistung, Wartbarkeit, Datenschutz und der Verfügbarkeit von Community-Support.

\subsection{Programmiersprache und Kernbibliotheken}

Die gesamte Implementierung erfolgte in \textbf{Python 3.9}, da diese Sprache ein umfangreiches Ökosystem für maschinelles Lernen, Bildverarbeitung und Datenmanipulation bietet und eine schnelle Prototypenentwicklung ermöglicht.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|l|l|p{7cm}|}
\hline
\textbf{Komponente} & \textbf{Bibliothek} & \textbf{Version} & \textbf{Einsatzzweck} \\
\hline
\multicolumn{4}{|c|}{\textit{Maschinelles Lernen}} \\
\hline
Deep Learning & PyTorch & 2.0.1 & Framework für neuronale Netze \\
Objekterkennung & Ultralytics & 8.0.196 & YOLOv8 Implementierung \\
OCR (primär) & PaddleOCR & 2.7.0 & Hauptengine für Texterkennung \\
OCR (Fallback 1) & Tesseract & 5.3.0 & Fallback für niedrig aufgelöste oder verrauschte Texte \\
OCR (Fallback 2) & EasyOCR & 1.7.0 & Optionale dritte Engine \\
\hline
\multicolumn{4}{|c|}{\textit{Bildverarbeitung}} \\
\hline
Bildmanipulation & OpenCV & 4.8.0 & Preprocessing, Transformationen \\
Bildoperationen & Pillow & 10.0.0 & Zusätzliche Bildoperationen \\
PDF-Verarbeitung & PyMuPDF & 1.23.3 & PDF-zu-Bild Konvertierung \\
\hline
\multicolumn{4}{|c|}{\textit{Datenverarbeitung}} \\
\hline
Numerik & NumPy & 1.24.3 & Array-Operationen, Mathematik \\
Datenstrukturen & Pandas & 2.0.3 & Tabellarische Datenmanipulation \\
\hline
\multicolumn{4}{|c|}{\textit{Benutzeroberfläche \& Persistenz}} \\
\hline
GUI-Framework & PyQt5 & 5.15.9 & Desktop-Anwendung \\
Datenbank & PostgreSQL & 14.9 & Datenpersistenz \\
ORM & psycopg2 & 2.9.7 & PostgreSQL-Adapter \\
\hline
\end{tabular}
\caption{Übersicht der verwendeten Technologien und Bibliotheken}
\label{tab:tech_stack}
\end{table}
\textbf{Lizenzkonformität (NFA-002):} Alle verwendeten Bibliotheken unterliegen 
Open-Source-Lizenzen (Apache 2.0, MIT, BSD), die für den kommerziellen Einsatz 
bei Siemens Mobility geeignet sind. Eine detaillierte Lizenzprüfung wurde durchgeführt 
und bestätigt die Konformität mit den Unternehmensrichtlinien.
\subsection{Entwicklungsumgebung}
Die Entwicklung erfolgte in Visual Studio Code mit Git-Versionskontrolle. 
Die Annotation wurde mittels CVAT (Version 2.5.0) in lokaler Docker-Umgebung 
durchgeführt \cite{cvat}. Abhängigkeiten sind in \texttt{requirements.txt} dokumentiert.


\subsection{Hardware-Infrastruktur}

Das YOLOv8-Training erfolgte auf einer GPU-Instanz (NVIDIA, AWS Instance g4dn.xlarge, 16 GB VRAM). Die finale Anwendung ist CPU-kompatibel (Intel Core i7, 32 GB RAM), um eine breite Einsetzbarkeit ohne spezialisierte Hardware zu gewährleisten.

\subsection{Projektstruktur}

Die Implementierung folgt einer modularen Architektur mit klarer Trennung der Verantwortlichkeiten:

\begin{itemize}
    \item \texttt{core/} - Kernlogik der Pipeline
    \begin{itemize}
        \item \texttt{detection.py} - YOLO Objekterkennung
        \item \texttt{ocr.py} - OCR-Engine Wrapper
        \item \texttt{linking.py} - Symbol-Text Verknüpfung
    \end{itemize}
    \item \texttt{ui/} - PyQt5 Benutzeroberfläche
    \item \texttt{utils/} - Hilfsfunktionen
    \item \texttt{validation/} - Validierungsregeln
    \item \texttt{database.py} - Datenbankzugriff
    \item \texttt{main.py} - Haupteinstiegspunkt
\end{itemize}

\textbf{Modularität (FA-014, NFA-008):} Die dargestellte Projektstruktur gewährleistet 
eine klare Trennung der Verantwortlichkeiten. Jedes Modul (\texttt{core/}, \texttt{ui/}, 
\texttt{validation/}) kann unabhängig weiterentwickelt werden. Die Integration neuer 
Symbolklassen erfordert lediglich Anpassungen in \texttt{core/detection.py} und 
\texttt{core/linking.py}, ohne die UI- oder Export-Logik zu modifizieren.

\section{Objekterkennung mit YOLOv8-OBB}
\label{sec:objekterkennungmityolov8obb}
Die Objekterkennung bildet die erste und fundamentale Stufe der Extraktionspipeline. Ihre Aufgabe besteht darin, alle relevanten Symbole in den Gleisplänen zu lokalisieren und zu klassifizieren. Die Qualität dieser Detektionen bestimmt maßgeblich die Leistungsfähigkeit der nachfolgenden OCR- und Verknüpfungsmodule. Die Wahl fiel auf YOLOv8 in der Oriented Bounding Box (OBB) Variante, da diese Architektur speziell für die Detektion rotierter Objekte konzipiert wurde und somit optimal für technische Zeichnungen mit beliebig orientierten Symbolen geeignet ist.

\subsection{Datensatzerstellung und Annotation}
\label{subsec:Datensatzerstellung}
Die Leistungsfähigkeit eines objekterkennenden Modells hängt direkt von der Qualität und Quantität des Trainingsdatensatzes ab. Die Erstellung des Datensatzes erfolgte in einem mehrstufigen Prozess, der sowohl die Vorverarbeitung der Rohdaten als auch die präzise Annotation der Symbole umfasste.

\subsubsection{Vorverarbeitung der Gleispläne}

Die initialen Gleispläne lagen im PDF-Format vor und mussten zunächst in ein für die Annotation geeignetes Bildformat überführt werden. Hierzu wurde die Python-Bibliothek PyMuPDF eingesetzt, welche eine zuverlässige Rasterisierung von PDF-Dokumenten ermöglicht. Die Konvertierung erfolgte mit einer Auflösung von 500 DPI (dots per inch), was einem Skalierungsfaktor von $\frac{500}{72} \approx 6.94$ gegenüber der Standard-PDF-Auflösung entspricht. Diese hohe Auflösung wurde gewählt, um auch kleine Symbole wie Gleiskoppelspulenplatten oder Isolierstöße mit ausreichender Detailgenauigkeit darzustellen.

Nach der Rasterisierung wurden die hochauflösenden Bilder in kleinere, überlappende Bildausschnitte segmentiert. Die gewählte Segmentgröße für die Annotation und das Training betrug $1024 \times 1024$ Pixel, entsprechend der YOLO-Eingabegröße. Für die Inferenz werden größere Tiles ($2048 \times 2048$ Pixel) verwendet und vor der Modellverarbeitung auf $1024 \times 1024$ Pixel herunterskaliert (siehe Abschnitt~\ref{Inferenz}). Diese Dimensionierung stellt einen Kompromiss dar zwischen ausreichendem Kontext für die Objekterkennung und der effizienten Verarbeitung auf GPU-Systemen mit begrenztem Videospeicher. Größere Tiles würden zwar mehr Kontext bieten, jedoch den verfügbaren VRAM der Trainings-GPU (NVIDIA, AWS Instance g4dn.xlarge, 16 GB VRAM) überschreiten und damit das Training unmöglich machen oder die Batch-Größe unzulässig reduzieren.

\subsubsection{Annotationsprozess}

Die Beschriftung erfolgte mittels CVAT \cite{cvat} in lokaler Docker-Umgebung. 
Jedes Symbol wurde mit einer Oriented Bounding Box (OBB) annotiert (vgl. 
Abschnitt \ref{sec:orientedboundingboxesalslösung}), repräsentiert durch Zentrum $(c_x, c_y)$, Dimensionen 
$(w, h)$ und Rotationswinkel $\theta$:

$$\text{OBB} = (c_x, c_y, w, h, \theta)$$

CVAT exportiert im YOLO-OBB-Format mit normalisierten Eckpunktkoordinaten 
$\{(x_i/W, y_i/H)\}_{i=1}^{4}$.

\subsubsection{Datenaufteilung und Statistiken}

Der finale Datensatz umfasst insgesamt 13 Symbolklassen, darunter Signale, verschiedene Weichentypen, Koordinatenbeschriftungen, Haltepunkte, Isolierstöße und weitere eisenbahnspezifische Elemente. Die Daten wurden in einen Validierungssatz mit 115 Bildern und 1.305 annotierten Objektinstanzen aufgeteilt. Der Trainingssatz umfasste entsprechend die verbliebenen Bilder.

Die Verteilung der Objektinstanzen über die Klassen hinweg zeigt Tabelle \ref{tab:dataset_distribution}:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Klasse} & \textbf{Bilder} & \textbf{Instanzen} \\
\hline
\multicolumn{3}{|c|}{\textit{Kernklassen (produktionsrelevant)}} \\
\hline
coordinate & 113 & 629 \\
signal & 81 & 218 \\
gks\_gesteuert & 67 & 69 \\
gm\_block & 55 & 91 \\
gks\_festkodiert & 43 & 60 \\
\hline
\textbf{Summe Kernklassen} & --- & \textbf{1.067 (81.8\%)} \\
\hline
\multicolumn{3}{|c|}{\textit{Auxiliarklassen (Erweiterbarkeit)}} \\
\hline
sverbinder & 81 & 85 \\
haltepunkt & 47 & 47 \\
weichen\_block & 25 & 39 \\
isolierstoß & 16 & 27 \\
prellblock & 14 & 16 \\
haltetafel & 11 & 11 \\
weichengruppeende & 9 & 9 \\
endeweichen & 4 & 4 \\
\hline
\textbf{Summe Auxiliarklassen} & --- & \textbf{238 (18.2\%)} \\
\hline
\hline
\textbf{Gesamt} & \textbf{115} & \textbf{1.305} \\
\hline
\end{tabular}
\caption{Verteilung der Annotationen über Klassen und Bilder im Validierungssatz, unterteilt nach Kern- und Auxiliarklassen (vgl. Anforderung FA-003)}
\label{tab:dataset_distribution}
\end{table}

Es ist ersichtlich, dass die Klassenverteilung unausgeglichen ist, mit Koordinaten und Signalen als dominierende Klassen. Diese Verteilung spiegelt die reale Häufigkeit der Symbole in Gleisplänen wider. Um ein Overfitting auf häufige Klassen zu vermeiden, wurde während des Trainings eine klassengewichtete Verlustfunktion zwar in Betracht gezogen, letztendlich jedoch zugunsten einer robusten Datenaugmentation verworfen.
\textbf{Klassenpriorisierung:} Gemäß Anforderung \textbf{FA-003} werden die Objektklassen 
in zwei Kategorien unterteilt. Die fünf \textit{Kernklassen} (signal, coordinate, 
gks\_festkodiert, gks\_gesteuert, gm\_block) repräsentieren die für die Projektierung 
bei Siemens Mobility essentiellen Symbole und bilden den Fokus der End-to-End-Evaluation 
in Kapitel~\ref{chap:evaluation}. Die acht \textit{Auxiliarklassen} wurden zusätzlich 
implementiert, um die Erweiterbarkeit des Systems zu demonstrieren (vgl. Anforderung 
\textbf{NFA-008}), werden jedoch nicht im Detail evaluiert.
Die Daten wurden in eine standardisierte COCO-Verzeichnisstruktur (Abbildung \ref{fig:COCODatenset}) überführt, welche die Organisation von Trainings- und Validierungsdaten sowie die zugehörigen Annotationen in separaten Verzeichnissen vorsieht.

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{images/Kapitel6/cocodatasetconfig.png} 
    \caption{COCO-konforme Datensetstruktur für YOLOv8-OBB Training}
    \label{fig:COCODatenset}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images/Kapitel7/labels.jpg}
    \caption{Charakteristika des Trainingsdatensatzes: (oben links) Klassenverteilung mit absoluten Instanzzahlen, (unten links) räumliche Verteilung der Bounding-Box-Zentren, (unten rechts) Größenverteilung der annotierten Objekte in normalisierten Koordinaten.}
    \label{fig:labels_distribution}
\end{figure}
Abbildung~\ref{fig:labels_distribution} visualisiert die Charakteristika des annotierten Trainingsdatensatzes. Die Klassenverteilung zeigt eine deutliche Dominanz der Koordinaten-Klasse (5.945 Instanzen), gefolgt von Signalen (1.875 Instanzen). Diese Ungleichverteilung spiegelt die tatsächliche Häufigkeit der Symbole in realen Gleisplänen wider und wurde bewusst beibehalten, um das Modell auf praxisnahe Verteilungen zu trainieren. Die räumliche Verteilung der Bounding-Box-Zentren zeigt eine gleichmäßige Abdeckung des Bildbereichs, während die Größenverteilung die charakteristisch kleinen Objektgrößen der Gleisplansymbole verdeutlicht.
\subsubsection{Synthetische Datenaugmentation durch Rotation}

Um die Robustheit des Modells gegenüber beliebigen Symbolorientierungen zu verbessern und die Klassenbalance zu optimieren, wurde eine gezielte synthetische Augmentation durch Rotation implementiert. Diese Strategie adressiert zwei zentrale Herausforderungen:

\begin{enumerate}
    \item \textbf{Klassenungleichgewicht}: Die natürliche Verteilung der Symbolklassen im Datensatz ist stark unausgeglichen (vgl. Tabelle~\ref{tab:dataset_distribution}), mit dominierenden Klassen wie \textit{coordinate} (629 Instanzen) und unterrepräsentierten Klassen wie \textit{endeweichen} (4 Instanzen).
    
    \item \textbf{Orientierungsvielfalt}: Obwohl technische Zeichnungen Symbole in verschiedenen Winkeln enthalten, sind bestimmte Orientierungen (insbesondere $0^\circ$ und $90^\circ$) überrepräsentiert, während diagonale Ausrichtungen seltener auftreten.
\end{enumerate}

\textbf{Augmentationsstrategie:}

Die synthetische Rotation wurde selektiv auf unterrepräsentierte Klassen angewendet. Für jede Instanz einer Zielklasse wurden systematische Rotationen um folgende Winkel durchgeführt:

$$\Theta_{\text{aug}} = \{-90^\circ, -60^\circ, -45^\circ, -30^\circ, -15^\circ, +15^\circ, +30^\circ, +45^\circ, +60^\circ, +90^\circ\}$$

Die Rotation erfolgte um den Zentroid $(c_x, c_y)$ der Oriented Bounding Box, wobei sowohl die Bildregion als auch die OBB-Koordinaten entsprechend transformiert wurden. Die Rotationsmatrix für einen Winkel $\theta$ lautet:

$$\mathbf{R}(\theta) = \begin{pmatrix} 
\cos(\theta) & -\sin(\theta) \\ 
\sin(\theta) & \cos(\theta) 
\end{pmatrix}$$

\textbf{Rotationsinvarianz (FA-002):} Die synthetische Augmentation mit 10 
Rotationswinkeln ($\Theta_{\text{aug}} = \{-90^\circ, -60^\circ, ..., +90^\circ\}$) stellt sicher, 
dass das Modell Symbole in beliebigen Orientierungen erkennen kann. In Kombination 
mit der OBB-Architektur wird eine vollständige Rotationsinvarianz von $0^\circ$ bis $360^\circ$ 
erreicht.


\textbf{Klassenspezifische Augmentation:}

Die Anzahl der generierten synthetischen Samples wurde klassenabhängig gesteuert, um eine Zielverteilung von ca. 600 Instanzen pro Klasse zu erreichen (Ausnahme: \textit{coordinate} mit Ziel +300 Instanzen aufgrund der bereits hohen Ausgangsanzahl).

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Klasse} & \textbf{Original} & \textbf{Augmentiert} & \textbf{Gesamt} & \textbf{Faktor} \\
\hline
\multicolumn{5}{|c|}{\textit{Kernklassen}} \\
\hline
coordinate & 629 & 300 & 929 & 1.48x\\
signal & 218 & 382 & 600 & 2.75x \\
gks\_gesteuert & 69 & 531 & 600 & 8.70x \\
gm\_block & 91 & 509 & 600 & 6.59x \\
gks\_festkodiert & 60 & 540 & 600 & 10.00x \\
\hline
\textbf{Summe Kernklassen} & \textbf{1.067} & \textbf{2.262} & \textbf{3.329} & --- \\
\hline
\multicolumn{5}{|c|}{\textit{Auxiliarklassen}} \\
\hline
sverbinder & 85 & 515 & 600 & 7.06x\\
haltepunkt & 47 & 553 & 600 & 12.77x\\
weichen\_block & 39 & 561 & 600 & 15.38x\\
isolierstoß & 27 & 573 & 600 & 22.22x \\
prellblock & 16 & 584 & 600 & 37.50x \\
haltetafel & 11 & 589 & 600 & 54.55x \\
endeweichen & 4 & 596 & 600 & 150.00x \\
weichengruppeende & 9 & 591 & 600 & 66.67x \\
\hline
\textbf{Summe Auxiliarklassen} & \textbf{238} & \textbf{4.462} & \textbf{4.700} & --- \\
\hline
\hline
\textbf{Gesamt} & \textbf{1.305} & \textbf{6.724} & \textbf{8.029} & \textbf{6.15x} \\
\hline
\end{tabular}
\caption{Verteilung der Instanzen vor und nach synthetischer Rotation, unterteilt nach Kern- und Auxiliarklassen}
\label{tab:augmentation_statistics}
\end{table}

\textbf{Technische Umsetzung:}
Die hohen Augmentationsfaktoren für seltene Klassen (z.B. 150x für \textit{endeweichen}) sind gerechtfertigt, da diese Symbole in realen Gleisplänen tatsächlich sehr selten vorkommen, das Modell aber dennoch in der Lage sein muss, sie zuverlässig zu erkennen. Die synthetische Rotation simuliert realistische Orientierungsvariationen und verhindert Overfitting auf die wenigen Original-Samples. 

Die finale Datensatzaufteilung erfolgte automatisch durch YOLOv8s integrierte Split-Funktion mit einem 80/20-Verhältnis, was zu 923 Trainings- und 208 Validierungsbildern führte. Nach manueller Filterung leerer oder irrelevanter Tiles verblieben 115 Validierungsbilder (1.305 Instanzen), die als dedizierter Testdatensatz für die Evaluation in Kapitel~\ref{chap:evaluation} dienen.
Die Rotation wurde in einem Preprocessing-Schritt vor dem Training durchgeführt und die augmentierten Samples wurden als separate Einträge im Trainingsdatensatz gespeichert. Dies ermöglicht eine deterministische Reproduzierbarkeit der Augmentation. Die Implementation erfolgte mittels OpenCV's \texttt{cv2.getRotationMatrix2D()} und \texttt{cv2.warpAffine()}, wobei die Bildgröße dynamisch angepasst wurde, um ein Abschneiden rotierter Objekte zu vermeiden.

\textbf{Validierung der Augmentation:}

Um sicherzustellen, dass die synthetische Rotation keine Artefakte einführt, wurden die augmentierten Samples manuell gesichtet. Die OBB-Annotationen wurden entsprechend transformiert und auf geometrische Korrektheit geprüft.
\subsection{Modelltraining und Optimierung}
\label{subsec:Modelltraining}
Das Training des YOLOv8-OBB Modells wurde auf einer cloud-basierten GPU-Instanz mit einer NVIDIA AWS Instance g4dn.xlarge, (16 GB VRAM) durchgeführt. Die Wahl einer externen GPU-Instanz war notwendig, da lokale Entwicklungsmaschinen nicht über ausreichende Rechenleistung verfügen, um das Training in angemessener Zeit abzuschließen.

\subsubsection{Modellarchitektur und Hyperparameter}

Als Basismodell wurde YOLOv8l-OBB gewählt, die \enquote{Large}-Variante von YOLOv8 mit OBB-Unterstützung. Diese Modellgröße stellt eine Balance zwischen Detektionspräzision und Inferenzgeschwindigkeit dar. Kleinere Varianten (YOLOv8n, YOLOv8s) wären zwar schneller, zeigen jedoch bei komplexen Szenen mit kleinen Objekten deutliche Leistungseinbußen. Größere Varianten (YOLOv8x) würden nur marginale Verbesserungen bringen, bei gleichzeitig erheblich längerer Trainings- und Inferenzzeit.

Die vollständige Konfiguration der Trainingsparameter ist in Tabelle \ref{tab:yolo_training_params} dargestellt:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|p{6cm}|}
\hline
\textbf{Parameter} & \textbf{Wert} & \textbf{Begründung} \\
\hline
Modellgröße & YOLOv8l-OBB & Balance zwischen Präzision und Inferenzgeschwindigkeit \\
Eingabeauflösung & $1024 \times 1024$ & Angepasst an Tile-Größe der Annotationen \\
Epochen & 120 & Ausreichend für vollständige Konvergenz ohne Overfitting \\
Batch Size & 6 & Maximal mögliche Größe bei 16 GB VRAM der T4 GPU \\
Optimizer & SGD & Momentum = 0.937, Weight Decay = 0.0005 \\
Learning Rate & 0.01 & Initiale LR mit Cosine Annealing Scheduler \\
Warmup Epochs & 3 & Graduelle Erhöhung der LR zu Trainingsbeginn \\
Workers & 2 & Moderat gewählt zur Vermeidung von Speicherüberläufen (Out of Memory) im Dataloader \\
Cache & Disk & Disk-Caching für deterministische Reproduzierbarkeit \\
Seed & 0 & Fixierter Seed für vollständige Reproduzierbarkeit \\
\hline
\end{tabular}
\caption{YOLOv8-OBB Trainingsparameter}
\label{tab:yolo_training_params}
\end{table}

Die Batch-Größe von 6 wurde empirisch durch schrittweise Erhöhung ermittelt, bis die GPU-Speichergrenze erreicht wurde. Eine größere Batch-Größe würde zwar stabilere Gradienten liefern, ist jedoch durch die Hardware-Limitierung nicht realisierbar.

Der Stochastic Gradient Descent (SGD) Optimizer wurde gegenüber adaptiven Optimierern wie Adam bevorzugt, da SGD in der Regel zu Modellen mit besserer Generalisierungsfähigkeit führt, auch wenn die Konvergenz langsamer verläuft. Der Momentum-Term von 0.937 beschleunigt die Konvergenz in relevanten Richtungen und dämpft Oszillationen.

Die Learning Rate wurde mittels Cosine Annealing Scheduler von einem initialen Wert von 0.01 graduell auf nahezu 0 reduziert. Diese Strategie ermöglicht zu Beginn des Trainings große Schritte im Parameterraum und verfeinert gegen Ende die Gewichte durch immer kleinere Anpassungen.

\subsubsection{Datenaugmentation}

Um die Robustheit des Modells gegenüber Variationen in den Eingangsdaten zu erhöhen und Overfitting zu vermeiden, wurden verschiedene Augmentierungstechniken angewendet. Die Konfiguration der Augmentierungsparameter zeigt Tabelle \ref{tab:augmentation}:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|p{7cm}|}
\hline
\textbf{Augmentation} & \textbf{Parameter} & \textbf{Beschreibung} \\
\hline
Rotation & $0.0^\circ$ & Deaktiviert, da OBB bereits Rotationsinvarianz bietet \\
Translation & $\pm 5\%$ & Horizontale und vertikale Verschiebung um bis zu 5\% der Bildgröße \\
Skalierung & $\pm 20\%$ & Zufällige Vergrößerung oder Verkleinerung um bis zu 20\% \\
Scherung & $0.0^\circ$ & Deaktiviert, um geometrische Integrität zu bewahren \\
Perspektive & 0.0 & Deaktiviert aus gleichem Grund \\
Horizontal Flip & 20\% & Spiegelung mit 20\% Wahrscheinlichkeit \\
Vertical Flip & 0\% & Deaktiviert, da vertikale Orientierung semantisch relevant ist \\
Mosaic & 40\% & Kombination von 4 Bildern, deaktiviert ab Epoch 110 \\
Mixup & 10\% & Lineare Überlagerung zweier Bilder mit 10\% Wahrscheinlichkeit \\
\hline
\end{tabular}
\caption{Datenaugmentierungsparameter}
\label{tab:augmentation}
\end{table}

Die Deaktivierung der Rotationsaugmentation mag zunächst kontraintuitiv erscheinen, da die OBB-Architektur explizit für rotierte Objekte entwickelt wurde. Jedoch sind die Rotationswinkel der Symbole bereits in den OBB-Annotationen kodiert, sodass eine zusätzliche künstliche Rotation redundant wäre. Stattdessen lernt das Modell, die Orientierung direkt aus den rotierten Bounding Boxes zu extrahieren.

Die Mosaic-Augmentation kombiniert vier zufällig ausgewählte Trainingsbilder zu einem neuen Bild, indem sie in einem 2x2-Raster angeordnet werden. Diese Technik zwingt das Modell, Objekte auch in ungewöhnlichen Kontexten und bei veränderten Größenverhältnissen zu erkennen. Die Deaktivierung der Mosaic-Augmentation in den letzten 10 Epochen (close\_mosaic = 10) ermöglicht dem Modell, sich in der Feinabstimmungsphase auf realistische, unveränderte Bilder zu fokussieren.

Die Trainingsdauer betrug insgesamt ca. 8 Stunden bei kontinuierlicher GPU-Auslastung von nahezu 100\%.

\subsubsection{Trainingsergebnisse}

Das Training wurde über 120 Epochen durchgeführt und erreichte eine stabile Konvergenz 
ohne Anzeichen von Overfitting. Die Trainingsverläufe (Abbildung \ref{fig:yolo_training_curves}) 
illustrieren die Entwicklung der Verlustfunktionen über den Trainingsverlauf.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/Kapitel6/results.png}
\caption{Trainings- und Validierungsmetriken über 120 Epochen. Die Kurven zeigen eine 
stabile Konvergenz ohne Anzeichen von Overfitting.}
\label{fig:yolo_training_curves}
\end{figure}

Folgende Beobachtungen lassen sich aus den Kurven ableiten:

\begin{itemize}
    \item Die Box-Loss-Kurve fällt kontinuierlich und stabilisiert sich nach ca. 80 Epochen 
    auf einem niedrigen Niveau, was auf eine erfolgreiche Lokalisierungsoptimierung hindeutet.
    
    \item Die Classification-Loss-Kurve zeigt einen ähnlichen Verlauf, was die erfolgreiche 
    Konvergenz der Klassifikation belegt.
    
    \item Kritisch ist die Beobachtung, dass zwischen Trainings- und Validierungskurven 
    kein signifikanter Gap existiert. Dies belegt, dass das Modell gut generalisiert und 
    nicht auf den Trainingsdaten overfittet.
\end{itemize}
\textbf{Update-Fähigkeit (NFA-009):} Die trainierten Modellgewichte werden als 
externe Datei (\texttt{best.pt}) gespeichert und können ohne Code-Änderungen 
ausgetauscht werden. Bei Bedarf an neuen Symbolklassen kann ein Nachtraining 
(Fine-Tuning) auf dem bestehenden Modell durchgeführt werden.
Die detaillierte quantitative Auswertung der Detektionsleistung sowie die Analyse der 
Konfusionsmatrix erfolgen in Kapitel \ref{chap:evaluation}.

\subsection{Inferenz-Pipeline Implementierung}
\label{Inferenz}
Nach Abschluss des Trainings wurde das Modell mit den optimalen Gewichten (\texttt{best.pt}) in die produktive Extraktionspipeline integriert. Die Inferenz auf neuen, zuvor ungesehenen Gleisplänen erfolgt in mehreren aufeinander aufbauenden Schritten. Abbildung \ref{fig:pipeline_flowchart} zeigt den vollständigen Ablauf der Inferenz-Pipeline von der PDF-Eingabe über das Tiling bis zur Rücktransformation der globalen Koordinaten.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        node distance=0.8cm, % Abstand zwischen den Boxen
        font=\small,
        % --- STYLES ---
        % 1. Prozess-Schritte (Rechtecke)
        process/.style={
            rectangle, 
            draw=blue!60!black, 
            fill=blue!5, 
            text width=4cm, 
            align=center, 
            minimum height=1cm,
            rounded corners=2pt,
            thick,
            drop shadow % Schatten für Tiefe
        },
        % 2. Input/Output Daten (Trapeze oder andere Farbe)
        data/.style={
            rectangle,
            draw=gray!60!black,
            fill=gray!10,
            text width=3.5cm,
            align=center,
            minimum height=0.8cm,
            dashed, % Gestrichelt für Dateien/Daten
            thick
        },
        % 3. Parameter-Notiz
        note/.style={
            rectangle, 
            draw=none, 
            fill=yellow!10, 
            text width=3.5cm, 
            align=left, 
            font=\footnotesize
        },
        % 4. Pfeile
        arrow/.style={
            -Stealth, % Moderne Pfeilspitze
            thick,
            draw=black!80
        }
    ]

    % --- KNOTEN ---
    
    % Input
    \node[data] (pdf) {\textbf{Input:}\\PDF-Gleisplan};

    % Schritte
    \node[process, below=of pdf] (raster) {Rasterisierung\\(Rendering @ 500 DPI)};
    
    \node[process, below=of raster] (tile) {Tiling / Slicing\\($2048 \times 2048$ px)};
    
    \node[process, below=of tile, fill=green!10, draw=green!60!black] (yolo) {\textbf{YOLO Inferenz}\\(Objekterkennung)};
    
    \node[process, below=of yolo] (coord) {Koordinaten-\\Rücktransformation};
    
    \node[process, below=of coord] (nms) {Post-Processing\\(NMS \& Filterung)};

    % Output
    \node[data, below=of nms] (out) {\textbf{Output:}\\Strukturierte Detektionsliste\\(JSON / DataFrame)};

    % --- PFEILE ---
    \draw[arrow] (pdf) -- (raster);
    \draw[arrow] (raster) -- (tile);
    \draw[arrow] (tile) -- (yolo);
    \draw[arrow] (yolo) -- (coord);
    \draw[arrow] (coord) -- (nms);
    \draw[arrow] (nms) -- (out);

    % --- ANNOTATIONEN (Seitlich) ---
    % Verbindungslinie zu Tiling
    \node[note, right=1cm of tile] (tilingparams) {
        \textbf{Parameter:}\\
        $\bullet$ Stride: 1792 px\\
        $\bullet$ Halo: 128 px\\
        $\bullet$ Overlap: $\sim$12.5\%
    };
    \draw[dashed, gray] (tile) -- (tilingparams);

    \end{tikzpicture}
    \caption{Ablauf der Inferenz-Pipeline}
    \label{fig:pipeline_flowchart}
\end{figure}

\FloatBarrier
\subsubsection{PDF-zu-Bild-Konvertierung}

Der erste Schritt besteht in der Konvertierung der PDF-Gleispläne in hochauflösende Rastergrafiken. Hierzu wird die Bibliothek PyMuPDF (auch bekannt als \texttt{fitz}) verwendet. Für jede Seite des PDF-Dokuments wird eine Transformationsmatrix $\mathbf{M}$ definiert, die die Skalierung von der Standard-PDF-Auflösung von 72 DPI auf die Zielauflösung von 500 DPI beschreibt:

\begin{equation}
\mathbf{M} = \begin{pmatrix}
\frac{500}{72} & 0 \\
0 & \frac{500}{72}
\end{pmatrix} \approx \begin{pmatrix}
6.94 & 0 \\
0 & 6.94
\end{pmatrix}
\end{equation}

Die resultierende Pixmap wird in ein RGB-Array mit den Dimensionen $H \times W \times 3$ überführt, wobei $H$ und $W$ von der Größe des PDF-Dokuments abhängen. Ein typischer A3-Gleisplan resultiert bei 500 DPI in einem Bild von ca. $8000 \times 6000$ Pixeln.

\subsubsection{Tiling-Strategie}
\label{subsubsec:tiling_strategie}
Da die resultierenden Bilder mit mehreren Millionen Pixeln die Eingabegröße des YOLO-Modells ($1024 \times 1024$) bei weitem überschreiten, wird das Vollbild in überlappende Kacheln (Tiles) zerlegt. Die Tile-Größe beträgt $T = 2048 \times 2048$ Pixel, was eine Verarbeitung auf gängigen GPUs ermöglicht und gleichzeitig ausreichend Kontext für die Detektion bietet.Die extrahierten Tiles mit einer Größe von $2048 \times 2048$ Pixeln werden vor der YOLO-Inferenz auf die Trainingsgröße von $1024 \times 1024$ Pixeln herunterskaliert (Downsampling). Die resultierenden Bounding-Box-Koordinaten werden anschließend mit dem Faktor 2 multipliziert, um die korrekten Positionen im Originaltile zu erhalten. Dieses Vorgehen ermöglicht eine höhere effektive Auflösung bei der Symbolerkennung, da feinere Details im größeren Eingabebild erhalten bleiben, bevor das Downsampling erfolgt.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        scale=0.85, 
        transform shape,
        font=\small,
        % --- STYLES ---
        tile/.style={
            fill=blue!5, 
            draw=blue!60!black, 
            line width=1.2pt,
            fill opacity=0.8
        },
        overlap/.style={
            fill=red!30, 
            draw=red!80!black, 
            line width=0.8pt,
            dashed,
            fill opacity=0.5
        },
        % Style for braces
        fetteKlammer/.style={
            decorate, 
            decoration={brace, amplitude=6pt, raise=2pt}, 
            line width=1.2pt
        },
        fetteKlammerMirror/.style={
            decorate, 
            decoration={brace, mirror, amplitude=6pt, raise=2pt}, 
            line width=1.2pt
        },
        fetterPfeil/.style={
            ->, 
            line width=1.2pt, 
            >={Stealth[length=3mm, width=2mm]}
        }
    ]

    % --- CONFIGURATION ---
    % Tile Width: 4
    % Tile Height: 4 (Square)
    % Overlap: 0.5
    % Stride: 3.5 (4 - 0.5)

    % --- ROW 1 (TOP) ---
    % Y-Start = 3.5 (because bottom row ends at 4, minus 0.5 overlap)
    % Y-End = 7.5
    
    % T1 (Top Left)
    \draw[tile] (0, 3.5) rectangle (4, 7.5);
    \node[blue!60!black] at (1, 7) {\textbf{T1}};

    % T2 (Top Mid) - X-Start = 3.5
    \draw[tile] (3.5, 3.5) rectangle (7.5, 7.5);
    \node[blue!60!black] at (5.75, 7) {\textbf{T2}};

    % T3 (Top Right) - X-Start = 7
    \draw[tile] (7, 3.5) rectangle (11, 7.5);
    \node[blue!60!black] at (9.5, 7) {\textbf{T3}};

    % --- ROW 2 (BOTTOM) ---
    % Y-Start = 0
    % Y-End = 4
    
    % T4 (Bot Left)
    \draw[tile] (0, 0) rectangle (4, 4);
    \node[blue!60!black] at (1, 1) {\textbf{T4}};

    % T5 (Bot Mid)
    \draw[tile] (3.5, 0) rectangle (7.5, 4);
    \node[blue!60!black] at (5.75, 1) {\textbf{T5}};

    % T6 (Bot Right)
    \draw[tile] (7, 0) rectangle (11, 4);
    \node[blue!60!black] at (9.5, 1) {\textbf{T6}};

    % --- OVERLAPS ---
    % 1. Vertical Strips (Horizontal Overlap)
    % Height is now from 0 to 7.5 (Full height of diagram)
    \draw[overlap] (3.5, 0) rectangle (4, 7.5);
    \draw[overlap] (7, 0) rectangle (7.5, 7.5);

    % 2. Horizontal Strip (Vertical Overlap)
    % Between Y=3.5 and Y=4
    \draw[overlap] (0, 3.5) rectangle (11, 4);

    % --- LABELS ---

    % X-Axis (Top)
    \draw[fetteKlammer] (0, 7.6) -- (4, 7.6) 
        node[midway, above=8pt] {Tile Width};
        

    % Overlap X Arrow
    \draw[fetterPfeil, red!80!black] (3.2, 8.5) -- (3.75, 7.6);
    \node[red!80!black, font=\footnotesize] at (3.2, 8.7) {Overlap X};

    % Stride X (Bottom)
    \draw[fetteKlammerMirror] (0, -0.2) -- (3.5, -0.2) 
        node[midway, below=8pt] {Stride X};

    % Y-Axis (Right)
    
    % Tile Height (Measures the full T6)
    \draw[fetteKlammerMirror] (11.4, 0) -- (11.4, 4) 
        node[midway, right=8pt] {Tile Height};

    % Stride Y (Measures distance from bottom of T6 to bottom of T3)
    % T6 bottom = 0, T3 bottom = 3.5
    \draw[fetteKlammerMirror] (11.8, 4) -- (11.8, 7.5) 
        node[midway, right=8pt] {Stride Y};
        
    % Overlap Y Arrow (Left side)
    \draw[fetterPfeil, red!80!black] (-1.5, 3.75) -- (0, 3.75);
    \node[red!80!black, font=\footnotesize, align=right] at (-1.6, 3.9) {Overlap Y};

    \end{tikzpicture}
    \caption{2D-Tiling-Strategie mit quadratischen Kacheln}
    \label{fig:grid_overlap_square}
\end{figure}

Die Überlappung zwischen benachbarten Tiles beträgt $O = 12.5\%$, was einer absoluten Überlappung von $256$ Pixeln bei einer Tile-Größe von $2048$ Pixeln entspricht. Der Stride (Schrittweite) zwischen Tiles ergibt sich zu:

\begin{equation}
S = T \cdot (1 - O) = 2048 \cdot 0.875 = 1792 \text{ Pixel}
\end{equation}
Abbildung \ref{fig:grid_overlap_square} illustriert diese Strategie, bei der durch die identische Schrittweite in X- und Y-Richtung eine gleichmäßige Abdeckung mit definiertem Überlappungsbereich gewährleistet wird.
Die Überlappung ist essenziell, um zu verhindern, dass Symbole, die nahe an Tile-Grenzen liegen, durch die Segmentierung geteilt werden und somit nicht oder nur unvollständig detektiert werden können. Ein Symbol, das im Überlappungsbereich liegt, wird von mindestens zwei benachbarten Tiles erfasst und hat somit eine hohe Chance, in mindestens einem der Tiles vollständig sichtbar zu sein.

Die Berechnung der Tile-Positionen erfolgt durch Iteration über die Koordinaten:

\begin{equation}
\begin{aligned}
X &= \{x \mid x \in \{0, S, 2S, \ldots\} \land x + T \leq W\} \cup \{W - T\} \\
Y &= \{y \mid y \in \{0, S, 2S, \ldots\} \land y + T \leq H\} \cup \{H - T\}
\end{aligned}
\end{equation}

Die Ergänzung der Mengen um $\{W - T\}$ bzw. $\{H - T\}$ stellt sicher, dass auch der rechte und untere Bildrand vollständig abgedeckt werden, selbst wenn sie nicht durch reguläre Schritte erreichbar sind.

Zusätzlich zur Überlappung wird ein sogenannter Halo um jedes Tile herum hinzugefügt. Der sogenannte Halo bezeichnet einen zusätzlichen Randbereich von $H_{\text{halo}} = 128$ Pixeln, der bei der Bildextraktion mitgeladen wird:

\begin{equation}
\text{Tile}_{\text{mit Halo}} = \text{Bild}[y - H_{\text{halo}} : y + T + H_{\text{halo}}, \; x - H_{\text{halo}} : x + T + H_{\text{halo}}]
\end{equation}

Der Halo stellt zusätzlichen Kontext für Symbole am Tile-Rand bereit, wird jedoch bei der späteren Filterung der Detektionen nicht berücksichtigt. Nur Detektionen, deren Zentrum innerhalb der eigentlichen Tile-Grenzen (ohne Halo) liegt, werden akzeptiert:

\begin{equation}
\text{Akzeptiere Detektion} \Leftrightarrow x \leq c_x \leq x + T \land y \leq c_y \leq y + T
\end{equation}

wobei $(c_x, c_y)$ das Zentrum der detektierten Bounding Box bezeichnet.
Abbildung \ref{fig:halo_tiling} veranschaulicht dieses Prinzip: Das neuronale Netz (YOLO) erhält einen erweiterten Bildausschnitt (grau dargestellt), um angeschnittene Objekte am Rand korrekt zu erkennen. Gewertet werden jedoch nur Detektionen, deren Zentrum im inneren Bereich (grün dargestellt) liegt.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        scale=1.5, 
        >=latex,
        font=\small
    ]
    
    % --- 1. Halo Region (Input für YOLO) ---
    % Graue Box außen
    \draw[fill=gray!10, dashed, thick, draw=gray!60] (0,0) rectangle (6,6);
    % Label oben
    \node[anchor=south] at (3,6) {\textbf{Input für YOLO} (Halo-Region)};
    
    % --- 2. Actual Tile (Valid Zone) ---
    % Grüne Box innen
    \draw[fill=green!5, thick, draw=green!60!black] (0.75,0.75) rectangle (5.25,5.25);
    \node[green!40!black] at (3,3.3) {\textbf{Valid Area} (Tile)};
    \node[green!40!black] at (3,2.8) {$2048 \times 2048$ px};
    
    % --- 3. Dimensionen (Braces/Klammern) ---
    % Wir nutzen braces für "wissenschaftlichen Look"
    \draw[decorate, decoration={brace, amplitude=5pt}, thick] (0, 6.2) -- (0.75, 6.2) 
        node[midway, above=8pt] {\scriptsize 128px};
        
    \draw[decorate, decoration={brace, amplitude=5pt}, thick] (6, 0) -- (6, 6) 
        node[midway, right=8pt, align=left] {Netzwerk-Input\\($2304 \times 2304$)};

    % --- 4. Symbol am Rand (Das Problem) ---
    % Ein Symbol, das auf der Kante liegt
    \draw[fill=blue!20, draw=blue, thick] (5.25, 3) circle (0.4);
    \node[blue] at (5.25, 3) {\textbf{S}};
    
    % Erklärungspfeil zum Symbol
    \draw[<-, thick] (5.5, 3.2) -- (6.5, 3.8) node[right, align=left, font=\footnotesize] {Symbol am Rand:\\Wird erkannt, da\\im Kontext sichtbar};

    % --- 5. Halo-Beschriftung ---
    \node[font=\footnotesize, align=center] at (3, 0.35) {Halo / Padding\\(wird nach Inferenz verworfen)};

    \end{tikzpicture}
    \caption{Visualisierung der Halo-Strategie}
    \label{fig:halo_tiling}
\end{figure}
\FloatBarrier
Der Halo-Bereich von 128 Pixeln dient als zusätzlicher Kontextbereich um jedes Tile und ist unabhängig vom Overlap. Während der Overlap (256 Pixel) die Mindestüberlappung zwischen benachbarten Tiles definiert, um Objekte an Tile-Grenzen zu erfassen, stellt der Halo sicher, dass Objekte nahe dem Tile-Rand ausreichend Kontextinformation für eine zuverlässige Erkennung haben. Detektionen, deren Zentrum im Halo-Bereich liegt, werden beim NMS-Schritt gefiltert.
\subsubsection{Winkeladaptive Parameterauswahl}
\label{subsubsec:winkelnormalisierung}
Eine Besonderheit der Implementierung besteht in der winkeladaptiven Anpassung der Bounding-Box-Parameter für jede Detektion. Nach der initialen YOLO-Detektion, die eine OBB in Form von $(c_x, c_y, w, h, \theta)$ liefert, werden die Dimensionen $w$ und $h$ sowie das Padding basierend auf dem Rotationswinkel $\theta$ angepasst.

Zunächst wird der Winkel normalisiert, um eine kanonische Repräsentation zu erhalten. Die Normalisierung erfolgt durch:

\begin{equation}
\theta_{\text{norm}} = \begin{cases}
\theta - \frac{\pi}{2}, & \text{falls } \theta > \frac{\pi}{4} \\
\theta + \frac{\pi}{2}, & \text{falls } \theta < -\frac{\pi}{4} \\
\theta, & \text{sonst}
\end{cases}
\end{equation}

Diese Normalisierung stellt sicher, dass der Winkel im Bereich $[-\frac{\pi}{4}, \frac{\pi}{4}]$ liegt. Falls eine Rotation außerhalb dieses Bereichs erforderlich ist, werden Breite und Höhe vertauscht:

\begin{equation}
(w_{\text{norm}}, h_{\text{norm}}) = \begin{cases}
(h, w), & \text{falls Rotation durchgeführt} \\
(w, h), & \text{sonst}
\end{cases}
\end{equation}

Basierend auf dem normalisierten Winkel $\theta_{\text{norm}}$ wird entschieden, ob das Symbol als horizontal, vertikal oder angular orientiert klassifiziert wird:

\begin{equation}
\text{Orientierung} = \begin{cases}
\text{Horizontal}, & \text{falls } |\theta_{\text{norm}}| < 15^\circ \text{ oder } |\theta_{\text{norm}} - 180^\circ| < 15^\circ \\
\text{Vertikal}, & \text{falls } |\theta_{\text{norm}} - 90^\circ| < 15^\circ \text{ oder } |\theta_{\text{norm}} - 270^\circ| < 15^\circ \\
\text{Angular}, & \text{sonst}
\end{cases}
\end{equation}

Für jede Orientierung werden unterschiedliche Expansionsfaktoren $(e_x, e_y)$ und Padding-Werte $p$ angewendet:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Orientierung} & \textbf{$e_x$} & \textbf{$e_y$} & \textbf{$p$ [Pixel]} \\
\hline
Horizontal & 2.5 & 1.5 & 16 \\
Vertikal & 1.5 & 2.5 & 16 \\
Angular & 2.0 & 2.0 & 24 \\
\hline
\end{tabular}
\caption{Winkeladaptive Bounding-Box-Parameter}
\label{tab:angle_adaptive_params}
\end{table}

Die effektiven Dimensionen der Bounding Box werden dann berechnet als:

\begin{equation}
\begin{aligned}
w_{\text{eff}} &= w_{\text{norm}} \cdot e_x + 2p \\
h_{\text{eff}} &= h_{\text{norm}} \cdot e_y + 2p
\end{aligned}
\end{equation}

Diese Anpassung ist kritisch für die nachfolgende OCR, da bei horizontal orientierten Texten mehr horizontaler Kontext benötigt wird (z.B. für Texte wie \enquote{Signal A123}), während bei vertikal orientierten Texten mehr vertikaler Kontext erforderlich ist. Angular orientierte Objekte erhalten symmetrische Expansion und erhöhtes Padding, um Rotationseffekte auszugleichen.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        scale=1.0, 
        >=latex, 
        font=\small,
        symbol/.style={fill=blue!20, draw=blue!80!black, thick},
        roi/.style={draw=red, dashed, thick, fill=red!5},
        dim_arrow/.style={<->, thick, black}
    ]

    % --- 1. Horizontal (0°) ---
    \begin{scope}[local bounding box=scope1]
        \draw[symbol] (0,0) rectangle (1, 0.6);
        \node[blue!80!black] at (0.5, 0.3) {\textbf{S}};
        \node[black, anchor=west] at (1.2, 0.3) {Text};
        
        \draw[roi] (-0.2, -0.2) rectangle (2.5, 0.8);
        
        % Dimensions
        \draw[dim_arrow] (1.1, 1.0) -- (2.5, 1.0) node[midway, above, font=\scriptsize] {$e_x$ (hoch)};
        \draw[dim_arrow] (2.6, -0.2) -- (2.6, 0.8) node[midway, right, font=\scriptsize] {$e_y$ (tief)};
        
        \node[align=center, anchor=north] at (1.25, -0.5) {\textbf{Horizontal ($0^\circ$)}\\Expansion primär in $x$};
    \end{scope}

    % --- 2. Vertikal (90°) ---
    \begin{scope}[shift={(5,0)}, local bounding box=scope2]
        \draw[symbol] (0,0) rectangle (0.6, 1);
        \node[blue!80!black] at (0.3, 0.5) {\rotatebox{90}{\textbf{S}}};
        \node[black, anchor=south] at (0.3, 1.2) {\rotatebox{90}{Text}};
        
        \draw[roi] (-0.2, -0.2) rectangle (0.8, 2.5);
        
        % Dimensions
        \draw[dim_arrow] (1.0, 1.1) -- (1.0, 2.5) node[midway, right, font=\scriptsize] {$e_y$ (hoch)};
        \draw[dim_arrow] (-0.2, 2.7) -- (0.8, 2.7) node[midway, above, font=\scriptsize] {$e_x$ (tief)};

        \node[align=center, anchor=north] at (0.3, -0.5) {\textbf{Vertikal ($90^\circ$)}\\Expansion primär in $y$};
    \end{scope}

    % --- 3. Angular (45°) ---
    \begin{scope}[shift={(9,0.5)}, local bounding box=scope3]
        % Rotation Scope
        \begin{scope}[rotate=45]
            % Symbol & Text
            \draw[symbol] (0,0) rectangle (1, 0.6);
            \node[blue!80!black, rotate=45] at (0.5, 0.3) {\textbf{S}};
            \node[black, rotate=45, anchor=west] at (1.2, 0.3) {Text};
            
            % ROI
            \draw[roi] (-0.2, -0.2) rectangle (2.5, 0.8);

            % --- NEW: MISSING ARROWS ADDED HERE ---
            % Dimension e_x (parallel to top edge)
            \draw[dim_arrow] (1.1, 1.0) -- (2.5, 1.0) 
                node[midway, above, font=\scriptsize, rotate=45] {$e_x$ (tief)};
            
            % Dimension e_y (parallel to right edge)
            \draw[dim_arrow] (2.7, -0.2) -- (2.7, 0.8) 
                node[midway, right, font=\scriptsize, rotate=45] {$e_y$ (hoch)};
        \end{scope}
        
        \node[align=center, anchor=north] at (0, -1.0) {\textbf{Rotiert ($45^\circ$)}\\Expansion entlang\\lokaler Achsen};
    \end{scope}

    \end{tikzpicture}
    \caption{Adaptive Expansion der Region of Interest (ROI)}
    \label{fig:adaptive_padding}
\end{figure}
Abbildung \ref{fig:adaptive_padding} visualisiert diese Anpassung für die drei Orientierungskategorien. Im Fall angular orientierter Objekte (rechts) erfolgt die Expansion entlang der rotierten Objektachsen.
\subsubsection{Post-Processing und Duplikatentfernung}

Nach der Inferenz auf allen Tiles müssen die Detektionen aggregiert und Duplikate entfernt werden. Die Koordinaten-Rücktransformation erfolgt durch Addition der Tile-Offsets:

\begin{equation}
\begin{aligned}
c_x^{\text{global}} &= c_x^{\text{lokal}} + x_{\text{Tile}} \\
c_y^{\text{global}} &= c_y^{\text{lokal}} + y_{\text{Tile}}
\end{aligned}
\end{equation}

Aufgrund der Überlappung zwischen Tiles werden Objekte im Überlappungsbereich mehrfach detektiert. Zur Eliminierung dieser Duplikate wird eine Non-Maximum Suppression (NMS) durchgeführt. Die NMS arbeitet klassenweise und behält für jede Klasse nur die Detektionen mit höchster Konfidenz bei, wenn mehrere Detektionen dasselbe Objekt beschreiben.

Der Überlappungsgrad zweier Bounding Boxes wird mittels Intersection over Union (IoU) quantifiziert:

\begin{equation}
\text{IoU}(B_1, B_2) = \frac{\text{Area}(B_1 \cap B_2)}{\text{Area}(B_1 \cup B_2)}
\end{equation}

Der NMS-Algorithmus funktioniert wie folgt:

\begin{enumerate}
    \item Sortiere alle Detektionen einer Klasse nach absteigender Konfidenz
    \item Wähle die Detektion mit höchster Konfidenz und füge sie zur Ergebnismenge hinzu
    \item Entferne alle Detektionen, deren IoU mit der gewählten Detektion einen klassenspezifischen Schwellenwert $\tau_{\text{NMS}}$ überschreitet
    \item Wiederhole Schritte 2-3, bis keine Detektionen mehr vorhanden sind
\end{enumerate}

Die NMS-Schwellenwerte sind klassenspezifisch konfiguriert:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|p{7cm}|}
\hline
\textbf{Klasse} & \textbf{$\tau_{\text{NMS}}$} & \textbf{Begründung} \\
\hline
coordinate & 0.3 & Strenger Schwellenwert, da Koordinaten oft dicht gruppiert sind \\
signal & 0.5 & Standard-Schwellenwert für mittlere Objektdichte \\
weichen\_block & 0.6 & Lockerer Schwellenwert für große Symbole mit natürlicher Varianz \\
default & 0.5 & Fallback für nicht explizit konfigurierte Klassen \\
\hline
\end{tabular}
\caption{Klassenspezifische NMS-Schwellenwerte}
\label{tab:nms_thresholds}
\end{table}

Zusätzlich zur NMS werden Detektionen unterhalb klassenspezifischer Konfidenzschwellen verworfen. Diese Schwellen wurden empirisch auf dem Validierungssatz optimiert, um ein Gleichgewicht zwischen Precision und Recall zu erreichen.

\subsubsection{Ausgabeformat}

Das Ergebnis der Objekterkennungsstufe ist eine strukturierte Liste aller detektierten Symbole, wobei jede Detektion durch folgende Attribute charakterisiert ist:

\begin{itemize}
    \item \textbf{Klassenname} und \textbf{Klassenindex}: Semantische Kategorie des Symbols
    \item \textbf{Konfidenz}: Wahrscheinlichkeit der Korrektheit der Detektion im Intervall $[0, 1]$
    \item \textbf{AABB-Koordinaten} $(x_1, y_1, x_2, y_2)$: Achsenparallele Bounding Box für UI-Darstellung
    \item \textbf{OBB-Parameter} $(c_x, c_y, w_{\text{eff}}, h_{\text{eff}}, \theta_{\text{norm}})$: Rotierte Bounding Box für OCR
    \item \textbf{Polygon-Eckpunkte} $\{(x_i, y_i)\}_{i=1}^{4}$: Exakte Umrisspolygone
    \item \textbf{Seitennummer}: Zuordnung zur entsprechenden PDF-Seite
\end{itemize}

Diese strukturierte Repräsentation dient als Eingabe für die nachfolgende OCR-Stufe, welche die Textinformationen in unmittelbarer Nähe der detektierten Symbole extrahiert.

\subsubsection{Performance-Charakteristika}

Die durchschnittliche Inferenzzeit pro Tile beträgt ca. 120 ms auf einer Standard-CPU (Intel Core i7 der 10. Generation). Für einen typischen A3-Gleisplan mit ca. 12-16 Tiles ergibt sich eine Gesamtverarbeitungszeit von ca. 15-20 Sekunden für die Objekterkennungsstufe. Diese CPU-basierte Inferenz wurde bewusst gewählt, um die Einsetzbarkeit der Software auf Standard-Workstations ohne dedizierte GPU zu gewährleisten, was die Verbreitung und Nutzung im Produktivumfeld erleichtert.

\section{Orientierungsadaptive OCR-Pipeline}
\label{sec:ocrpipeline}
Die Texterkennung aus den detektierten Symbolen stellt eine zentrale Herausforderung dar, 
da die Textregionen beliebig im Gleisplan rotiert sein können. Dieses Kapitel beschreibt 
die implementierte OCR-Pipeline, die durch Multi-Engine-Kaskadierung, rotationsadaptive 
Strategien und klassenspezifische Bildvorverarbeitung eine robuste Texterkennung ermöglicht 
und damit die Anforderungen \textbf{FA-004} (OCR-Genauigkeit) und \textbf{FA-005} 
(OCR-Robustheit) adressiert.

\subsection{Multi-Engine Kaskadierung}

Das System implementiert eine kaskadierende Architektur mit drei OCR-Engines: PaddleOCR als primäre Engine, Tesseract als Fallback-Engine und optional EasyOCR für schwierige Fälle. Die Grundidee dieser Kaskadierung ist, dass verschiedene OCR-Engines unterschiedliche Stärken haben. PaddleOCR ist schnell und präzise bei gut ausgerichteten Texten, Tesseract ist besonders robust bei verrauschten oder pixeligen Bildern, und EasyOCR ist spezialisiert auf schwierige Winkel und perspektivische Verzerrungen.

\subsubsection{Funktionsweise der Kaskadierung}

Die Kaskadierung funktioniert nach einem sequenziellen Prinzip. Zunächst wird PaddleOCR auf dem Bildausschnitt ausgeführt, und zwar viermal mit unterschiedlichen Rotationswinkeln: $0^\circ, 90^\circ, 180^\circ und 270^\circ$. Der Grund für diese vier Rotationen ist, dass PaddleOCR am besten mit horizontal ausgerichtetem Text funktioniert. Durch das Ausprobieren aller vier Hauptrichtungen wird sichergestellt, dass mindestens eine Rotation den Text horizontal ausrichtet.

Für jede dieser Rotationen wird das OCR-Ergebnis durch eine klassenspezifische Validierungsfunktion geprüft. Diese Validierung überprüft, ob der erkannte Text dem erwarteten Muster der jeweiligen Symbolklasse entspricht. Beispielsweise muss ein Signal-Text mit einem Großbuchstaben beginnen, gefolgt von 1-3 Ziffern (wie \enquote{A12} oder \enquote{B234}). Zusätzlich zur Mustererkennung wird der Konfidenz-Wert (Vertrauenswert) des OCR-Ergebnisses überprüft.

Der Konfidenz-Wert von PaddleOCR liegt typischerweise zwischen 0 und etwa 2, wobei höhere Werte eine größere Sicherheit bedeuten. Ein empirisch bestimmter Schwellenwert von 1.3 hat sich als optimal erwiesen: Ergebnisse über diesem Wert sind in der Regel korrekt, während niedrigere Werte oft Fehler enthalten. Dieser Schwellenwert wurde durch Tests auf dem Trainingsdatensatz bestimmt und stellt einen guten Kompromiss zwischen Präzision (wie viele der als \enquote{korrekt} markierten Ergebnisse wirklich korrekt sind) und Recall (wie viele der tatsächlich vorhandenen Texte erkannt werden) dar.

Wenn PaddleOCR bei keiner der vier Rotationen ein validiertes Ergebnis mit ausreichender Konfidenz liefert, wird Tesseract als Fallback-Engine aktiviert. Tesseract verwendet einen anderen Algorithmus als PaddleOCR und ist besonders robust gegenüber verrauschten oder pixeligen Bildern. Tesseract wird mit dem Page Segmentation Mode 7 konfiguriert, der das Bild als einzelne Textzeile behandelt-eine Annahme, die für die kurzen Symbol-Beschriftungen im Gleisplan zutreffend ist.

Falls auch Tesseract kein valides Ergebnis liefert und die optionale dritte Engine EasyOCR aktiviert ist, wird diese als letzte Instanz aufgerufen. EasyOCR ist besonders gut bei stark geneigten oder perspektivisch verzerrten Texten, benötigt aber erheblich mehr Rechenzeit (etwa 5-10 Mal langsamer als PaddleOCR).

\subsubsection{Konfiguration der OCR-Engines}

Jede OCR-Engine wurde mit spezifischen Parametern konfiguriert, um optimale Ergebnisse für die Domäne der Gleispläne zu erzielen.

\textbf{PaddleOCR} wurde mit den folgenden Einstellungen verwendet:
\begin{itemize}
    \item \textbf{Detection Model}: \enquote{ch\_PP-OCRv4\_det} - ein vortrainiertes Modell zur Textlokalisierung, das ursprünglich für chinesische Dokumente entwickelt wurde, aber auch für deutsche/englische Texte ausgezeichnet funktioniert
    \item \textbf{Recognition Model}: \enquote{ch\_PP-OCRv4\_rec} - das zugehörige Erkennungsmodell für die eigentliche Zeichenerkennung
    \item \textbf{Angle Classification}: Aktiviert - ermöglicht automatische Erkennung und Korrektur der Textorientierung
    \item \textbf{Sprache}: \enquote{en} (Englisch) - da die meisten Beschriftungen alphanumerisch sind
\end{itemize}

\textbf{Tesseract} wurde mit den folgenden Parametern konfiguriert:
\begin{itemize}
    \item \textbf{Page Segmentation Mode (psm)}: 7 - behandelt das Bild als einzelne Textzeile, was für kurze Symbol-Beschriftungen optimal ist
    \item \textbf{OCR Engine Mode (oem)}: 3 - verwendet den LSTM-basierten neuronalen Netzwerk-Modus, der die beste Genauigkeit bietet
    \item \textbf{Character Whitelist}: Beschränkt auf \enquote{0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ+-.} - dies verhindert, dass Tesseract falsche Sonderzeichen oder Kleinbuchstaben erkennt, die in den meisten Symbolklassen nicht vorkommen
\end{itemize}

Die Whitelist ist besonders wichtig, da sie die Fehlerrate deutlich reduziert. Ohne diese Einschränkung würde Tesseract manchmal exotische Sonderzeichen oder Umlaute erkennen, die in technischen Gleisplänen nie vorkommen.

\textbf{EasyOCR} wurde mit minimaler Konfiguration verwendet:
\begin{itemize}
    \item \textbf{Sprachen}: [\enquote{en}] - Englisch als Basissprache
    \item \textbf{GPU-Nutzung}: Aktiviert - beschleunigt die Verarbeitung erheblich, wenn eine GPU verfügbar ist
\end{itemize}



\begin{figure}[h]
\centering
\begin{tikzpicture}[
    % Moderne Bibliothek für Positionierung
    scale=0.75,
    transform shape,
    node distance=1.0cm and 1.5cm, % Vertikaler und Horizontaler Abstand
    % Styles
    box/.style={
        rectangle, 
        draw=blue!60!black, 
        fill=blue!5, 
        text width=3cm, 
        align=center, 
        rounded corners, 
        minimum height=1cm,
        thick
    },
    decision/.style={
        diamond, 
        draw=orange!60!black, 
        fill=orange!10, 
        text width=2cm, 
        align=center, 
        aspect=2, 
        thick,
        inner sep=0pt
    },
    arrow/.style={
        ->, 
        >=latex, % Schönere Pfeilspitze
        thick, 
        rounded corners=5pt % Abgerundete Ecken für professionellen Look
    }
]

% --- NODES (mit positioning library) ---
\node[box] (start) {Bildausschnitt extrahieren};

\node[box, below=of start] (paddle) {PaddleOCR\\(4 Rotationen)};

\node[decision, below=of paddle] (check1) {Konfidenz\\$> 1.3$?};

% "right=of" statt "right of=" verhindert Überlappung
\node[box, right=of check1, xshift=0.5cm] (accept1) {Ergebnis\\akzeptieren};

\node[box, below=of check1] (tesseract) {Tesseract\\OCR};

\node[decision, below=of tesseract] (check2) {Regex\\validiert?};

\node[box, right=of check2, xshift=0.5cm] (accept2) {Ergebnis\\akzeptieren};

\node[box, below=of check2] (easyocr) {EasyOCR\\(optional)};

\node[box, below=of easyocr] (final) {Finales\\Ergebnis};


% --- ARROWS ---
% Standard Pfade (Links)
\draw[arrow] (start) -- (paddle);
\draw[arrow] (paddle) -- (check1);

% Check 1 Pfade
\draw[arrow] (check1) -- node[above, font=\footnotesize] {Ja} (accept1);
\draw[arrow] (check1) -- node[right, font=\footnotesize] {Nein} (tesseract);

% Tesseract -> Check 2
\draw[arrow] (tesseract) -- (check2);

% Check 2 Pfade
\draw[arrow] (check2) -- node[above, font=\footnotesize] {Ja} (accept2);
\draw[arrow] (check2) -- node[right, font=\footnotesize] {Nein} (easyocr);

% EasyOCR -> Final
\draw[arrow] (easyocr) -- (final);


% --- HIER IST DER TRICK FÜR DIE RECHTEN LINIEN ---

% Linie von Accept 1 (Ganz außen)
% ++(2.0, 0) heißt: Gehe vom Knoten 2cm nach rechts, bevor du abbiegst
\draw[arrow] (accept1.east) -- ++(1.5, 0) |- (final.east);

% Linie von Accept 2 (Etwas weiter innen)
% ++(0.7, 0) heißt: Gehe vom Knoten 0.7cm nach rechts (weniger als oben)
% |- ([yshift=2mm]final.east) versetzt den Pfeil am Ende etwas nach oben, damit sie sich nicht treffen
\draw[arrow] (accept2.east) -- ++(0.5, 0) |- ([yshift=2mm]final.east);

\end{tikzpicture}
\caption{Ablaufdiagramm der Multi-Engine OCR-Kaskadierung}
\label{fig:ocr_cascade}
\end{figure}
\FloatBarrier

\subsection{Dual-Winkel-Routing System}
\label{subsec:dualwinkelrouting}
Eine zentrale Herausforderung bei der OCR ist die beliebige Rotation der Symbole im Gleisplan. Während einige Symbole horizontal ausgerichtet sind, können andere um $ 45^\circ, 30^\circ$ oder sogar beliebige Winkel gedreht sein. Um diese Vielfalt zu bewältigen, wurde ein Dual-Path-Routing-System implementiert, das zwischen zwei verschiedenen Verarbeitungswegen unterscheidet.

\subsubsection{Cardinal Path und Angular Path}

Das System unterscheidet zwischen dem \textbf{Cardinal Path} (für nahezu horizontal/vertikal ausgerichtete Texte) und dem \textbf{Angular Path} (für beliebig gedrehte Texte). Die Entscheidung, welcher Pfad verwendet wird, basiert auf dem Rotationswinkel der Bounding Box.

Zunächst wird der von YOLO gelieferte Winkel normalisiert. Da YOLO Winkel im Bereich von $0^\circ$ bis $360^\circ$ liefern kann, werden Winkel über $180^\circ$ durch Subtraktion von $360^\circ$ in den Bereich $-180^\circ$ bis $180^\circ$ transformiert. Ein Winkel von $270^\circ$ wird beispielsweise zu $-90^\circ$. Dies vereinfacht die Logik erheblich, da nun positive Winkel eine Drehung gegen den Uhrzeigersinn und negative Winkel eine Drehung im Uhrzeigersinn darstellen.

Anschließend wird geprüft, ob der Absolutwert des normalisierten Winkels unter $15^\circ$ liegt. Diese $15^\circ$ wurden empirisch durch Experimente bestimmt: Bei Winkeln bis zu dieser Größe liefert eine einfache Rotation auf den nächsten kardinalen Winkel ($0^\circ, 90^\circ, 180^\circ$ oder $270^\circ$) bessere Ergebnisse als eine komplexe Perspektiventransformation. Der Grund ist, dass bei kleinen Winkeln die Perspektiventransformation zu leichten Verzerrungen führen kann, während die diskrete Rotation verlustfrei ist.

\textbf{Cardinal Path (Winkel $\leq$ $15^\circ$):} Wenn der Winkel nahe an einem kardinalen Wert liegt, wird der Bildausschnitt einfach auf den nächstgelegenen kardinalen Winkel rotiert. Die Rotation erfolgt mittels einer Standard-Rotationsmatrix, die das Bild um den Mittelpunkt dreht. Diese Methode ist schnell, einfach und vermeidet Interpolationsartefakte, da nur $90^\circ$ Schritte verwendet werden.

Konkret wird aus den vier Kandidaten ($0^\circ, 90^\circ, 180^\circ, 270^\circ$) derjenige gewählt, der die kleinste Winkeldifferenz zum normalisierten Winkel hat. Ein Winkel von $12^\circ$ wird also auf $0^\circ$ rotiert, ein Winkel von $85^\circ$ auf $90^\circ$, und so weiter.


\textbf{Angular Path (Winkel > $15^\circ$):} Bei größeren Abweichungen von den kardinalen Winkeln wird eine Perspektiventransformation verwendet. Diese Methode nutzt die vier Eckpunkte der Oriented Bounding Box (OBB), um eine Transformationsmatrix zu berechnen. Die Transformation wandelt die gedrehte OBB in ein achsenparalleles Rechteck um, wodurch der Text horizontal ausgerichtet wird.

Die Perspektiventransformation wird mit OpenCV's \texttt{getPerspectiveTransform()}-Funktion implementiert. Diese Funktion benötigt zwei Sätze von je vier Punkten: die Quellpunkte (die vier Ecken der gedrehten OBB) und die Zielpunkte (die vier Ecken eines aufrechten Rechtecks). Aus diesen acht Punkten berechnet die Funktion eine 3x3-Transformationsmatrix, die dann mit \texttt{warpPerspective()} auf das Bild angewendet wird.

Obwohl diese Methode rechenintensiver ist (etwa 2-3 Mal langsamer als eine einfache Rotation), liefert sie bei beliebigen Winkeln deutlich bessere Ergebnisse, da der Text exakt horizontal ausgerichtet wird, nicht nur annähernd.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    >=latex, 
    font=\small,
    % Styles für Konsistenz
    box/.style={thick, fill=gray!20},
    result/.style={thick, fill=green!20},
    arrow/.style={->, very thick}
]

% --- OBERER TEIL: CARDINAL PATH ---
\begin{scope}[yshift=0cm] % Lokaler Bereich oben
    \node at (0, 1.5) {\textbf{Cardinal Path ($|\theta| \leq 15^\circ$)}};

    % Original (lokal rotiert um 0,0)
    \draw[box, rotate=10] (-1, -0.5) rectangle (1, 0.5);
    \node[below=0.7cm] at (0,0) {Original};
    \node[below=1.1cm] at (0,0) {$\theta = 10^\circ$};

    % Pfeil
    \draw[arrow] (2, 0) -- (3.5, 0) node[midway, above] {Rotation};

    % Ergebnis
    \draw[result] (4.5, -0.5) rectangle (6.5, 0.5);
    \node at (5.5, 0) {Text};
    \node[below=0.7cm] at (5.5,0) {Rotiert};
    \node[below=1.1cm] at (5.5,0) {$\theta = 0^\circ$};
\end{scope}


% --- UNTERER TEIL: ANGULAR PATH ---
% Wir verschieben den ganzen Block nach unten (y = -4cm)
\begin{scope}[yshift=-4cm] 
    \node at (0, 1.5) {\textbf{Angular Path ($|\theta| > 15^\circ$)}};

    % Original (Jetzt zentriert gezeichnet, Rotation wirkt lokal!)
    \draw[box, rotate=35] (-1, -0.5) rectangle (1, 0.5);
    
    % Labels sitzen nun perfekt unter dem Kasten
    \node[below=0.9cm] at (0,0) {Original};
    \node[below=1.3cm] at (0,0) {$\theta = 35^\circ$};

    % Pfeil
    \draw[arrow] (2, 0) -- (3.5, 0) node[midway, above] {Perspektive};

    % Ergebnis
    \draw[result] (4.5, -0.5) rectangle (6.5, 0.5);
    \node at (5.5, 0) {Text};
    \node[below=0.9cm] at (5.5,0) {Transformiert};
    \node[below=1.3cm] at (5.5,0) {Ausgerichtet};
\end{scope}

\end{tikzpicture}
\caption{Vergleich: Cardinal Path vs. Angular Path Rotationsstrategien}
\label{fig:dual_path}
\end{figure}
Abbildung~\ref{fig:angular_ocr_example} demonstriert die erfolgreiche Verarbeitung eines um $37,5^\circ$ rotierten Koordinatentexts durch den Angular Path.



Das System erkannte den Koordinatentext \texttt{0.0629} korrekt trotz der nicht-kardinalen Orientierung von $\theta = -37{,}5^\circ$. Die Perspektiventransformation richtete die gedrehte Oriented Bounding Box horizontal aus, wodurch PaddleOCR mit einer Konfidenz von 0.95 den Text extrahieren konnte. Die erfolgreiche Verknüpfung mit dem benachbarten Symbol (Gl.112) validiert sowohl die OCR-Robustheit (FA-005) als auch die Rotationsinvarianz (FA-002) für beliebige Winkel außerhalb der kardinalen Orientierungen.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/Kapitel6/angulartextv1.png}
\caption{Verarbeitung eines um $37,5^\circ$ rotierten Elemente durch Angular-Path-OCR}
\label{fig:angular_ocr_example}
\end{figure}
\subsection{Klassenspezifische Strategien}

Verschiedene Symbolklassen im Gleisplan haben unterschiedliche visuelle Eigenschaften, die spezielle Vorverarbeitungsschritte erfordern. Signale haben beispielsweise oft einen farbigen Hintergrund und größere Beschriftungen, während GKS häufig von dünnen Linien umgeben sind und sehr kompakte Texte haben. Um diese Unterschiede zu berücksichtigen, wurde für jede Klasse eine spezialisierte Verarbeitungsstrategie implementiert.

\subsubsection{Adaptive Padding}

Das Padding erweitert den von YOLO gelieferten Bildausschnitt, um sicherzustellen, dass der gesamte Text erfasst wird. Oft liegt der Text teilweise außerhalb der detektierten Bounding Box, besonders bei Signalen, wo die Beschriftung über oder neben dem eigentlichen Symbol sein kann.

Die Größe des Paddings wird primär relativ zur Größe der Bounding Box berechnet. Um jedoch auch bei sehr schmalen Objekten (z.B. Signalmasten) eine Mindesterfassung zu gewährleisten, wird zusätzlich ein absolutes Minimum pmin=10 Pixel definiert. Die Berechnung erfolgt durch:

\begin{equation} 
    p_x = \max(\alpha \cdot w_{\text{bbox}}, p_{\text{min}}), \quad p_y = \max(\beta \cdot h_{\text{bbox}}, p_{\text{min}}) 
\end{equation}

wobei $p_x$ das horizontale Padding (in Pixeln), $p_y$ das vertikale Padding (in Pixeln), $w_{\text{bbox}}$ die Breite der Bounding Box, $h_{\text{bbox}}$ die Höhe der Bounding Box, und $\alpha$ sowie $\beta$ klassenspezifische Faktoren sind.

Die folgende Tabelle zeigt die optimierten Faktoren für jede Symbolklasse:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|p{5cm}|}
\hline
\textbf{Klasse} & $\boldsymbol{\alpha}$ & $\boldsymbol{\beta}$ & \textbf{Begründung} \\
\hline
Signal & 0.2 & 0.3 & Beschriftung oft oberhalb des Symbols \\
GKS (beide Typen) & 0.15 & 0.15 & Kompakte, zentrierte Beschriftung \\
Weiche & 0.1 & 0.1 & Text direkt am Symbol \\
Koordinate & 0.25 & 0.25 & Längere Zahlenfolgen, mehr Platz nötig \\
\hline
\end{tabular}
\caption{Klassenspezifische Padding-Faktoren und deren Begründung}
\end{table}

Für Signale wurde beispielsweise ein Faktor von $\alpha = 0.2$ (20\% der Breite) und $\beta = 0.3$ (30\% der Höhe) als optimal ermittelt. Das größere vertikale Padding berücksichtigt, dass Signal-Beschriftungen typischerweise oberhalb des Symbols positioniert sind. Bei einer Bounding Box von $100 \times 50$ Pixeln würde dies ein Padding von 20 Pixeln horizontal und 15 Pixeln vertikal bedeuten.

Das finale erweiterte Bild hat dann die Dimensionen $(h + 2p_y) \times (w + 2p_x)$, wobei das \enquote{2} berücksichtigt, dass das Padding sowohl oben und unten als auch links und rechts hinzugefügt wird.

\subsubsection{Morphologische Filterung für GKS}

GKS-Symbole sind häufig von dünnen Linien umgeben, die die OCR-Qualität beeinträchtigen können. Diese Linien können fälschlicherweise als Buchstaben wie \enquote{I}, \enquote{l} oder \enquote{1} interpretiert werden. Um diese Störungen zu reduzieren, wird ein morphologischer Opening-Filter angewendet.

Opening ist eine Kombination aus zwei aufeinanderfolgenden Operationen: Zunächst wird eine \textbf{Erosion} durchgeführt, die kleine helle Strukturen (wie dünne Linien) entfernt. Anschließend wird eine \textbf{Dilation} durchgeführt, die die verbleibenden Strukturen (die Textzeichen) wieder auf ihre ursprüngliche Größe erweitert. Der Effekt ist, dass dünne Linien komplett verschwinden, während dickere Strukturen (Buchstaben und Zahlen) erhalten bleiben.

Der Filter verwendet ein quadratisches strukturierendes Element mit der Größe 3x3 Pixel - also eine Matrix aus 9 Einsen. Diese Größe wurde gewählt, weil sie ausreicht, um typische Störlinien (1-2 Pixel dick) zu entfernen, aber nicht so groß ist, dass sie die Textzeichen selbst beschädigt. Die Anwendung erfolgt auf dem binärisierten Bild, wobei die Pixel entweder komplett weiß (Text) oder komplett schwarz (Hintergrund) sind.
\begin{figure}[h]
\centering
\begin{tikzpicture}

% Original with noise
\node at (1.8, 3.5) {\textbf{Original (mit Rauschen)}};
\draw[thick] (0, 0) rectangle (3.5, 3);
% GKS (trapezoid)
\draw[thick, fill=gray!30] (1, 1.5) -- (1.5, 2.3) -- (2, 2.3) -- (2.5, 1.5) -- cycle;
\node at (1.75, 1.9) {\small 1234};
% Noise pixels
\fill[black] (0.5, 0.5) circle (0.1cm);
\fill[black] (3, 2.5) circle (0.1cm);
\fill[black] (0.8, 2.7) circle (0.1cm);
\fill[black] (2.8, 0.7) circle (0.1cm);

% Arrow
\draw[->, very thick] (4, 1.5) -- (5, 1.5) node[midway, above] {\small Erosion};

% After erosion
\node at (7.3, 3.5) {\textbf{Nach Erosion}};
\draw[thick] (5.5, 0) rectangle (9, 3);
% Smaller GKS symbol
\draw[thick, fill=gray!30] (6.6, 1.6) -- (7, 2.2) -- (7.4, 2.2) -- (7.8, 1.6) -- cycle;
\node[font=\tiny] at (7.2, 1.9) {1234};

% Arrow
\draw[->, very thick] (9.5, 1.5) -- (10.5, 1.5) node[midway, above] {\small Dilation};

% After dilation (cleaned)
\node at (12.8, 3.5) {\textbf{Nach Dilation (gereinigt)}};
\draw[thick] (11, 0) rectangle (14.5, 3);
% Restored GKS symbol
\draw[thick, fill=gray!30] (12, 1.5) -- (12.5, 2.3) -- (13, 2.3) -- (13.5, 1.5) -- cycle;
\node at (12.75, 1.9) {\small 1234};

\end{tikzpicture}
\caption{Morphologisches Opening zur Rauschentfernung bei GKS-Symbolen}
\label{fig:morphology}
\end{figure}

\subsubsection{Multi-Scale Processing für Koordinaten}

Koordinaten-Beschriftungen sind oft klein und können bei niedriger Bildauflösung schwer lesbar sein. Ein typisches Problem: Bei einem eingescannten Gleisplan mit 150 DPI kann eine Koordinaten-Beschriftung nur 15-20 Pixel hoch sein, was für OCR-Engines an der Grenze der Lesbarkeit liegt.

Um dies zu kompensieren, wird ein Multi-Scale-Ansatz verwendet: Das Bild wird mit drei verschiedenen Skalierungsfaktoren vergrößert - 1.5x (50\% größer), 2.0x (doppelte Größe) und 2.5x (2.5-fache Größe). Die OCR wird auf allen drei Skalen durchgeführt, und am Ende wird das Ergebnis mit der höchsten Konfidenz ausgewählt.

Die Skalierung erfolgt mit Lanczos-Interpolation, einem hochwertigen Algorithmus, der scharfe Kanten erhält und keine Treppeneffekte oder Blockbildung verursacht. Lanczos ist zwar rechenintensiver als einfachere Methoden (wie bilineare Interpolation), liefert aber deutlich schärfere Ergebnisse, was bei der Vergrößerung kleiner Texte entscheidend ist.

Dieser Ansatz erhöht zwar die Rechenzeit erheblich (die OCR wird dreimal durchgeführt), verbessert aber die Erkennungsrate bei kleinen Texten von etwa 60\% auf über 85\%. Der Trade-off zwischen Geschwindigkeit und Genauigkeit wurde hier bewusst zugunsten der Genauigkeit entschieden, da Koordinaten kritische Informationen enthalten.

\subsection{Bildvorverarbeitungs-Algorithmen}

Vor der eigentlichen OCR-Verarbeitung werden die Bildausschnitte durch eine Reihe von Vorverarbeitungsschritten optimiert, um die Textqualität zu verbessern und störende Elemente zu entfernen.

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.75]

% Step 1: Original with lines
\node at (1, 3.2) {\small \textbf{1. Original}};
\draw[thick] (0, 0) rectangle (2.5, 2.5);
% GKS symbol (trapezoid)
\draw[thick, fill=gray!20] (0.5, 1.5) -- (1, 2.2) -- (1.5, 2.2) -- (2, 1.5) -- cycle;
\node at (1.25, 1.85) {\tiny 1234};
% Track lines (störende Gleislinien)
\draw[gray, thick] (0, 1) -- (2.5, 1);
\draw[gray, thick] (1.25, 0) -- (1.25, 2.5);
\node[below, font=\tiny] at (1.25, 1) {12345,6};

% Arrow
\draw[->, very thick] (3, 1.25) -- (3.8, 1.25);

% Step 2: Lines removed
\node at (5.5, 3.2) {\small \textbf{2. Linien entfernt}};
\draw[thick] (4.3, 0) rectangle (6.8, 2.5);
% GKS symbol (trapezoid) - ohne störende Linien
\draw[thick, fill=gray!20] (4.8, 1.5) -- (5.3, 2.2) -- (5.8, 2.2) -- (6.3, 1.5) -- cycle;
\node at (5.55, 1.85) {\tiny 1234};
\node[below, font=\tiny] at (5.55, 1) {12345,6};

% Arrow
\draw[->, very thick] (7.3, 1.25) -- (8.1, 1.25);

% Step 3: CLAHE applied
\node at (10, 3.2) {\small \textbf{3. CLAHE}};
\draw[thick, fill=yellow!10] (8.8, 0) rectangle (11.3, 2.5);
% GKS symbol (trapezoid) - besserer Kontrast
\draw[thick, fill=gray!40] (9.3, 1.5) -- (9.8, 2.2) -- (10.3, 2.2) -- (10.8, 1.5) -- cycle;
\node[font=\bfseries] at (10.05, 1.85) {\tiny 1234};
\node[below, font=\tiny\bfseries] at (10.05, 1) {12345,6};

% Arrow
\draw[->, very thick] (11.8, 1.25) -- (12.6, 1.25);

% Step 4: Scaled
\node at (14.5, 3.2) {\small \textbf{4. Skaliert}};
\draw[thick, fill=yellow!10] (13, -0.3) rectangle (16, 2.8);
% GKS symbol (trapezoid) - vergrößert
\draw[thick, fill=gray!40] (13.25, 1) -- (13.95, 2.2) -- (15.05, 2.2) -- (15.75, 1) -- cycle;
\node[font=\bfseries] at (14.50, 1.6) {\small 1234};
\node[below, font=\small\bfseries] at (14.5, 0.5) {12345,6};

\end{tikzpicture}
\caption{Schritte der Bildvorverarbeitung (schematische Darstellung)}
\label{fig:preprocessing}
\end{figure}

\subsubsection{Orientierte Linienentfernung}

Gleispläne enthalten zahlreiche Linien (Gleise, Verbindungen, Umrandungen), die oft direkt über oder neben den Textbeschriftungen verlaufen. Diese Linien können die OCR-Engines verwirren, da sie fälschlicherweise als Buchstaben interpretiert werden können - typischerweise als \enquote{I}, \enquote{l} oder \enquote{1}.

Die Linienentfernung erfolgt in mehreren Schritten und berücksichtigt die Orientierung der Bounding Box:

\begin{enumerate}
    \item \textbf{Konvertierung in Graustufen}: Das Farbbild wird in ein Graustufenbild umgewandelt, da für die Linienentfernung keine Farbinformationen benötigt werden.
    
    \item \textbf{Binarisierung}: Das Graustufenbild wird mit einem Schwellenwert von 127 in ein Schwarz-Weiß-Bild umgewandelt. Pixel über 127 werden weiß (Hintergrund), Pixel darunter schwarz (Linien und Text).
    
    \item \textbf{Horizontale Linienentfernung}: Es wird ein strukturierendes Element verwendet, das eine horizontale Linie mit 25 Pixeln Länge darstellt. Dieses Element wird um den Winkel der Bounding Box rotiert, um die Gleislinien zu erfassen, die parallel zur Textrichtung verlaufen. Eine morphologische Opening-Operation mit diesem Element entfernt alle entsprechenden Linien.
    
    \item \textbf{Vertikale Linienentfernung}: Analog wird ein vertikales strukturierendes Element (ebenfalls 25 Pixel lang) verwendet, das um $90^\circ$ zusätzlich gedreht wird, um senkrechte Linien zu entfernen.
    
    \item \textbf{Inpainting}: Die detektierten Linien werden zu einer Maske kombiniert. Diese Maske wird verwendet, um die Linien durch Inpainting aus dem Originalbild zu entfernen. Inpainting ist ein Algorithmus, der fehlende oder beschädigte Bildbereiche durch plausible Pixel aus der Umgebung ersetzt. Konkret wird der TELEA-Algorithmus mit einem Radius von 3 Pixeln verwendet.
\end{enumerate}

Die Länge von 25 Pixeln für die strukturierenden Elemente wurde so gewählt, dass typische Gleislinien erfasst werden (die oft 20-30 Pixel lang durchgehend sind), aber Textzeichen nicht betroffen sind (die selten länger als 10-15 Pixel in einer Dimension sind).

\subsubsection{CLAHE (Contrast Limited Adaptive Histogram Equalization)}

CLAHE ist ein adaptiver Kontrast-Verbesserungsalgorithmus, der besonders bei Bildern mit ungleichmäßiger Beleuchtung effektiv ist. Viele Gleispläne sind alte Scans oder Fotografien mit Schatten, Verfärbungen oder ungleichmäßiger Belichtung. In solchen Fällen kann einfache Histogramm-Entzerrung zu übermäßigem Rauschen führen.

CLAHE arbeitet anders als globale Histogrammentzerrung: Das Bild wird in ein Raster von 8×8 Kacheln (Tiles) aufgeteilt. Für jede Kachel wird separat eine Histogrammentzerrung durchgeführt, die dunkle Bereiche aufhellt und helle Bereiche abdunkelt. Der entscheidende Unterschied: Für jede Kachel wird der Kontrast \textit{lokal} optimiert, nicht global.

Der Parameter \enquote{Clip Limit} (hier: 2.0) begrenzt die maximale Verstärkung des Kontrasts. Ohne diese Begrenzung würde CLAHE in homogenen Bereichen (wie einem gleichmäßigen Hintergrund) minimales Rauschen extrem verstärken. Der Wert 2.0 bedeutet, dass die Histogramm-Verstärkung auf das Doppelte des Durchschnitts begrenzt wird.

Die Übergänge zwischen den Kacheln werden durch bilineare Interpolation geglättet, sodass keine sichtbaren Kanten zwischen benachbarten Kacheln entstehen. Das Ergebnis ist ein Bild mit gleichmäßigem Kontrast über die gesamte Fläche, was die OCR-Qualität insbesondere bei schlechten Scans erheblich verbessert.

\subsubsection{Adaptive Skalierung bei niedriger Auflösung}

Wenn die Höhe eines Bildausschnitts unter 40 Pixel liegt, wird dieser als zu klein für zuverlässige OCR betrachtet. In solchen Fällen wird das Bild mit einem Faktor von 2.0 hochskaliert, wodurch sich die Höhe verdoppelt.

Die Skalierung erfolgt ebenfalls mit Lanczos-Interpolation, die eine hohe Bildqualität gewährleistet und Treppeneffekte vermeidet. Nach der Skalierung sind die Textzeichen größer und besser erkennbar für die OCR-Engines.

Die Schwelle von 40 Pixeln wurde empirisch bestimmt: Tests zeigten, dass die OCR-Genauigkeit unterhalb dieser Größe dramatisch sinkt (von etwa 85\% auf unter 50\%), während oberhalb dieser Größe keine signifikante Verbesserung durch Hochskalierung erzielt wird. Der Skalierungsfaktor 2.0 wurde gewählt, weil er die meisten zu kleinen Texte in einen gut lesbaren Bereich bringt (80+ Pixel Höhe), ohne die Datenmenge unnötig zu vergrößern.

\subsection{Validierung und Qualitätssicherung}

Nach der OCR-Verarbeitung durchläuft jedes Ergebnis mehrere Validierungsschritte, um die Qualität der Extraktion sicherzustellen und offensichtliche Fehler zu erkennen.

\subsubsection{Regelbasierte Textvalidierung}
Die in Tabelle~\ref{tab:regex_patterns} (Kapitel~\ref{chap:theoretischeundtechnischegrundlagen}) eingeführten domänenspezifischen Validierungsmuster werden wie folgt implementiert:
\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{3.7cm}|p{4.5cm}|c|}
\hline
\textbf{Klasse} & \textbf{Pattern} & \textbf{Beispiele} \\
\hline
Signal & \verb|^[A-Z]\d{1,3}[a-z]?$| & A1, B234, C45a  \\
GKS (beide Typen) & \verb|^\d{3,4}$| & 1234, 567  \\
Koordinate & \verb|^\d+[.,]\d+$| & 18.1606, 123,456  \\
GM-Block & \verb|^\d+[.,]\d+$| & 18.1606  \\
Haltepunkt & \verb|^[A-Z]{2,4}$| & AB, WXYZ  \\
Weiche & \verb|^W\d{2,4}[a-z]?$| & W12, W234  \\
\hline
\end{tabular}
\caption{Validierungs-Patterns pro Symbolklasse mit Klassifizierung}
\end{table}

Für Signale bedeutet das Pattern \verb|^[A-Z]\d{1,3}[a-z]?$| konkret:
\begin{itemize}
    \item \verb|^| = Start der Zeichenkette
    \item \verb|[A-Z]| = Ein Großbuchstabe (A bis Z)
    \item \verb|\d{1,3}| = 1 bis 3 Ziffern
    \item \verb|[a-z]?| = Optional ein Kleinbuchstabe
    \item \verb|$| = Ende der Zeichenkette
\end{itemize}

Gültige Beispiele sind also \enquote{A1}, \enquote{B234} oder \enquote{C45a}, während \enquote{1A} (Ziffer zuerst), \enquote{ABC} (keine Ziffer) oder \enquote{A1234} (zu viele Ziffern) ungültig sind.

Die Validierungsfunktion prüft das OCR-Ergebnis gegen das klassenspezifische Muster und gibt einen Boolean-Wert zurück (True = gültig, False = ungültig). Dieser einfache Mechanismus filtert die meisten OCR-Fehler heraus, da fehlerhafte Erkennungen selten exakt dem erwarteten Muster entsprechen.

\subsubsection{Kombiniertes Konfidenz-Scoring}

Zusätzlich zur binären Ja/Nein-Validierung wird ein numerischer Gesamtscore berechnet, der die Qualität eines OCR-Ergebnisses bewertet. Dieser Score kombiniert mehrere Faktoren:

\begin{equation}
S_{\text{total}} = 0.5 \cdot \gamma_{\text{OCR}} + 0.3 \cdot S_{\text{regex}} + 0.2 \cdot S_{\text{length}}
\end{equation}

Die drei Komponenten sind:

\begin{itemize}
    \item $\gamma_{\text{OCR}}$: Die von der OCR-Engine gelieferte Konfidenz, normalisiert auf den Bereich 0 bis 1. PaddleOCR liefert Werte zwischen 0 und \textasciitilde2, die durch Division durch 2 normalisiert werden.
    
    \item $S_{\text{regex}}$: Regex-Übereinstimmung - Wert 1, falls das Pattern passt, sonst 0.
    
    \item $S_{\text{length}}$: Wie gut passt die Textlänge zur erwarteten Länge dieser Klasse? Berechnet mit einer Gauß-Kurve: Je näher die Länge am Erwartungswert, desto höher der Score.
\end{itemize}

Die Gewichte (0.5, 0.3, 0.2) wurden empirisch optimiert. Die OCR-Konfidenz wird am stärksten gewichtet (50\%), da sie direkt von der Engine stammt und deren \enquote{Sicherheit} widerspiegelt. Die Regex-Validierung erhält 30\%, da sie strukturelle Korrektheit prüft. Der Längen-Score erhält nur 20\%, da Längenabweichungen manchmal legitim sind (z.B. \enquote{A1} vs. \enquote{A123} - beide gültige Signale).
\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=1.5cm,
    ylabel={Gewichtung},
    symbolic x coords={OCR-Conf., Regex-Score, Längen-Score},
    xtick=data,
    ymin=0, ymax=0.6,
    nodes near coords,
    nodes near coords align={vertical},
    width=10cm,
    height=6cm
]
\addplot coordinates {(OCR-Conf., 0.5) (Regex-Score, 0.3) (Längen-Score, 0.2)};
\end{axis}
\end{tikzpicture}
\caption{Gewichtung der Komponenten im Konfidenz-Scoring}
\label{fig:Konfidenz_weights}
\end{figure}

Beispiel: Ein Signal-Text \enquote{A12} mit OCR-Konfidenz 1.5, gültiger Regex und erwarteter Länge würde den Score erhalten:
$S_{\text{total}} = 0.5 \cdot 0.75 + 0.3 \cdot 1.0 + 0.2 \cdot 1.0 = 0.875$ (sehr gutes Ergebnis).

\subsubsection{Fehlerbehandlung und Fuzzy-Matching}

Wenn die OCR ein leeres Ergebnis liefert oder der Text keinerlei Zeichen enthält, wird ein Platzhalter-Text generiert: \enquote{UNREADABLE\_} gefolgt von der Symbolklasse (z.B. \enquote{UNREADABLE\_signal}). Dies ermöglicht es, solche Fälle später in der Benutzeroberfläche zu identifizieren und manuell zu korrigieren.

Bei ungültigen Ergebnissen (die das Validierungsmuster nicht erfüllen) wird ein Fuzzy-Matching-Mechanismus aktiviert. Dieser vergleicht den erkannten Text mit einer Liste bekannter korrekter Werte aus früheren Extraktionen. Der Vergleich erfolgt mittels \textbf{Levenshtein-Distanz}, die misst, wie viele Einfüge-, Lösch- oder Ersetzungsoperationen notwendig sind, um einen String in einen anderen zu transformieren.\cite{levenshtein1966}

Beispiele für Levenshtein-Distanz:
\begin{itemize}
    \item \enquote{A12} → \enquote{A1Z}: Distanz = 1 (ein Zeichen ersetzt)
    \item \enquote{A12} → \enquote{A123}: Distanz = 1 (ein Zeichen eingefügt)
    \item \enquote{A12} → \enquote{B34}: Distanz = 3 (alle drei Zeichen ersetzt)
\end{itemize}

Wenn die minimale Levenshtein-Distanz zu einem bekannten Wert kleiner als 2 ist (maximal 2 Änderungen erforderlich), wird dieser bekannte Wert als automatische Korrektur übernommen. Dieser Mechanismus ist besonders hilfreich bei systematischen OCR-Fehlern wie der Verwechslung von:
\begin{itemize}
    \item \enquote{O} (Buchstabe) und \enquote{0} (Ziffer)
    \item \enquote{l} (kleines L) und \enquote{1} (Eins)
    \item \enquote{S} und \enquote{5}
    \item \enquote{B} und \enquote{8}
\end{itemize}

Der Schwellenwert von 2 wurde so gewählt, dass er echte Tippfehler korrigiert, aber keine falschen Korrekturen einführt. Tests zeigten, dass eine Distanz von 3 oder mehr zu oft zu falschen Korrekturen führt.

Falls kein ähnlicher bekannter Wert gefunden wird, wird das ungültige Ergebnis mit dem Präfix \enquote{INVALID\_} markiert (z.B. \enquote{INVALID\_A1Z3X}), um dem Benutzer zu signalisieren, dass hier eine manuelle Überprüfung erforderlich ist.

\subsection{Zusammenfassung der Pipeline-Integration}

Die gesamte OCR-Pipeline wird als modulare Funktion implementiert, die eine Liste von YOLO-Detektionen sowie das Originalbild als Eingabe erhält. Für jede Detektion werden folgende sieben Schritte nacheinander ausgeführt:

\begin{enumerate}
    \item \textbf{Bildausschnitt extrahieren}: Basierend auf den Koordinaten $(x, y)$, der Größe $(w, h)$ und dem Winkel $\theta$ der Bounding Box wird der relevante Bereich aus dem Originalbild ausgeschnitten.
    
    \item \textbf{Klassenspezifisches Preprocessing}: Je nach Symbolklasse werden die entsprechenden Vorverarbeitungsschritte angewendet - adaptive Padding-Berechnung, morphologische Filterung für GKS, oder Multi-Scale-Processing für Koordinaten.
    
    \item \textbf{Dual-Angle Routing}: Basierend auf dem Rotationswinkel wird entweder der Cardinal Path (bei Winkeln $\leq$ $15^\circ$) oder der Angular Path (bei Winkeln >$15^\circ$) gewählt und die entsprechende Transformation (diskrete Rotation oder Perspektiventransformation) durchgeführt.
    
    \item \textbf{Multi-Engine OCR}: Der vorverarbeitete Bildausschnitt wird durch die kaskadierende OCR-Pipeline geschickt. Zuerst PaddleOCR mit vier Rotationen, dann bei Bedarf Tesseract, und optional EasyOCR.
    
    \item \textbf{Post-Processing}: Das OCR-Ergebnis wird bereinigt - führende/nachfolgende Leerzeichen werden entfernt, und die Groß-/Kleinschreibung wird bei Bedarf normalisiert.
    
    \item \textbf{Validierung und Scoring}: Das Ergebnis wird gegen das klassenspezifische Regex-Muster validiert, und der kombinierte Konfidenz-Score wird berechnet.
    
    \item \textbf{Fehlerbehandlung}: Bei ungültigen Ergebnissen wird Fuzzy-Matching mit bekannten Werten versucht. Falls dies fehlschlägt, wird ein entsprechender Platzhalter (INVALID\_ oder UNREADABLE\_) generiert.
\end{enumerate}

Das finale Ergebnis der gesamten OCR-Pipeline ist eine Liste von Tupeln. Jedes Tupel enthält die ursprüngliche YOLO-Detektion, den erkannten Text und die zugehörige Konfidenz. Diese Liste wird an die nachfolgende Komponente weitergereicht - die Symbol-Text-Verknüpfung, die in Abschnitt \ref{sec:intelligentesymboltextverknüpfung} beschrieben wird.


\section{Intelligente Symbol-Text Verknüpfung}
\label{sec:intelligentesymboltextverknüpfung}
Nach der erfolgreichen Detektion von Symbolen durch YOLOv8-OBB und der Extraktion von 
Textinhalten mittels der orientierungsadaptiven OCR-Pipeline besteht die zentrale 
Herausforderung in der korrekten Zuordnung der extrahierten Texte zu ihren zugehörigen 
Symbolen. Diese Verknüpfung ist essenziell für die Generierung semantisch korrekter 
Datensätze aus den Gleisplänen und adressiert die Anforderungen \textbf{FA-006} 
(Fahrtrichtungsdetektion) und \textbf{FA-007} (Symbol-Koordinaten-Verknüpfung).

Die naive Anwendung rein distanzbasierter Proximity-Algorithmen -- bei denen einfach der nächstgelegene Text einem Symbol zugeordnet wird -- führt bei rotierten Symbolen zu systematischen Fehlzuordnungen. Ein Signal, das beispielsweise um $45^\circ$ gedreht ist, besitzt eine eigene lokale Orientierung, und der zugehörige Text befindet sich typischerweise \enquote{unterhalb} des Signals in dessen lokaler Ausrichtung, nicht notwendigerweise in globalen Bildkoordinaten. Die Implementierung muss daher rotationsinvariante Algorithmen einsetzen, die geometrische Transformationen nutzen, um die semantische räumliche Beziehung zwischen Symbolen und Texten korrekt zu erfassen.

\subsection{Rotationsinvariante Koordinatentransformation}
\label{sec:Koordinatentransformation}
\subsubsection{Problemstellung und mathematisches Modell}

Die Herausforderung liegt darin, dass die von YOLO detektierten Symbole beliebige Rotationswinkel $\theta \in [0^\circ, 360^\circ)$ aufweisen können. Die Textpositionen werden in einem globalen kartesischen Koordinatensystem $(x_{\text{global}}, y_{\text{global}})$ angegeben, welches üblicherweise mit der Bildebene übereinstimmt. Für ein rotiertes Symbol ist jedoch die Definition von Richtungen wie \enquote{unterhalb}, \enquote{rechts von} oder \enquote{links von} in globalen Koordinaten nicht mehr sinnvoll.

Zur Lösung dieses Problems wird für jedes Symbol $A$ ein lokales Koordinatensystem eingeführt, dessen Achsen mit der Orientierung des Symbols übereinstimmen. In diesem lokalen System wird die y-Achse parallel zur Längsrichtung des Symbols definiert, sodass die Begriffe \enquote{oberhalb} und \enquote{unterhalb} relativ zur Symbolausrichtung eindeutig werden.

Für ein Symbol $A$ mit Mittelpunkt $\vec{A}_{\text{pos}} = (x_A, y_A)$ und Rotationswinkel $\theta_A$ wird die Position eines Textkandidaten $C$ mit Mittelpunkt $\vec{C}_{\text{pos}} = (x_C, y_C)$ transformiert. Der globale Distanzvektor ist definiert als:

\begin{equation}
\vec{d}_{\text{global}} = \vec{C}_{\text{pos}} - \vec{A}_{\text{pos}} = \begin{pmatrix} dx \\ dy \end{pmatrix} = \begin{pmatrix} x_C - x_A \\ y_C - y_A \end{pmatrix}
\end{equation}

Die Transformation in das lokale Koordinatensystem des Symbols erfolgt durch eine Rotation um den negativen Winkel $-\theta_A$. Die Rotationsmatrix lautet:

\begin{equation}
\mathbf{R}(-\theta_A) = \begin{pmatrix} 
\cos(-\theta_A) & -\sin(-\theta_A) \\ 
\sin(-\theta_A) & \cos(-\theta_A) 
\end{pmatrix}
= \begin{pmatrix} 
\cos(\theta_A) & \sin(\theta_A) \\ 
-\sin(\theta_A) & \cos(\theta_A) 
\end{pmatrix}
\end{equation}

Der transformierte Distanzvektor im lokalen Koordinatensystem ergibt sich zu:

\begin{equation}
\begin{pmatrix} dx_{\text{local}} \\ dy_{\text{local}} \end{pmatrix} 
= \mathbf{R}(-\theta_A) \cdot \begin{pmatrix} dx \\ dy \end{pmatrix}
= \begin{pmatrix} 
dx \cdot \cos(\theta_A) + dy \cdot \sin(\theta_A) \\ 
-dx \cdot \sin(\theta_A) + dy \cdot \cos(\theta_A) 
\end{pmatrix}
\end{equation}

Mit dieser Transformation können nun Richtungsrelationen unabhängig von der globalen Rotation definiert werden:

\begin{itemize}
    \item \textbf{Unterhalb} des Symbols: $dy_{\text{local}} > 0$
    \item \textbf{Oberhalb} des Symbols: $dy_{\text{local}} < 0$
    \item \textbf{Rechts} vom Symbol: $dx_{\text{local}} > 0$
    \item \textbf{Links} vom Symbol: $dx_{\text{local}} < 0$
\end{itemize}

Die euklidische Distanz im lokalen System ist identisch zur Distanz im globalen System, da Rotationen längentreu sind:

\begin{equation}
d_{\text{local}} = \sqrt{dx_{\text{local}}^2 + dy_{\text{local}}^2} = \sqrt{dx^2 + dy^2} = d_{\text{global}}
\end{equation}

\subsubsection{Praktische Berechnung der Koordinatentransformation}

Die Implementierung der Koordinatentransformation erfolgt für jedes Symbol-Text-Paar einzeln. Zunächst werden die globalen Koordinaten des Symbols $(x_A, y_A)$ und des Textkandidaten $(x_C, y_C)$ aus den YOLO- bzw. OCR-Ergebnissen extrahiert. Der Rotationswinkel $\theta_A$ des Symbols liegt ebenfalls aus der YOLO-Detektion vor und muss von Gradmaß in Bogenmaß konvertiert werden. Die Berechnung der lokalen Koordinaten erfolgt dann durch direkte Anwendung der Transformationsgleichung (6.3), wobei die trigonometrischen Funktionen numerisch ausgewertet werden. 


\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.4]
    
    % LEFT: Rotated GKS with coordinate - THE PROBLEM
    \begin{scope}
        \node[font=\Large\bfseries] at (0,4) {Globales System};
        
        % Global axes (black, same style as right side)
        \draw[->, black, thick] (-1.5,0.5) -- (1.5,0.5) node[right, font=\small] {$x$};
        \draw[->, black, thick] (0,0.2) -- (0,3.5) node[above, font=\small] {$y$};
        
        % Rotated GKS at angle 40°
        \begin{scope}[shift={(0,1.3)}, rotate=40]
            % Inner trapezoid
            \draw[thick, blue, fill=blue!20, line width=1.5pt] 
                (-0.35,-0.4) -- (-0.25,0) -- (0.25,0) -- (0.35,-0.4) -- cycle;
            % Outer rectangle
            \draw[thick, blue, line width=1.5pt] 
                (-0.4,-0.45) rectangle (0.4,0.05);
            % Number inside
            \node[blue, font=\scriptsize] at (0,-0.2) {1234};
            
            % Coordinate positioned "above" the GKS AND ROTATED with it
            % Need to use 'transform shape' to rotate the node itself
            \node[green!70!black, draw=green!70!black, thick, font=\small, fill=white, 
                  transform shape] 
                 at (0,1.2) {12345,6};
        \end{scope}
        
        % Question mark and label
        \node[red, font=\Huge] at (-1.8,1.5) {\textbf{?}};
        \node[red, font=\normalsize, align=center] at (-1.8,0.5) {Beziehung\\unklar};
    \end{scope}
    
    % ARROW between systems (black, professional, simple)
    \draw[->, line width=1mm, black] (2.2,2) -- (3.8,2);
    \node[black, font=\normalsize, align=center] at (3,2.7) {Rotation um $-\theta$};
    
    % RIGHT: After transformation - THE SOLUTION  
    \begin{scope}[xshift=7cm]
        \node[font=\Large\bfseries] at (0,4) {Lokales System};
        
        % Local coordinate axes (black, same style as left side)
        \draw[->, black, thick] (-1.5,1.1) -- (1.8,1.1) node[right, font=\small] {$x_{\text{local}}$};
        \draw[->, black, thick] (0,0.3) -- (0,3.5) node[above, font=\small] {$y_{\text{local}}$};
        
        % GKS now aligned with local axes (horizontal)
        \begin{scope}[shift={(0,1.3)}]
            % Inner trapezoid
            \draw[thick, blue, fill=blue!20, line width=1.5pt] 
                (-0.35,-0.4) -- (-0.25,0) -- (0.25,0) -- (0.35,-0.4) -- cycle;
            % Outer rectangle
            \draw[thick, blue, line width=1.5pt] 
                (-0.4,-0.45) rectangle (0.4,0.05);
            % Number inside
            \node[blue, font=\scriptsize] at (0,-0.2) {1234};
        \end{scope}
        
        % Coordinate now directly above (in local coordinates, horizontal)
        \node[green!70!black, draw=green!70!black, thick, font=\small, fill=white] 
             at (0,2.5) {12345,6};
        
        % Show the measurement
        \draw[<->, green!70!black, line width=2.5pt] (1.0,1.35) -- (1.0,2.5);
        \node[green!70!black, font=\Large] at (2.0,1.95) {$dy > 0$};
        
        % Checkmark and label
        \node[green!70!black, font=\Huge] at (-1.8,2) {$\checkmark$};
        \node[green!70!black, font=\normalsize, align=center] at (-1.8,0.8) {Eindeutig:\\``oberhalb''};
    \end{scope}
    
\end{tikzpicture}
\caption{Prinzip der rotationsinvarianten Koordinatentransformation}
\label{fig:coordinate_transformation}
\end{figure}

Für jeden Textkandidaten im Umkreis eines Symbols wird diese Transformation durchgeführt, um die räumliche Beziehung in der lokalen Orientierung des Symbols zu bestimmen. Das Ergebnis sind drei Werte: die lokale x-Komponente $dx_{\text{local}}$, die lokale y-Komponente $dy_{\text{local}}$ sowie die euklidische Gesamtdistanz $d_{\text{local}}$. Diese Werte bilden die Grundlage für alle nachfolgenden Verknüpfungsoperationen, da sie eine rotationsinvariante Beschreibung der räumlichen Relation zwischen Symbol und Text ermöglichen.

\subsection{Proximity-basierter Linking-Algorithmus}
\label{subsec:proximity_linking}

\subsubsection{Dynamische Suchbereichsberechnung}

Im Gegensatz zu statischen Suchradien verwendet die Implementierung einen \textit{dimensionsadaptiven} Ansatz, bei dem die Suchbereiche dynamisch aus den Abmessungen der detektierten Bounding Boxes berechnet werden. Dies gewährleistet Robustheit gegenüber unterschiedlichen Symbolgrößen und Planauflösungen.

Für die Verknüpfung eines Ankersymbols (z.B. Signal, GKS-Platte) mit seiner zugehörigen Koordinatenbeschriftung werden die maximalen Suchentfernungen wie folgt berechnet:

\textbf{Vertikaler Suchbereich:}
\begin{equation}
dy_{\text{max}} = 1{,}6 \cdot h_{\text{anchor}} \cdot k_{dy}
\label{eq:dy_max}
\end{equation}

\textbf{Horizontaler Suchbereich:}
\begin{equation}
dx_{\text{max}} = \max\left( k_{dx} \cdot 0{,}6 \cdot \max(w_{\text{anchor}}, w_{\text{coord}}), \; 30 \right)
\label{eq:dx_max}
\end{equation}

wobei $h_{\text{anchor}}$ die Höhe und $w_{\text{anchor}}$ die Breite der Anker-Bounding-Box bezeichnen, $w_{\text{coord}}$ die Breite der Koordinaten-Box, und $k_{dy}$, $k_{dx}$ klassenspezifische Multiplikatoren sind. Der Mindestwert von 30 Pixeln für $dx_{\text{max}}$ stellt sicher, dass auch bei sehr schmalen Symbolen ein ausreichender horizontaler Suchbereich gewährleistet ist.

\subsubsection{Klassenspezifische Multiplikatoren}

Die Multiplikatoren wurden empirisch auf Basis der typischen Layoutkonventionen in Gleisplänen optimiert. Tabelle~\ref{tab:linking_multipliers} zeigt die konfigurierten Werte für alle Symbolklassen.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|r|r|c|}
\hline
\textbf{Klasse} & $\boldsymbol{k_{dy}}$ & $\boldsymbol{k_{dx}}$ & \textbf{$dy_{\text{max}}$} & \textbf{$dx_{\text{max}}$} & \textbf{Typ} \\
\hline
\multicolumn{6}{|c|}{\textit{Kernklassen}} \\
\hline
signal & 1.0 & 1.0 & 80--130 px & 30--50 px & Kern \\
gks\_festkodiert & 1.0 & 1.0 & 80--130 px & 30--50 px & Kern \\
gks\_gesteuert & 1.0 & 1.0 & 80--130 px & 30--50 px & Kern \\
gm\_block & 1.0 & 1.0 & 80--130 px & 30--50 px & Kern \\
coordinate & --- & --- & \multicolumn{2}{c|}{(wird verknüpft, nicht Anker)} & Kern \\
\hline
\multicolumn{6}{|c|}{\textit{Auxiliarklassen}} \\
\hline
haltepunkt & 1.0 & 1.0 & 80--130 px & 30--50 px & Auxiliar \\
sverbinder & 1.0 & 1.0 & 80--130 px & 30--50 px & Auxiliar \\
isolierstoß & 1.0 & 1.0 & 80--130 px & 30--50 px & Auxiliar \\
haltetafel & 1.0 & 2.0 & 80--130 px & 60--100 px & Auxiliar \\
prellblock & 1.0 & 2.0 & 80--130 px & 60--100 px & Auxiliar \\
weichenende & 1.0 & 3.0 & 80--130 px & 90--150 px & Auxiliar \\
weichengruppeende & 1.0 & 4.0 & 80--130 px & 120--200 px & Auxiliar \\
\hline
\end{tabular}
\caption{Klassenspezifische Linking-Parameter. Die Wertebereiche für $dy_{\text{max}}$ und $dx_{\text{max}}$ basieren auf typischen Symbolgrößen von 50--80 Pixeln bei 500 DPI.}
\label{tab:linking_multipliers}
\end{table}

Die erhöhten horizontalen Multiplikatoren ($k_{dx} > 1$) für Auxiliarklassen wie \textit{haltetafel}, \textit{prellblock} und \textit{weichengruppeende} reflektieren deren typische Layoutcharakteristik: Diese Symbole haben häufig horizontal versetzte Beschriftungen, während die Kernklassen überwiegend vertikal ausgerichtete Text-Symbol-Relationen aufweisen.

\subsubsection{Spezialfälle mit festen Suchparametern}

Für bestimmte komplexe Verknüpfungsaufgaben, die über die einfache Anker-Koordinaten-Zuordnung hinausgehen, werden feste Pixelwerte verwendet. Diese wurden empirisch auf Basis der bei 500 DPI typischen physikalischen Abstände kalibriert.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|r|r|p{5.5cm}|}
\hline
\textbf{Anwendungsfall} & \textbf{Parameter} & \textbf{Wert} & \textbf{Beschreibung} \\
\hline
\multicolumn{4}{|c|}{\textit{Fahrtrichtungsdetektion (vgl. Abschnitt~\ref{sec:fahrtrichtung})}} \\
\hline
GKS-Suche & $d_{\text{max}}$ & 250 px & Max. euklidische Distanz zum Signal \\
& $dy_{\text{min}}$ & 30 px & Min. vertikale Separation \\
& $dy_{\text{max}}$ & 200 px & Max. vertikale Separation \\
& $dx_{\text{tol}}$ & $\pm$120 px & Horizontale Toleranz \\
\hline
\multicolumn{4}{|c|}{\textit{Haltetafel-GKS-Fallback}} \\
\hline
GKS-Proximity & $d_{\text{max}}$ & 250 px & Euklidische Distanz zur GKS \\
& $dy_{\text{tol}}$ & 100 px & Max. vertikale Separation \\
& $dx_{\text{tol}}$ & 300 px & Max. horizontale Separation \\
\hline
\multicolumn{4}{|c|}{\textit{Fallback-Mechanismen}} \\
\hline
Isolierstoß & $r_{\text{max}}$ & 300 px & $360^\circ$-Suchradius bei fehlender Zuordnung \\
Gleismittellinie & $d_{\text{ray}}$ & 1500 px & Sicherheitslimit für Ray-Casting \\
\hline
\end{tabular}
\caption{Feste Suchparameter für Spezialfälle (kalibriert für 500 DPI)}
\label{tab:linking_special_params}
\end{table}

\textbf{Physikalische Interpretation:} Bei einer Renderauflösung von 500 DPI entsprechen die Parameter folgenden physikalischen Abständen:
\begin{itemize}
    \item 250 px $\approx$ 12,7 mm (typischer Abstand zwischen Signal und zugehöriger GKS-Platte)
    \item 120 px $\approx$ 6 mm (horizontale Toleranz für leicht versetzte Symbole)
    \item 300 px $\approx$ 15 mm (erweiterter Fallback-Suchbereich)
\end{itemize}

\subsubsection{Richtungsbasierte Filterung}

Neben der Distanzprüfung wird für jede Symbolklasse eine erwartete Richtung definiert, in der sich der zugehörige Text relativ zum Symbol befinden sollte. Die Richtungsbedingungen werden im lokalen Koordinatensystem (vgl. Abschnitt~\ref{sec:Koordinatentransformation}) formuliert:

\begin{itemize}
    \item \textbf{Kernklassen (Signal, GKS, GM-Block):} Text primär \emph{unterhalb} des Symbols erwartet ($dy_{\text{local}} > 0$). Dies entspricht der standardisierten Zeichnungskonvention für Bahnanlagenpläne.
    
    \item \textbf{Auxiliarklassen mit erhöhtem $k_{dx}$:} Flexiblere Richtungstoleranz, da Beschriftungen auch seitlich positioniert sein können ($dy_{\text{local}} > 0$ \textbf{oder} $|dx_{\text{local}}| < dx_{\text{max}}$).
\end{itemize}

Für den Spezialfall der \textit{tight}-Verknüpfung (bei eindeutiger räumlicher Nähe) wird der horizontale Suchbereich reduziert:

\begin{equation}
dx_{\text{max,tight}} = k_{dx} \cdot 0{,}45 \cdot \max(w_{\text{anchor}}, w_{\text{coord}})
\end{equation}

Zusätzlich wird bei aktivierter Linkssuche (\texttt{search\_left=True}) der horizontale Suchbereich asymmetrisch erweitert:

\begin{equation}
dx_{\text{max,left}} = 1{,}3 \cdot dx_{\text{max}} \quad \text{(falls Koordinate links vom Anker)}
\end{equation}

Diese Asymmetrie berücksichtigt, dass in einigen Gleisplänen Koordinatenangaben bevorzugt links von Symbolen platziert werden.

\subsubsection{Algorithmus zur Symbol-Text-Zuordnung}

Der vollständige Linking-Algorithmus kombiniert Distanzprüfung, Richtungsfilterung und Konflikterkennung in einem mehrstufigen Verfahren. Für jedes detektierte Symbol wird zunächst die Menge aller Textkandidaten betrachtet, die sich innerhalb des klassenspezifischen Suchradius befinden. Aus dieser Menge werden diejenigen Kandidaten ausgeschlossen, die nicht der erwarteten Richtungsbedingung genügen. Die verbleibenden Kandidaten werden als potenziell zugehörig betrachtet.

Aus der Menge der gültigen Kandidaten wird derjenige mit der geringsten euklidischen Distanz zum Symbol als beste Zuordnung ausgewählt. Bei Kandidaten mit sehr ähnlichen Distanzen (Differenz kleiner als 5 Pixel) wird zusätzlich die OCR-Konfidenz berücksichtigt: Der Kandidat mit höherer Erkennungssicherheit wird bevorzugt. Dies stellt sicher, dass bei räumlich ambigen Situationen die Qualität der Texterkennung als Entscheidungskriterium herangezogen wird.

Falls für ein Symbol keine Textkandidaten die Filterkriterien erfüllen, bleibt das Symbol zunächst ohne Textzuordnung. In diesem Fall kann im nächsten Schritt der adaptive Lernmechanismus aktiviert werden, um eine Zuordnung basierend auf statistischen Mustern zu versuchen. Symbole, für die auch nach allen Verknüpfungsversuchen kein Text zugeordnet werden konnte, werden im Validierungsdialog der Benutzeroberfläche zur manuellen Überprüfung gekennzeichnet.

\subsection{Adaptive Learning Mechanismus}
\label{subsec:adaptivelearningmechanism}
\subsubsection{Motivation und Konzept}

Obwohl technische Zeichnungen üblicherweise Konventionen folgen, existieren in der Praxis Variationen zwischen verschiedenen Planversionen, Zeichnern oder historischen Perioden. Ein statisches Regelwerk mit festen Suchradien und Richtungen kann in solchen Fällen suboptimal sein. Aus diesem Grund implementiert das System einen adaptiven Lernmechanismus, der erfolgreiche Verknüpfungen analysiert und statistische Muster extrahiert, um die Linking-Parameter dynamisch anzupassen.

Der Mechanismus basiert auf der Beobachtung, dass innerhalb eines einzelnen Gleisplans die Positionierung von Texten relativ zu Symbolen oft konsistent ist. Wenn beispielsweise alle Signalbezeichnungen systematisch 10 Pixel rechts und 20 Pixel unterhalb der Signalsymbole platziert sind, kann das System dieses Muster lernen und für nachfolgende unsichere Fälle nutzen.

\subsubsection{Pattern-Erfassung und statistische Auswertung}

Jede erfolgreiche Symbol-Text-Verknüpfung, die eine definierte Konfidenz-Schwelle überschreitet (typischerweise OCR-Konfidenz $> 0.85$ und eindeutige Regex-Validierung), wird als \enquote{validated link} betrachtet und zur statistischen Auswertung herangezogen. Für diese Links werden die lokalen Offset-Vektoren $(dx_{\text{local}}, dy_{\text{local}})$ persistiert und klassenweise aggregiert.

Das System verwaltet für jede Symbolklasse eine Sammlung von Offset-Vektoren sowie deren statistische Kenngrößen. Zu den gespeicherten Informationen gehören die Liste aller beobachteten Offsets, der Erwartungswert des Offsets, die Standardabweichung sowie die Anzahl der zugrundeliegenden Beobachtungen und der Zeitstempel der letzten Aktualisierung.

Für jede Symbolklasse werden folgende statistische Kenngrößen kontinuierlich aktualisiert. Der Erwartungswert des Offsets $(\mu_x, \mu_y)$ wird als arithmetisches Mittel aller beobachteten Offsets berechnet:

\begin{equation}
\mu_x = \frac{1}{N} \sum_{i=1}^{N} dx_i, \quad \mu_y = \frac{1}{N} \sum_{i=1}^{N} dy_i
\end{equation}

wobei $N$ die Anzahl der validierten Links für die entsprechende Klasse bezeichnet. Die Standardabweichung $(\sigma_x, \sigma_y)$ quantifiziert die Streuung der Offsets und wird berechnet als:

\begin{equation}
\sigma_x = \sqrt{\frac{1}{N-1} \sum_{i=1}^{N} (dx_i - \mu_x)^2}, \quad \sigma_y = \sqrt{\frac{1}{N-1} \sum_{i=1}^{N} (dy_i - \mu_y)^2}
\end{equation}

Die Verwendung der Stichprobenstandardabweichung (Division durch $N-1$ statt $N$) liefert einen erwartungstreuen Schätzer für die Populationsvarianz. Ein Minimum von $N \geq 10$ wird gefordert, bevor die statistischen Kenngrößen als hinreichend zuverlässig für prädiktive Zwecke betrachtet werden. Diese Schwelle stellt sicher, dass die Schätzungen der Verteilungsparameter nicht durch einzelne Ausreißer dominiert werden.

\subsubsection{Probabilistische Fenstersuche}

Wenn für ein Symbol keine Textkandidaten im Standard-Suchradius gefunden werden, wird die probabilistische Fenstersuche aktiviert, sofern für die entsprechende Symbolklasse ausreichend gelernte Patterns vorliegen. Dieses Verfahren nutzt die statistischen Kenngrößen aus den zuvor erfassten erfolgreichen Verknüpfungen, um eine erwartete Position für den fehlenden Text zu berechnen und dort gezielt zu suchen.

Die erwartete Position des Textes im globalen Koordinatensystem wird durch Rücktransformation des mittleren lokalen Offsets berechnet. Zunächst wird der in den Pattern-Statistiken gespeicherte mittlere Offset $(\mu_x, \mu_y)$ aus dem lokalen Koordinatensystem des Symbols zurück ins globale System transformiert. Dies erfolgt durch Anwendung der inversen Rotation, also durch Rotation um den positiven Winkel $+\theta_A$:

\begin{equation}
\begin{pmatrix} \Delta x_{\text{global}} \\ \Delta y_{\text{global}} \end{pmatrix}
= \begin{pmatrix} 
\cos(\theta_A) & -\sin(\theta_A) \\ 
\sin(\theta_A) & \cos(\theta_A) 
\end{pmatrix}
\cdot
\begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}
\end{equation}

Die erwartete globale Position des Textes ergibt sich dann durch Addition dieses transformierten Offsets zur Symbolposition:

\begin{equation}
\vec{C}_{\text{expected}} = \begin{pmatrix} x_{\text{expected}} \\ y_{\text{expected}} \end{pmatrix} 
= \begin{pmatrix} x_A \\ y_A \end{pmatrix} + \begin{pmatrix} \Delta x_{\text{global}} \\ \Delta y_{\text{global}} \end{pmatrix}
\end{equation}

Um die erwartete Position herum wird ein achsenparalleles Suchfenster definiert, dessen Ausdehnung durch die Standardabweichungen der gelernten Offset-Verteilung bestimmt wird. Das Suchfenster wird so gewählt, dass es einem zweifachen der Standardabweichung in jeder Koordinatenrichtung entspricht. Dies entspricht bei Annahme einer Normalverteilung einem Konfidenzintervall von ca. 95\%, d.h. die meisten tatsächlichen Textpositionen sollten innerhalb dieses Fensters liegen:

\begin{equation}
\text{Suchfenster}: \quad |x - x_{\text{expected}}| \leq 2\sigma_x, \quad |y - y_{\text{expected}}| \leq 2\sigma_y
\end{equation}

Alle Textkandidaten, deren Positionen innerhalb dieses Fensters liegen, werden als potenzielle Zuordnungen betrachtet. Aus dieser gefilterten Menge wird der zur erwarteten Position $\vec{C}_{\text{expected}}$ nächstgelegene Kandidat ausgewählt. Die Distanz wird dabei als euklidische Distanz zur erwarteten Position berechnet, nicht zur Symbolposition, da die erwartete Position die durch das Lernen verfeinerte beste Schätzung der Textlage darstellt.
Abbildung \ref{fig:adaptive_window} illustriert dieses Verfahren: Ausgehend vom Signal wird die erwartete Textposition durch den mittleren gelernten Offset bestimmt (roter Punkt). Das Suchfenster (rot gestrichelt) umfasst einen $2\sigma$ Bereich um diese Position. Nur Textkandidaten innerhalb dieses Fensters (grün) werden für die Zuordnung berücksichtigt, während außenliegende Kandidaten (schwarz) ignoriert werden
Die probabilistische Fenstersuche wird nur dann aktiviert, wenn mindestens 10 validierte Links für die entsprechende Symbolklasse vorliegen. Dies stellt sicher, dass die statistischen Schätzer eine ausreichende Präzision besitzen. Für seltene Symbolklassen oder zu Beginn der Verarbeitung, wenn noch wenige Links validiert wurden, greift das System auf die konventionelle Proximity-Suche zurück.

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.3]

    % Signal at origin
    \draw[very thick, blue] (-0.5,0) -- (0.5,0);
    \draw[very thick, blue] (0,-0.5) -- (0,0.5);
    \filldraw[blue] (0,0) circle (2pt);
    \node[blue, font=\normalsize] at (0,-0.8) {Signal $A$};
    
    % Standard search radius (very faint, for context only)
    \draw[dashed, black, thin] (0,0) circle (2.5);
    \node[black, font=\footnotesize] at (-1.6,-1.8) {Standard-Suchradius};
    
    % Learned offset vector
    \draw[->, very thick, red!70, line width=1.5pt] (0,0) -- (2.5,1.8);
    \node[red!70, font=\small, anchor=south east] at (1.2,0.9) {Gelernter Offset};
    
    % Expected position
    \filldraw[red] (2.5,1.8) circle (4pt);
    \node[red, font=\small, anchor=south] at (2.5,2.5) {Erwartete Position};
    \node[red, font=\small] at (2.5,1.4) {$(\mu_x, \mu_y)$};
    
    % Probabilistic window (larger, clearer)
    \draw[very thick, red, dashed, line width=1.5pt] (1.3,0.6) rectangle (3.7,3.0);
    
    % Window dimensions - placed outside, clearer
    \draw[<->, red, thick] (1.3,0.2) -- (3.7,0.2);
    \node[red, font=\normalsize, below] at (2.7,0.2) {$2\sigma_x$};
    
    \draw[<->, red, thick] (4.1,0.6) -- (4.1,3.0);
    \node[red, font=\normalsize, right] at (4.1,1.8) {$2\sigma_y$};
    
    % Text candidates INSIDE window (green box, black text, no dots)
    \node[green!70!black, draw=green!70!black, thick, fill=white, font=\normalsize] 
         at (2.7,2.2) {12345,6};
    
    \node[green!70!black, draw=green!70!black, thick, fill=white, font=\normalsize] 
         at (2.2,1.3) {12346,2};
    
    % Label for inside candidates
    \node[green!70!black, font=\small, anchor=west] at (4.0,2.5) {Im Fenster: berücksichtigt};
    
    % Text candidates OUTSIDE window (black box, black text, no dots)
    \node[black, draw=black, thick, fill=white, font=\normalsize] 
         at (4.5,3.5) {12347,8};
    
    \node[black, draw=black, thick, fill=white, font=\normalsize] 
         at (0.5,2.8) {12344,1};
    
    % Label for outside candidates (black)
    \node[black, font=\small, anchor=west] at (4.5,3.9) {Außerhalb: ignoriert};
    \node[black, font=\small, anchor=east] at (0.5,3.2) {Außerhalb: ignoriert};
    
\end{tikzpicture}
\caption{Probabilistische Fenstersuche basierend auf gelernten Offset-Patterns}
\label{fig:adaptive_window}
\end{figure}

Dieser adaptive Mechanismus ermöglicht es dem System, sich an plan-spezifische Layoutkonventionen anzupassen. Insbesondere bei historischen Plänen oder Zeichnungen von verschiedenen Bearbeitern, die von modernen Standardkonventionen abweichen, verbessert der Lernmechanismus die Linking-Rate signifikant. Empirisch zeigt sich eine Verbesserung der erfolgreichen Zuordnungen um ca. 12\% gegenüber rein statischen Regeln, insbesondere bei inkonsistenten oder nicht-standardkonformen Layouts.

\subsection{Komplexe Verknüpfungslogik für Spezialfälle}

\subsubsection{Fahrtrichtungsdetektion}
\label{sec:fahrtrichtung}

In Gleisplänen ist die Fahrtrichtung eines Signals ein kritisches Attribut, das angibt, in welche Richtung das Signal wirkt und für welche Fahrtrichtung es relevant ist. Die konventionelle Kodierung sieht zwei mögliche Fahrtrichtungen vor: Richtung A entspricht üblicherweise der Fahrt in positiver Streckenrichtung, Richtung B der Fahrt in negativer Streckenrichtung. 

Anstatt die Fahrtrichtung als separate Klassifikationsaufgabe zu behandeln, wird sie im implementierten System durch geometrische Analyse der räumlichen Beziehung zwischen Signal und zugeordneter GKS(festkodiert) bestimmt. Diese Methode nutzt die in Gleisplänen übliche Konvention, dass die relative Position der GKS(festkodiert) zum Signal die Fahrtrichtung kodiert.

Die Fahrtrichtung wird durch die Vorzeichenbetrachtung der lokalen y-Koordinate der GKS(festkodiert) relativ zum Signal bestimmt. Hierbei wird die in Abschnitt \ref{sec:Koordinatentransformation} beschriebene Koordinatentransformation angewendet, um die Position der GKS(festkodiert) im lokalen Koordinatensystem des Signals zu ermitteln. Die Entscheidungsregel lautet:

\begin{equation}
\text{Fahrtrichtung} = \begin{cases} 
\text{A} & \text{falls } dy_{\text{local}}(\text{GKS}, \text{Signal}) < 0 \\
\text{B} & \text{falls } dy_{\text{local}}(\text{GKS}, \text{Signal}) > 0
\end{cases}
\end{equation}

Eine negative lokale y-Koordinate bedeutet, dass die GKS sich oberhalb des Signals in dessen lokaler Orientierung befindet, was nach Konvention der Fahrtrichtung A entspricht. Eine positive lokale y-Koordinate indiziert, dass die GKS unterhalb des Signals liegt, was der Fahrtrichtung B zugeordnet wird. Abbildung~\ref{fig:signal_direction} veranschaulicht diese Bestimmung durch die relative Position der zugeordneten GKS im lokalen Koordinatensystem. Die Koordinatenangabe befindet sich zwischen Signal und GKS. Links ist der Fall $dy_{\text{local}} < 0$ (GKS oberhalb) dargestellt, was Fahrtrichtung~A entspricht; rechts der Fall $dy_{\text{local}} > 0$ (GKS unterhalb), entsprechend Fahrtrichtung~B.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=1.3]
    % --- LINKE SEITE (Fahrtrichtung A) ---
    \begin{scope}
        \node at (0,3.2) {\textbf{Fahrtrichtung A}};
        
        % Achsen (werden zuerst gezeichnet -> liegen hinten)
        \draw[->, red, thick] (-1.5,0) -- (1.5,0) node[right, font=\small] {$x_{\text{local}}$};
        \draw[->, red, thick] (0,-0.5) -- (0,2.5) node[above, font=\small] {$y_{\text{local}}$};
        
        % Signal (bei 0,0)
        \begin{scope}[shift={(0,0)}]
            \draw[thick, blue, fill=white] (-0.25,0) circle (0.25);
            \draw[thick, blue, fill=white] (0.25,0) circle (0.25);
            \draw[thick, blue] (0.50,0) -- (0.95,0);
        \end{scope}
        \node[blue, font=\small] at (0,-0.5) {Signal};
        
        % Koordinate (Box entfernt, fill=white verdeckt die Linie dahinter)
        \node[fill=white, font=\small, inner sep=2pt] at (0,1.2) {12345};
        
        % GKS (oberhalb)
        \begin{scope}[shift={(0,1.4)}]
            \draw[thick, green!70!black, fill=green!10] 
                (-0.35,0) -- (-0.25,0.4) -- (0.25,0.4) -- (0.35,0) -- cycle;
            \draw[thick, green!70!black] 
                (-0.4,-0.05) rectangle (0.4,0.45);
            \node[green!70!black, font=\scriptsize] at (0,0.2) {1234};
        \end{scope}
        \node[green!70!black, font=\small] at (0.8,1.6) {GKS};
        
        \node[font=\footnotesize] at (0,-1.0) {$dy_{\text{local}} < 0$};
        \node[font=\footnotesize] at (0,-1.4) {(GKS oberhalb)};
    \end{scope}
    
    % --- RECHTE SEITE (Fahrtrichtung B) ---
    \begin{scope}[xshift=5.5cm]
        \node at (0,3.2) {\textbf{Fahrtrichtung B}};
        
        % Offset (Symmetrie nach oben geschoben)
        \def\yoff{1.5} 
        
        % Achsen
        \draw[->, red, thick] (-1.5,\yoff) -- (1.5,\yoff) node[right, font=\small] {$x_{\text{local}}$};
        \draw[->, red, thick] (0,-0.5) -- (0,2.5) node[above, font=\small] {$y_{\text{local}}$};
        
        % Signal (bei y = 1.5)
        \begin{scope}[shift={(0,\yoff)}]
            \draw[thick, blue, fill=white] (-0.25,0) circle (0.25);
            \draw[thick, blue, fill=white] (0.25,0) circle (0.25);
            \draw[thick, blue] (0.50,0) -- (0.95,0);
        \end{scope}
        \node[blue, font=\small] at (0,\yoff+0.5) {Signal};
        
        % Koordinate (Box weg, Linie verdeckt)
        \node[fill=white, font=\small, inner sep=2pt] at (0,\yoff-1.2) {12345};
        
        % GKS unterhalb
        \begin{scope}[shift={(0,\yoff-1.4)}]
            \draw[thick, green!70!black, fill=green!10] 
                (-0.35,-0.4) -- (-0.25,0) -- (0.25,0) -- (0.35,-0.4) -- cycle;
            \draw[thick, green!70!black] 
                (-0.4,-0.45) rectangle (0.4,0.05);
            \node[green!70!black, font=\scriptsize] at (0,-0.2) {1234};
        \end{scope}
        \node[green!70!black, font=\small] at (0.8,\yoff-1.6) {GKS};
        
        % Annotation
        \node[font=\footnotesize] at (0,-1.0) {$dy_{\text{local}} > 0$};
        \node[font=\footnotesize] at (0,-1.4) {(GKS unterhalb)};
    \end{scope}
    
\end{tikzpicture}
\caption{Geometrische Bestimmung der Fahrtrichtung eines Signals}
\label{fig:signal_direction}
\end{figure}
Die konkreten Suchparameter für die GKS-Lokalisierung sind in Tabelle~\ref{tab:linking_special_params} dokumentiert: Die maximale Suchdistanz beträgt 250 Pixel ($\approx$ 12,7 mm bei 500 DPI), wobei die vertikale Separation zwischen 30 und 200 Pixeln liegen muss und eine horizontale Toleranz von $\pm$120 Pixeln erlaubt ist.
Diese geometrische Ableitung der Fahrtrichtung hat mehrere Vorteile gegenüber einer expliziten Klassifikation. Erstens wird kein separater Klassifikator benötigt, der auf Trainingsbeispielen für Fahrtrichtungen trainiert werden müsste. Zweitens ist die Methode robust gegenüber verschiedenen graphischen Darstellungsformen von Signalen, da sie ausschließlich auf der räumlichen Anordnung basiert und nicht auf visuellen Merkmalen des Symbols selbst. Drittens ist die Bestimmung rotationsinvariant, da sie im lokalen Koordinatensystem des Signals operiert.

\subsubsection{Haltepunkt-Gruppierung}

Ein Haltepunkt in einem Gleisplan ist eine komplexe Entität, die aus drei separaten Komponenten besteht, die räumlich assoziiert werden müssen. Die erste Komponente ist das Haltepunkt-Symbol selbst, welches die Position des Haltepunkts im Plan markiert. Die zweite Komponente ist das zugeordnete Signal, das den Haltepunkt absichert und den Zügen das Halt-Gebot signalisiert. Die dritte Komponente ist die Koordinatenangabe, typischerweise in Form einer Kilometerangabe, die den Haltepunkt auf der Strecke verortet.

Die korrekte Gruppierung dieser drei Elemente ist essentiell für die semantische Interpretation des Haltepunkts. Die geometrische Bedingung, die zur Identifikation zusammengehöriger Elemente herangezogen wird, basiert auf der Beobachtung, dass die Koordinate sich räumlich zwischen dem Haltepunkt-Symbol und dem zugeordneten Signal befinden sollte. Dies reflektiert die übliche Zeichnungskonvention, bei der die Kilometerangabe des Haltepunkts in der Nähe des Haltepunkt-Symbols angebracht wird, während das Signal etwas weiter entfernt positioniert ist.

Die Implementierung dieser geometrischen Bedingung erfolgt durch einen approximativen Kollinearitätstest. Für drei Punkte H (Haltepunkt-Symbol), K (Koordinate) und S (Signal) wird geprüft, ob K näherungsweise auf der Verbindungslinie zwischen H und S liegt. Exakte Kollinearität würde bedeuten, dass die Summe der Teilstrecken $d(H, K) + d(K, S)$ exakt gleich der direkten Distanz $d(H, S)$ ist. In der Praxis wird eine Toleranz $\epsilon$ eingeführt, um kleine Abweichungen von der perfekten Kollinearität zu erlauben:

\begin{equation}
d(H, K) + d(K, S) \leq d(H, S) + \epsilon
\end{equation}

wobei $d(\cdot, \cdot)$ die euklidische Distanz zwischen zwei Punkten bezeichnet. Die Toleranz $\epsilon$ wird typischerweise auf 20 Pixel gesetzt, um eine gewisse Flexibilität gegenüber nicht-perfekt linearen Anordnungen zu ermöglichen, gleichzeitig aber sicherzustellen, dass die Koordinate tatsächlich zwischen den beiden anderen Elementen liegt und nicht deutlich von der Verbindungslinie abweicht.

Der Gruppierungsalgorithmus iteriert über alle detektierten Haltepunkt-Symbole. Für jedes Haltepunkt-Symbol werden alle Signale und Koordinaten innerhalb eines definierten Umkreises (typischerweise 250 Pixeln) als potenzielle Gruppierungspartner betrachtet. Für jede Kombination aus Haltepunkt, Signal und Koordinate wird die Kollinearitätsbedingung geprüft. Die erste Kombination, die die Bedingung erfüllt, wird als gültige Gruppierung akzeptiert und persistiert.

Diese geometrische Gruppierungsmethode stellt sicher, dass die semantisch zusammengehörigen Elemente eines Haltepunkts korrekt identifiziert werden. In der exportierten Datenstruktur werden gruppierte Haltepunkte als zusammenhängende Einheiten behandelt, wobei alle drei Komponenten miteinander verlinkt sind. Dies ermöglicht es nachgelagerten Systemen oder menschlichen Anwendern, die vollständige Information eines Haltepunkts effizient abzurufen, ohne die drei Komponenten manuell zusammensuchen zu müssen.

\subsection{Zusammenfassung der Verknüpfungslogik}

Die implementierte Symbol-Text-Verknüpfung kombiniert mehrere komplementäre Ansätze:

\begin{enumerate}
    \item \textbf{Rotationsinvariante Geometrie}: Transformation in lokale Koordinatensysteme ermöglicht richtungsbasierte Filterung unabhängig von der globalen Orientierung.
    
    \item \textbf{Dimensionsadaptive Suchbereiche}: Dynamische Berechnung der Suchparameter aus Bounding-Box-Dimensionen (Gleichungen~\ref{eq:dy_max} und \ref{eq:dx_max}) mit klassenspezifischen Multiplikatoren gewährleistet Robustheit gegenüber unterschiedlichen Symbolgrößen und Planauflösungen.
    
    \item \textbf{Adaptive Lernmechanismen}: Statistische Analyse erfolgreicher Links ermöglicht Anpassung an plan-spezifische Layouts.
    
    \item \textbf{Komplexe Relationserkennung}: Spezielle Algorithmen für Fahrtrichtungsdetektion und Haltepunkt-Gruppierung behandeln semantisch reiche Objektbeziehungen.
\end{enumerate}

Die Kombination dieser Techniken erreicht eine hohe Linking-Genauigkeit, die in Kapitel~\ref{chap:evaluation} quantitativ evaluiert wird.


\section{Datenvalidierung und Qualitätssicherung}
\label{sec:validierungundsicherung}
Die Validierung extrahierter Daten stellt einen kritischen Schritt zur Sicherstellung 
der Datenqualität dar. Das implementierte System kombiniert regelbasierte Validierung 
mit intelligenten Korrekturvorschlägen und adressiert damit die Anforderungen 
\textbf{FA-008} (manuelle Korrektur durch Human-in-the-Loop), \textbf{NFA-004} 
(Robustheit gegenüber fehlerhaften Eingaben) und \textbf{NFA-005} (Prüfbarkeit 
durch strukturierte Fehlerberichte).

\subsection{Validierungsarchitektur}

\subsubsection{Grundstruktur des Validierungsframeworks}
Die Validierungsarchitektur basiert auf einem modularen Framework, das die extrahierten Daten in tabellarischer Form entgegennimmt und eine strukturierte Fehleranalyse durchführt. Das zentrale Validierungsmodul prüft dabei einen DataFrame, der alle Erkennungen aus der YOLO-Pipeline, die zugehörigen OCR-Texte sowie die durch das Linking-Modul hergestellten Verknüpfungen enthält. Die Ausgabe der Validierung besteht aus einer Liste von Validierungsproblemen, wobei jedes Problem durch mehrere Attribute charakterisiert wird.

Ein Validierungsproblem wird durch folgende Eigenschaften beschrieben. Der Schweregrad klassifiziert das Problem in die Kategorien Fehler, Warnung oder Information, wobei Fehler kritische Inkonsistenzen darstellen, die zwingend einer Korrektur bedürfen, während Warnungen auf potenzielle Probleme hinweisen, die einer Überprüfung bedürfen. Die Kategorie beschreibt die Art des Problems, beispielsweise Formatfehler, Duplikate oder fehlende Daten. Das betroffene Feld identifiziert die spezifische Datenspalte, in der das Problem auftritt. Bei automatisch korrigierbaren Problemen wird zusätzlich ein Korrekturvorschlag mit dem empfohlenen neuen Wert bereitgestellt. Die Konfidenz des Korrekturvorschlags wird als normalisierter Wert im Intervall $[0, 1]$ angegeben, wobei höhere Werte eine größere Zuverlässigkeit der vorgeschlagenen Korrektur indizieren.

Die Validierung erfolgt durch sequenzielle Ausführung aller implementierten Prüfungen, wobei das Ergebnis die Gesamtheit aller identifizierten Probleme sowie statistische Kennzahlen zur Fehlerverteilung umfasst. Diese strukturierte Repräsentation ermöglicht sowohl die programmatische Weiterverarbeitung als auch die Präsentation in einer benutzerfreundlichen Oberfläche.
\textbf{Validierungsfokus:} Die regelbasierte Validierung konzentriert sich primär auf 
die fünf Kernklassen, da diese für den produktiven Einsatz bei Siemens Mobility 
relevant sind. Für Auxiliarklassen werden grundlegende Formatprüfungen durchgeführt, 
jedoch keine domänenspezifischen Plausibilitätsprüfungen.
\subsubsection{Dreistufige Validierungsstrategie}

Die Validierung ist hierarchisch in drei Ebenen organisiert, die aufeinander aufbauen und unterschiedliche Aspekte der Datenqualität adressieren. Diese Strukturierung ermöglicht eine systematische Fehleridentifikation von grundlegenden Formatproblemen bis hin zu komplexen semantischen Inkonsistenzen.

\begin{figure}[htbp]
    \centering
    % --- HIER IST DIE ÄNDERUNG ---
    % Ein horizontaler Abstand von 1.5cm schiebt das Bild nach rechts.
    % Passe den Wert an (z.B. 1cm, 2.5cm), bis es perfekt sitzt.
    \hspace*{1.5cm} 
    % -----------------------------
    \begin{tikzpicture}[
        node distance=1.5cm,
        box/.style={rectangle, draw, minimum width=6cm, minimum height=1cm, align=center, fill=blue!10},
        layer/.style={rectangle, draw, minimum width=7cm, minimum height=1.5cm, align=center, fill=green!10, rounded corners},
        arrow/.style={->, >=stealth, thick}
    ]

    % Input
    \node[box] (input) {DataFrame\\(YOLO + OCR + Linking)};

    % Layer 1
    \node[layer, below=of input] (layer1) {
        \textbf{Ebene 1: Syntaktische Validierung}\\
        Regex-Patterns pro Klasse
    };

    % Layer 2
    \node[layer, below=of layer1] (layer2) {
        \textbf{Ebene 2: Semantische Validierung}\\
        Wertebereich, Konsistenzprüfung
    };

    % Layer 3
    \node[layer, below=of layer2] (layer3) {
        \textbf{Ebene 3: Räumliche Validierung}\\
        Proximity, Duplikate, YOLO-Konfidenz
    };

    % Output
    \node[box, below=of layer3, fill=orange!10] (output) {ValidationIssue Liste\\(Fehler, Warnungen, Hinweise)};

    % Arrows
    \draw[arrow] (input) -- (layer1);
    \draw[arrow] (layer1) -- (layer2);
    \draw[arrow] (layer2) -- (layer3);
    \draw[arrow] (layer3) -- (output);

    % Side annotations
    \node[right=1.5cm of layer1, text width=3cm, font=\small] {Format-\\korrektheit};
    \node[right=1.5cm of layer2, text width=3cm, font=\small] {Inhaltliche\\Plausibilität};
    \node[right=1.5cm of layer3, text width=3cm, font=\small] {Geometrische\\Beziehungen};

    \end{tikzpicture}
    \caption{Dreistufige Validierungsarchitektur mit hierarchischer Fehleranalyse}
    \label{fig:validation_architecture}
\end{figure}

Die erste Ebene, die syntaktische Validierung, prüft die formale Korrektheit der extrahierten Texte anhand klassenspezifischer Muster. Für jede Objektklasse wurde ein regulärer Ausdruck definiert, der das erwartete Textformat beschreibt. Diese Muster basieren auf der Analyse realer Gleispläne und den geltenden Standards für Signalbezeichnungen und Kilometrierungen.

Für Signalbezeichnungen wird das Muster durch den regulären Ausdruck beschrieben, der eine bis vier Großbuchstaben (einschließlich deutscher Umlaute Ä, Ö, Ü) gefolgt von einer bis vier Ziffern erwartet. Das entsprechende Regex-Muster lautet: ein bis vier Zeichen aus der Menge A bis Z sowie Ä, Ö, Ü, gefolgt von einer bis vier Ziffern. Dieses Format entspricht der standardisierten Signalnomenklatur, bei der Buchstaben den Signaltyp oder die Streckenbezeichnung kodieren, während Ziffern die spezifische Signalnummer angeben. Beispiele für gültige Signalbezeichnungen sind somit A101, BHR201 oder F3a.

Für GKS-Platten (Gleiskontakte) gilt ein numerisches Format mit drei bis vier Ziffern. GKS-Platten werden ausschließlich durch diese Ziffernfolgen identifiziert, beispielsweise 1234 oder 567.

Die Validierung von Koordinatentexten erfolgt mehrstufig, da Koordinatenangaben sowohl einen numerischen Teil als auch optionale Zusatzinformationen enthalten können. Der numerische Hauptteil besteht aus einer bis drei Ziffern, gefolgt von einem Dezimaltrenner (Punkt oder Komma), gefolgt von drei bis vier Ziffern. Dies berücksichtigt unterschiedliche Schreibweisen in verschiedenen Dokumentversionen. Zusätzlich wird geprüft, dass keine Kleinbuchstaben im Text vorkommen, mit Ausnahme der standardisierten Abkürzung Gl für Gleis. Ebenfalls wird validiert, dass zwischen den Ziffern keine Buchstaben eingefügt sind, da dies auf OCR-Fehler hinweist. Bei Koordinaten mit Gleisangaben wird zudem die korrekte Klammerung geprüft, sodass Angaben wie 10.5123(Gl.1) als gültig erkannt werden, während fehlerhafte Klammerungen wie 10.5123(Gl.1 als problematisch gekennzeichnet werden.

Für die Fahrtrichtungsangabe bei Signalen gilt das restriktive Muster, das ausschließlich die beiden Werte A und B zulässt, entsprechend der binären Richtungskodierung im Eisenbahnwesen.

Die zweite Ebene, die semantische Validierung, überprüft die inhaltliche Plausibilität der extrahierten Werte über die reine Formatprüfung hinaus. Koordinatenwerte werden auf einen plausiblen Wertebereich geprüft, wobei Kilometrierungen im Intervall $[0, 300]$ erwartet werden, da dies dem typischen Streckenbereich deutscher Bahnstrecken entspricht. Werte außerhalb dieses Bereichs werden als potenzielle OCR-Fehler gekennzeichnet.

Bei mehrfach vorkommenden Signalen wird die Konsistenz der zugeordneten Attribute überprüft. Ein Signal mit identischer Bezeichnung sollte in allen Instanzen dieselbe Kilometrierung aufweisen, da physisch identische Signale an einem festen geografischen Punkt lokalisiert sind. Ebenso wird die Konsistenz der Fahrtrichtung geprüft, wobei Inkonsistenzen auf Erkennungsfehler oder tatsächliche Duplikate hindeuten können.

Die dritte Ebene, die räumliche Validierung, analysiert die geometrischen Beziehungen zwischen den erkannten Objekten. Für Objektklassen, die zwingend eine Kilometrierung benötigen wie Signale, GKS-Platten, Weichenblöcke und Streckentrenner wird geprüft, ob eine Koordinate in räumlicher Nähe vorhanden ist. Die Definition räumlicher Nähe erfolgt klassenspezifisch gemäß den in Abschnitt~\ref{subsec:proximity_linking} definierten dynamischen Suchbereichen. Für die Kernklassen (Signal, GKS, GM-Block) resultiert dies typischerweise in vertikalen Suchbereichen von 80--130 Pixeln und horizontalen Bereichen von 30--50 Pixeln (vgl. Tabelle~\ref{tab:linking_multipliers}). Diese Schwellenwerte basieren auf der durchschnittlichen Größe der Symbole und der typischen Anordnung von Text relativ zu Symbolen in Gleisplänen.

Zusätzlich werden YOLO-Konfidenzwerte gegen klassenspezifische Schwellenwerte validiert. Jede Objektklasse besitzt einen eigenen Mindestkonfidenzwert, der aus der Trainingsphase des YOLO-Modells abgeleitet wurde. Erkennungen unterhalb dieser Schwellenwerte werden als unsicher gekennzeichnet und zur manuellen Überprüfung vorgeschlagen.

Die Duplikaterkennung identifiziert mehrfache Detektionen desselben Objekts durch Analyse der räumlichen Positionen. Zwei Erkennungen derselben Klasse werden als potenzielle Duplikate betrachtet, wenn ihre euklidische Distanz
\begin{equation}
d = \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}
\end{equation}
einen Schwellenwert von 50 Pixeln unterschreitet, wobei $(x_{1}, y_{1})$ und $(x_{2}, y_{2})$ die Zentrumskoordinaten der beiden Detektionen darstellen.

\subsection{Automatische Fehlerkorrektur}

\subsubsection{Korrekturmuster und Heuristiken}

Basierend auf der Analyse häufiger OCR-Fehler in der Gleisplanerkennung wurden regelbasierte Korrekturmuster entwickelt, die systematisch auftretende Fehler automatisch beheben können. Diese Korrekturmuster adressieren typische Fehlerquellen, die aus den Limitierungen der OCR-Engines sowie aus der spezifischen Natur von Gleisplandokumenten resultieren.

Ein häufiger Fehler betrifft die Groß- und Kleinschreibung bei alphanumerischen Codes. OCR-Engines tendieren dazu, Großbuchstaben als Kleinbuchstaben zu interpretieren, insbesondere bei geringer Schriftgröße oder suboptimaler Bildqualität. Die Korrektur erfolgt durch systematische Umwandlung aller Kleinbuchstaben in Großbuchstaben für Signalbezeichnungen und ähnliche Codes, da die standardisierte Notation ausschließlich Großbuchstaben vorsieht. Diese Transformation wird mit einer Konfidenz von 0.95 bewertet, da die Regel nahezu ausnahmslos anwendbar ist.

Leerzeichenfehler (Whitespace-Fehler) entstehen, wenn OCR-Engines fälschlicherweise Leerzeichen in zusammenhängende Zeichenketten einfügen. Dies tritt besonders bei Signalbezeichnungen auf, wo beispielsweise A101 als A 101 erkannt werden kann. Die Korrektur besteht in der Entfernung aller Leerzeichen aus alphanumerischen Codes. Diese Operation wird mit einer Konfidenz von 0.80 bewertet, da in seltenen Fällen legitime Leerzeichen existieren können, die jedoch im Kontext der Gleisplanerkennung unüblich sind.

Die Normalisierung der Fahrtrichtungsangaben adressiert das Problem, dass OCR-Engines die vorgeschriebenen Großbuchstaben A und B gelegentlich als Kleinbuchstaben a und b interpretieren. Die Korrektur erfolgt durch Konversion zu Großbuchstaben, sofern der erkannte Text genau einem Zeichen entspricht und entweder a oder b ist. Diese hochspezifische Regel erreicht eine Konfidenz von 0.90.

Für GKS-Nummern wurde beobachtet, dass OCR-Engines gelegentlich Trennzeichen wie Bindestriche oder Punkte in reine Ziffernfolgen einfügen. Die Korrekturstrategie entfernt alle nicht-numerischen Zeichen aus GKS-Codes und validiert anschließend, ob das Ergebnis dem erwarteten Format von drei bis vier Ziffern entspricht. Aufgrund der höheren Unsicherheit dieser Transformation wird eine Konfidenz von 0.70 vergeben, da in Einzelfällen die entfernten Zeichen tatsächlich Teil der korrekten Bezeichnung sein könnten.

\subsubsection{Konfidenzbasierte Anwendungsstrategie}

Die Anwendung automatischer Korrekturen erfolgt nicht undifferenziert, sondern wird durch ein konfidenzbasiertes Bewertungssystem gesteuert. Jede potenzielle Korrektur wird mit einem normalisierten Konfidenzwert im Intervall $[0, 1]$ versehen, der die Zuverlässigkeit der Transformation quantifiziert. Dieser Wert wird aus mehreren Faktoren abgeleitet, insbesondere der Eindeutigkeit der Korrektur, der Häufigkeit des identifizierten Fehlermusters und dem Risiko einer Fehlkorrektur.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    scale=0.75, 
    transform shape, 
    % --- HIER IST DER TRICK ---
    % Wir setzen die Grundschrift auf \large. 
    % Durch scale=0.75 wird sie im PDF dann zu \normalsize.
    font=\large, 
    % --------------------------
    node distance=2cm and 2cm,
    decision/.style={diamond, draw, fill=yellow!20, text width=3cm, align=center, inner sep=2pt},
    action/.style={rectangle, draw, fill=green!20, minimum width=3cm, minimum height=1cm, align=center},
    reject/.style={rectangle, draw, fill=red!20, minimum width=3cm, minimum height=1cm, align=center},
    arrow/.style={->, >=stealth, thick}
]

    % Root
    \node[decision] (root) {Korrektur-\\vorschlag\\verfügbar};

    % First level
    \node[decision, below=of root] (conf_check) {Konfidenz\\$c$?};

    % Actions
    \node[action, below left=of conf_check, xshift=-0.5cm] (high) {Automatische\\Anwendung};
    \node[action, below=of conf_check] (medium) {Benutzer-\\bestätigung};
    \node[reject, below right=of conf_check, xshift=0.5cm] (low) {Nur\\Dokumentation};

    % Arrows
    \draw[arrow] (root) -- node[right] {ja} (conf_check);
    
    % HIER AUCH ANPASSEN:
    % Vorher war hier \small. Da alles skaliert wird, nehmen wir jetzt \normalsize, 
    % damit es im Bild wie \small aussieht.
    \draw[arrow] (conf_check) -- node[above left, font=\large] {$c \geq 0.8$} (high);
    \draw[arrow] (conf_check) -- node[right, font=\large] {$0.6 \leq c < 0.8$} (medium);
    \draw[arrow] (conf_check) -- node[above right, font=\large] {$c < 0.6$} (low);

    % Confidence labels (Auch hier eine Stufe größer wählen)
    \node[below=0.3cm of high, font=\normalsize, text=black] {Hohe Konfidenz};
    \node[below=0.3cm of medium, font=\normalsize, text=black] {Mittlere Konfidenz};
    \node[below=0.3cm of low, font=\normalsize, text=black] {Niedrige Konfidenz};

\end{tikzpicture}
\caption{Konfidenzbasierte Entscheidungslogik für Korrekturvorschläge}
\label{fig:confidence_decision_readable}
\end{figure}



Die Konfidenzwerte werden in drei Klassen kategorisiert, die unterschiedliche Handlungsempfehlungen implizieren. Korrekturen mit hoher Konfidenz, definiert als $c \geq 0.8$, repräsentieren eindeutige Transformationen mit minimalem Risiko. Diese beinhalten typischerweise regelbasierte Transformationen wie die Konversion von Klein- zu Großbuchstaben oder die Entfernung offensichtlich fehlerhafter Leerzeichen. Solche Korrekturen können mit hoher Sicherheit automatisch angewendet werden.

Korrekturen mit mittlerer Konfidenz im Bereich $0.6 \leq c < 0.8$ indizieren plausible Transformationen, die jedoch ein gewisses Restrisiko bergen. Diese Kategorie umfasst Korrekturen, bei denen kontextabhängige Faktoren eine Rolle spielen oder wo die OCR-Ausgabe prinzipiell mehrdeutig sein könnte. Solche Vorschläge werden dem Benutzer präsentiert, aber nicht automatisch angewendet, sondern bedürfen einer manuellen Bestätigung.

Korrekturen mit niedriger Konfidenz, charakterisiert durch $c < 0.6$, werden als unsicher eingestuft und primär zu Dokumentationszwecken erfasst. Diese Vorschläge dienen dazu, den Benutzer auf potenzielle Probleme aufmerksam zu machen, ohne eine konkrete Handlungsempfehlung zu implizieren.

Die technische Umsetzung der Korrekturanwendung erfolgt durch selektive Modifikation des zugrundeliegenden Datencontainers. Für jedes identifizierte Problem mit dem Attribut automatisch korrigierbar und vorhandenem Korrekturvorschlag wird der entsprechende Eintrag im Datensatz lokalisiert und der Feldwert durch den vorgeschlagenen Wert ersetzt. Diese Operation erfolgt atomar für jede Korrektur und wird in einem Korrekturprotokoll dokumentiert, das die ursprünglichen Werte, die neuen Werte sowie die Zeilennummern der betroffenen Einträge enthält.

\subsection{Integration in die Benutzeroberfläche}

\subsubsection{Dialogbasierte Validierungsansicht}

Die Präsentation der Validierungsergebnisse erfolgt über einen modalen Dialog, der eine umfassende Übersicht über alle identifizierten Probleme sowie interaktive Funktionen zur Korrekturverwaltung bietet. Der Dialog ist als Tab-basiertes Interface konzipiert, das eine kategoriebasierte Navigation durch die Validierungsergebnisse ermöglicht und unterschiedliche Ansichten für verschiedene Arbeitsschritte bereitstellt.

Der erste Tab präsentiert die vollständige Liste aller identifizierten Validierungsprobleme in tabellarischer Form. Diese Gesamtansicht ermöglicht einen schnellen Überblick über den Validierungsstatus des gesamten Datensatzes und unterstützt die Priorisierung von Korrekturmaßnahmen. Die Tabelle umfasst Spalten für den Schweregrad, die Problemkategorie, die betroffene Objektklasse, die Zeilenidentifikation, das fehlerhafte Feld, eine Problembeschreibung sowie den aktuellen fehlerhaften Wert.

Ein spezialisierter Tab ist den automatisch korrigierbaren Problemen gewidmet und implementiert ein interaktives Auswahlsystem. Jede Zeile in dieser Ansicht repräsentiert einen konkreten Korrekturvorschlag und ist mit einem Auswahlfeld versehen, das dem Benutzer die selektive Auswahl einzelner Korrekturen ermöglicht. Die visuelle Gestaltung unterstützt die Entscheidungsfindung durch eine farbliche Differenzierung: Der aktuelle fehlerhafte Wert wird mit rotem Hintergrund dargestellt, während der vorgeschlagene Korrekturwert grün hinterlegt ist. Die Konfidenz der Korrektur wird numerisch sowie durch Farbcodierung visualisiert, wobei grüne Schrift hohe Konfidenz, orange Schrift mittlere Konfidenz und rote Schrift niedrige Konfidenz signalisiert.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{images/Kapitel6/Validierungsdialog.png} 
\caption{Schematische Darstellung der fünfteiligen Tab-Struktur des Validierungsdialogs}
\label{fig:validation_dialog_ui}
\end{figure}



Weitere Tabs bieten gefilterte Ansichten, die ausschließlich Fehler, Warnungen oder Informationshinweise anzeigen. Diese kategoriespezifischen Ansichten reduzieren die kognitive Last bei der Fehleranalyse, indem sie eine fokussierte Bearbeitung nach Priorität ermöglichen. Kritische Fehler, die zwingend einer Korrektur bedürfen, können somit isoliert betrachtet werden, ohne durch weniger dringende Warnungen oder informative Hinweise überlagert zu werden.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}

% Severity colors
\node[font=\small\bfseries] at (0,2) {Schweregrad:};
\node[rectangle, draw, fill=red!20, minimum width=2cm, minimum height=0.6cm] at (0,1.3) {Fehler};
\node[rectangle, draw, fill=orange!20, minimum width=2cm, minimum height=0.6cm] at (0,0.6) {Warnung};
\node[rectangle, draw, fill=blue!20, minimum width=2cm, minimum height=0.6cm] at (0,-0.1) {Hinweis};

% Confidence colors
\node[font=\small\bfseries] at (5,2) {Konfidenz:};
\node[rectangle, draw, fill=green!30, minimum width=2.5cm, minimum height=0.6cm] at (5,1.3) {Hoch ($c \geq 0.8$)};
\node[rectangle, draw, fill=orange!30, minimum width=2.5cm, minimum height=0.6cm] at (5,0.6) {Mittel ($0.6 \leq c < 0.8$)};
\node[rectangle, draw, fill=red!30, minimum width=2.5cm, minimum height=0.6cm] at (5,-0.1) {Niedrig ($c < 0.6$)};

% Value highlighting
\node[font=\small\bfseries] at (2.5,-1.2) {Wert-Darstellung:};
\node[rectangle, draw, fill=red!15, minimum width=2cm, minimum height=0.5cm, font=\footnotesize] at (1,-1.9) {Aktuell};
\node[rectangle, draw, fill=green!15, minimum width=2cm, minimum height=0.5cm, font=\footnotesize] at (4,-1.9) {Vorschlag};

\end{tikzpicture}
\caption{Farbkodierung für Validierungsprobleme und Korrekturvorschläge}
\label{fig:validation_color_coding}
\end{figure}


\subsubsection{Jump-to-Detection Funktionalität}

Eine wesentliche Herausforderung bei der manuellen Validierung besteht in der effizienten Lokalisierung problematischer Erkennungen im Originaldokument. Das System adressiert dies durch eine integrierte Navigation, die eine direkte Verlinkung zwischen Validierungsproblemen und der entsprechenden Position im PDF-Viewer ermöglicht.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{images/Kapitel6/Validierungsdialog2.png} 
\caption{Jump-to-Detection Workflow zwischen Validierungsdialog und Gleisplan-Ansicht}
\label{fig:jump_to_detection}
\end{figure}

Für Validierungsprobleme, die eine räumliche Position besitzen, wird in der Tabelle eine zusätzliche Aktionsspalte mit einem Navigationsbutton bereitgestellt. Die Aktivierung dieses Buttons löst ein Signal aus, das die Identifikation der betroffenen Zeile sowie die räumlichen Koordinaten $(x, y)$ der Erkennung transportiert. Dieses Signal wird vom übergeordneten Workspace-Modul empfangen und verarbeitet. Die Verarbeitung umfasst mehrere Schritte: Zunächst wird die entsprechende Zeile im Baumansicht-Widget des Workspace selektiert, wodurch die Detailinformationen der Erkennung im unteren Panel angezeigt werden. Anschließend wird die PDF-Ansicht auf die entsprechende Seite navigiert und die Ansicht so zentriert, dass die Position der Erkennung im sichtbaren Bereich liegt. Abschließend wird ein visuelles Highlighting der Erkennungsregion aktiviert, typischerweise durch Einblendung eines farbigen Rahmens oder einer semitransparenten Überlagerung.

Die technische Implementierung dieser Funktionalität basiert auf dem Signal-Slot-Mechanismus von PyQt5. Der Validierungsdialog emittiert ein Signal, das die Zeilenidentifikation sowie die Position als Tupel überträgt. Der Workspace registriert einen Slot für dieses Signal, der die beschriebene Navigations- und Highlighting-Logik implementiert.

\subsubsection{Interaktiver Validierungsworkflow}

Der typische Arbeitsablauf bei der Validierung und Korrektur extrahierter Daten gliedert sich in mehrere Phasen. Initial wählt der Benutzer die Validierungsfunktion aus dem Anwendungsmenü, was die Ausführung aller konfigurierten Validierungsprüfungen auf dem aktuellen Datensatz initiiert. Die Validierungsengine analysiert sequenziell alle Daten und sammelt identifizierte Probleme in einer strukturierten Liste.

Nach Abschluss der Validierung wird der Ergebnisdialog präsentiert, der eine statistische Zusammenfassung der identifizierten Probleme am oberen Rand anzeigt. Diese Zusammenfassung quantifiziert die Anzahl der Fehler, Warnungen und Hinweise sowie die Anzahl automatisch korrigierbarer Probleme, aufgeschlüsselt nach Konfidenzklassen. Diese Übersicht ermöglicht eine schnelle Einschätzung des Validierungsstatus und der erforderlichen Korrekturmaßnahmen.

Der Benutzer navigiert anschließend durch die verschiedenen Tabs, um die Detailansichten der Probleme zu inspizieren. Im Tab für automatische Korrekturen werden die Korrekturvorschläge geprüft und durch Aktivierung oder Deaktivierung der zugehörigen Checkboxen selektiert. Die Anzahl der selektierten Korrekturen wird dynamisch aktualisiert und in der Beschriftung des Anwendungs-Buttons reflektiert.

Die Anwendung der selektierten Korrekturen erfolgt durch Aktivierung des entsprechenden Buttons, was zunächst einen Bestätigungsdialog aufruft. Dieser Dialog präsentiert eine Zusammenfassung der anzuwendenden Änderungen und warnt den Benutzer, dass die Modifikationen sofort im Datensatz wirksam werden. Nach Bestätigung werden die Korrekturen sequenziell auf den Datensatz angewendet, und der Dialog schließt sich. Die korrigierten Daten sind unmittelbar im Workspace sichtbar und können anschließend persistent in der Datenbank gespeichert werden.

\subsection{Persistierung und Export der Validierungsergebnisse}

\subsubsection{Export-Formate für Validierungsberichte}

Das System bietet zwei komplementäre Export-Optionen für Validierungsergebnisse, die unterschiedliche Anwendungsfälle adressieren. Der detaillierte Textbericht stellt eine menschenlesbare, narrative Darstellung der Validierungsergebnisse bereit, während der tabellarische CSV-Export eine maschinenlesbare Repräsentation für statistische Auswertungen und Weiterverarbeitung ermöglicht.

Der Textbericht beginnt mit einer strukturierten Kopfzeile, die den Berichtstyp und den Generierungszeitpunkt dokumentiert. Es folgt eine statistische Zusammenfassung, die alle aggregierten Kennzahlen der Validierung präsentiert, einschließlich der Gesamtanzahl identifizierter Probleme, der Verteilung nach Schweregraden sowie der Anzahl automatisch korrigierbarer Probleme differenziert nach Konfidenzklassen. Falls automatische Korrekturen bereits angewendet wurden, werden diese in einem separaten Abschnitt aufgelistet, wobei für jede Korrektur die Zeilenidentifikation, das betroffene Feld sowie die Transformation vom ursprünglichen zum korrigierten Wert dokumentiert wird.

Der Hauptteil des Textberichts gruppiert die identifizierten Probleme nach Kategorien. Für jede Kategorie wird ein dedizierter Abschnitt erstellt, der alle zugehörigen Probleme in strukturierter Form auflistet. Jedes Problem wird durch mehrere Zeilen beschrieben: die Zeilenidentifikation mit Schweregrad, das betroffene Feld, die detaillierte Problembeschreibung, den aktuellen fehlerhaften Wert sowie gegebenenfalls den Korrekturvorschlag inklusive Konfidenzwert. Diese narrative Struktur ermöglicht eine intuitive Erfassung der Validierungsergebnisse und eignet sich für manuelle Überprüfungen oder als Dokumentation des Validierungsprozesses.

Der tabellarische CSV-Export repräsentiert die Validierungsergebnisse in einer flachen, relationalen Struktur. Jede Zeile der CSV-Datei korrespondiert zu einem identifizierten Problem, wobei folgende Spalten definiert sind: Schweregrad als kategoriale Variable, Kategorie zur Klassifikation des Problemtyps, Zeilenidentifikation zur Referenzierung im ursprünglichen Datensatz, betroffenes Feld als Spaltenname, detaillierte Problembeschreibung als Freitext, aktueller fehlerhafter Wert, Korrekturvorschlag falls vorhanden, und Konfidenzwert des Vorschlags. Diese Struktur ermöglicht den Import in Tabellenkalkulationssoftware oder statistische Analysewerkzeuge und unterstützt quantitative Auswertungen der Validierungsergebnisse.

\subsubsection{Datenbankbasierte Persistierung}

Zur langfristigen Speicherung und Nachverfolgbarkeit werden Validierungsergebnisse in einer PostgreSQL-Datenbank persistiert. Das Datenbankschema (Abbildung~\ref{fig:validation_db_schema}) umfasst drei spezialisierte Tabellen für Validierungsprotokolle, Qualitätsmetriken und manuelle Korrekturen.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=3cm,
    table/.style={rectangle split, rectangle split parts=2, draw, minimum width=4.5cm, fill=blue!10, font=\small},
    arrow/.style={->, >=stealth, thick}
]

% Main reference table (top)
\node[table] (layout) {
    \textbf{track\_layouts}
    \nodepart{second}
    \begin{tabular}{@{}l@{}}
    \underline{id} (PK)\\
    layout\_name
    \end{tabular}
};

% validation_log table (bottom left)
\node[table, below left=2.5cm and 1cm of layout] (vlog) {
    \textbf{validation\_log}
    \nodepart{second}
    \begin{tabular}{@{}l@{}}
    \underline{id} (PK)\\
    layout\_id (FK)\\
    validation\_type\\
    severity\\
    message\\
    row\_id\\
    details (JSONB)\\
    created\_at
    \end{tabular}
};

% quality_metrics table (bottom center)
\node[table, below=2.5cm of layout] (qmetrics) {
    \textbf{quality\_metrics}
    \nodepart{second}
    \begin{tabular}{@{}l@{}}
    \underline{id} (PK)\\
    layout\_id (FK)\\
    metric\_name\\
    metric\_value\\
    metric\_data (JSONB)\\
    computed\_at
    \end{tabular}
};

% manual_corrections table (bottom right)
\node[table, below right=2.5cm and 1cm of layout] (mcorr) {
    \textbf{manual\_corrections}
    \nodepart{second}
    \begin{tabular}{@{}l@{}}
    \underline{id} (PK)\\
    layout\_id (FK)\\
    row\_id\\
    column\_name\\
    old\_value\\
    new\_value\\
    correction\_type\\
    corrected\_by\\
    correction\_reason\\
    created\_at
    \end{tabular}
};

% Foreign key relationships
\draw[arrow] (vlog.north) -- (layout.south) node[midway, left, font=\tiny, sloped] {N:1};
\draw[arrow] (qmetrics.north) -- (layout.south) node[midway, right, font=\tiny] {N:1};
\draw[arrow] (mcorr.north) -- (layout.south) node[midway, right, font=\tiny, sloped] {N:1};

% Optional: Add constraint annotations
\node[below=0.2cm of vlog.south, font=\tiny, text width=4cm, align=center] 
    {severity $\in$ \{ERROR, WARNING, INFO\}};

\end{tikzpicture}
\caption{PostgreSQL Datenbankschema für Validierungspersistierung}
\label{fig:validation_db_schema}
\end{figure}


Die Tabelle für Validierungsprotokolle (validation\_log) speichert detaillierte Informationen zu jedem identifizierten Problem. Die Struktur umfasst eine eindeutige Identifikation des Protokolleintrags, eine Referenz zum zugehörigen Gleisplan, die Klassifikation des Validierungstyps, den Schweregrad als standardisierte Kategorie (ERROR, WARNING, INFO), eine textuelle Problembeschreibung, eine optionale Referenz zur betroffenen Zeile im extrahierten Datensatz, ein JSON-Feld für zusätzliche kontextuelle Informationen sowie einen Zeitstempel der Protokollierung. Diese Struktur ermöglicht sowohl die Rekonstruktion des Validierungszustands zu einem bestimmten Zeitpunkt als auch die Analyse von Fehlertrends über mehrere Extraktionsdurchläufe hinweg.






Die Qualitätsmetriken-Tabelle (quality)aggregiert quantitative Kennzahlen zur Bewertung der Extraktionsqualität. Gespeichert werden der Name der Metrik als Identifikator, ein numerischer Wert für quantitative Metriken, ein JSON-Feld für strukturierte Metrikdaten sowie ein Zeitstempel der Berechnung. Typische Metriken umfassen die durchschnittliche YOLO-Konfidenz aggregiert über alle Klassen sowie klassenspezifisch, die OCR-Erfolgsrate für Koordinaten und Signalbezeichnungen als Quotient erfolgreich erkannter Texte zur Gesamtzahl der Objekte, die Linking-Erfolgsrate als Anteil der Objekte mit korrekt verknüpften Koordinaten, sowie die Anzahl niedrig-konfidenter Erkennungen differenziert nach Objektklasse. Diese Metriken ermöglichen eine quantitative Bewertung der Systemleistung und unterstützen die Identifikation von Optimierungspotenzialen.

Die dritte Tabelle (manual\_corrections)protokolliert manuelle Korrekturen, die durch den Benutzer nach Abschluss der automatischen Validierung vorgenommen werden. Für jede Korrektur werden die Zeilenidentifikation, der Name der modifizierten Spalte, der ursprüngliche Wert vor Korrektur, der neue Wert nach Korrektur, eine Typisierung der Korrektur zur Kategorisierung, die Identifikation des korrigierenden Benutzers, eine optionale textuelle Begründung sowie ein Zeitstempel gespeichert. Diese Daten dienen mehreren Zwecken: Sie ermöglichen eine vollständige Nachvollziehbarkeit aller Datenmodifikationen, unterstützen die Analyse häufiger Korrekturmuster zur Identifikation systematischer Fehler in der Extraktionspipeline, und liefern wertvolle Trainingsdaten für die iterative Verbesserung der zugrundeliegenden Modelle.

Zur Unterstützung statistischer Analysen wurde eine Datenbankansicht implementiert, die Korrekturstatistiken aggregiert. Diese Ansicht fasst Korrekturen nach Gleisplan und Spaltenname zusammen und berechnet die Gesamtanzahl der Korrekturen, die Anzahl betroffener unterschiedlicher Zeilen sowie den Zeitraum der ersten und letzten Korrektur. Diese aggregierten Daten ermöglichen die Identifikation besonders fehleranfälliger Datenfelder und unterstützen die Priorisierung von Verbesserungsmaßnahmen.

Die Implementierung der Datenbankpersistierung nutzt prepared statements und parametrisierte Abfragen zur Vermeidung von SQL-Injection-Vulnerabilitäten. Transaktionale Semantik gewährleistet die atomare Speicherung zusammengehöriger Datensätze. Indizes auf häufig abgefragten Spalten optimieren die Leistung bei Analyseabfragen über große Validierungsdatenmengen.

\subsection{Zusammenfassung}

Das Validierungssystem kombiniert regelbasierte Prüfungen mit intelligenten Korrekturvorschlägen und bietet eine benutzerfreundliche UI zur Qualitätssicherung. Durch Persistierung der Ergebnisse in PostgreSQL werden langfristige Qualitätsanalysen und iterative Modellverbesserungen ermöglicht. Korrekturen mit niedriger Konfidenz oder mehrdeutigen Fällen werden dem Benutzer zur manuellen Prüfung vorgelegt.

\section{Unterstützende Komponenten}
\label{sec:unterstützendekomponente}
Neben der Kernfunktionalität der Extraktionspipeline (Objekterkennung, Texterkennung, Symbol-Text-Verknüpfung) wurden mehrere unterstützende Komponenten entwickelt, die für den praktischen Einsatz des Systems in realen Anwendungsszenarien erforderlich sind. Diese Komponenten adressieren die Anforderungen an Versionsverwaltung, Rückverfolgbarkeit und persistente Datenhaltung, welche für den operativen Einsatz in ingenieurwissenschaftlichen Workflows unerlässlich sind.

\subsection{Versionsvergleich und Änderungsdetektion}
\label{subsec:vergleichundänderung}
Die systematische Identifikation und Dokumentation von Änderungen zwischen verschiedenen Versionen eines Gleisplans stellt eine zentrale Anforderung in der Eisenbahnplanung dar. Diese Funktionalität wird vom Benutzer manuell über die Benutzeroberfläche initiiert, nicht als Teil der automatischen Verarbeitungspipeline. Zur Realisierung dieser Funktionalität wurde ein Diff-Algorithmus implementiert, der auf der Generierung eindeutiger Identifikatoren und mengentheoretischen Operationen basiert.

\subsubsection{Generierung eindeutiger Identifikatoren}

Für die eindeutige Identifikation extrahierter Objekte über verschiedene Planversionen hinweg wird ein deterministischer Unique Identifier (UID) generiert. Die UID-Generierung erfolgt durch Konkatenation der Objektklasse, des normalisierten Textinhalts und eines räumlichen Index, um Dopplungen bei identischem Textinhalt zu vermeiden:

\begin{equation} \text{UID}(o) = \text{class}(o) \oplus \text{normalize}(\text{content}(o)) \oplus \text{GridIndex}(x, y) \end{equation}

wobei GridIndex(x,y) die Koordinaten auf ein grobes Raster abbildet, um Robustheit gegen minimale Pixelverschiebungen zu gewährleisten, während Identität bei gleicher Semantik an verschiedenen Orten unterschieden wird. $\oplus$ bezeichnet die String-Konkatenation und die Normalisierungsfunktion $\text{normalize}: \Sigma^* \rightarrow \Sigma^*$ Leerzeichen entfernt und Sonderzeichen standardisiert, um konsistente Identifikatoren zu gewährleisten.
\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, minimum width=2.5cm, minimum height=0.8cm, align=center},
    arrow/.style={->, >=stealth, thick}
]

% Example 1: Signal
\node[box] (sig1) {Signal\\AS102};
\node[box, below=0.5cm of sig1] (sig2) {class: ``signal''\\text: ``AS102''};
\node[box, below=0.5cm of sig2] (sig3) {\texttt{signal\_AS102}};

\draw[arrow] (sig1) -- (sig2) node[midway, right, font=\small] {Extraktion};
\draw[arrow] (sig2) -- (sig3) node[midway, right, font=\small] {UID-Gen.};

% Example 2: GKS
\node[box, right=2cm of sig1] (gks1) {GKS\\12345};
\node[box, below=0.5cm of gks1] (gks2) {class: ``gks''\\text: ``12345''};
\node[box, below=0.5cm of gks2] (gks3) {\texttt{gks\_12345}};

\draw[arrow] (gks1) -- (gks2);
\draw[arrow] (gks2) -- (gks3);

% Example 3: Coordinate
\node[box, right=2cm of gks1] (coord1) {Koordinate\\18.1606};
\node[box, below=0.5cm of coord1] (coord2) {class: ``coordinate''\\text: ``18.1606''};
\node[box, below=0.5cm of coord2] (coord3) {\texttt{coordinate\_18.1606}};

\draw[arrow] (coord1) -- (coord2);
\draw[arrow] (coord2) -- (coord3);

\end{tikzpicture}
\caption{Schematische Darstellung des UID-Generierungsprozesses für verschiedene Objekttypen}
\label{fig:uid_generation}
\end{figure}


\textbf{Beispiele für generierte UIDs:}
\begin{itemize}
    \item Signal mit Bezeichnung ``AS102'' $\rightarrow$ \texttt{signal\_AS102}
    \item GKS mit Kennung ``12345'' $\rightarrow$ \texttt{gks\_12345}
    \item Kilometerangabe ``18.1606'' $\rightarrow$ \texttt{coordinate\_18.1606}
\end{itemize}

Die Verwendung von UIDs ermöglicht eine effiziente Objektzuordnung zwischen Planversionen, selbst bei Änderungen der räumlichen Position oder anderer Attribute.

\subsubsection{Differenzalgorithmus}

Der Vergleichsalgorithmus basiert auf mengentheoretischen Operationen zwischen zwei Extraktionsmengen $A$ (Altversion) und $B$ (Neuversion). Seien $A = \{a_1, a_2, \ldots, a_m\}$ und $B = \{b_1, b_2, \ldots, b_n\}$ die Mengen extrahierter Objekte mit ihren UIDs als Mengenelemente.


\textbf{Primäre Differenzmengen:}
\begin{align}
\Delta_{\text{add}} &= B \setminus A \quad \text{(hinzugefügte Objekte)} \\
\Delta_{\text{rem}} &= A \setminus B \quad \text{(entfernte Objekte)} \\
\Delta_{\text{pot}} &= A \cap B \quad \text{(potentiell modifizierte Objekte)}
\end{align}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.2]
    % Draw two bigger circles, adjust position of the second circle
    \draw[thick, fill=blue!20, fill opacity=0.5] (0,0) circle (3cm);
    \draw[thick, fill=red!20, fill opacity=0.5] (4,0) circle (3cm);

    % Labels for circles, adjusted position
    \node at (-2, 3.5) {\Large\textbf{Version A (Alt)}};
    \node at (6, 3.5) {\Large\textbf{Version B (Neu)}};

    % Region labels, adjusted position (moved up)
    \node at (-1.5, 1.5) {\textbf{$\Delta_{\text{rem}}$}};
    \node at (2, 1.5) {\textbf{$\Delta_{\text{pot}}$}};
    \node at (5.5, 1.5) {\textbf{$\Delta_{\text{add}}$}};

    % Example objects, adjusted positions to fit the larger circles
    \node[font=\small] at (-1.8, 0.5) {signal\_F3};
    \node[font=\small] at (-1.8, -0.5) {gks\_98765};

    \node[font=\small] at (2, 0.7) {signal\_AS102};
    \node[font=\small] at (2, 0) {coordinate\_18.1606};
    \node[font=\small] at (2, -0.7) {gks\_12345};
    \node[font=\small] at (5.8, 0.5) {signal\_F5};
    \node[font=\small] at (5.8, -0.5) {gks\_11111};

    % Legend removed as requested
\end{tikzpicture}
\caption{Mengentheoretische Visualisierung des Diff-Algorithmus}
\label{fig:diff_algorithm}
\end{figure}
Für Objekte in der Schnittmenge $\Delta_{\text{pot}}$ wird eine attributbasierte Änderungsanalyse durchgeführt. Sei $o_A \in A$ und $o_B \in B$ ein korrespondierendes Objektpaar mit identischer UID, dann erfolgt der Vergleich folgender Attribute:

\textbf{Textmodifikation:}
\begin{equation}
\text{isModified}_{\text{text}}(o_A, o_B) = \begin{cases}
\text{true}, & \text{falls } \text{content}(o_A) \neq \text{content}(o_B) \\
\text{false}, & \text{sonst}
\end{cases}
\end{equation}

\textbf{Positionsänderung:}
Zur Detektion signifikanter Positionsänderungen wird die euklidische Distanz zwischen den Zentroiden berechnet:

\begin{equation}
d(o_A, o_B) = \sqrt{(x_B - x_A)^2 + (y_B - y_A)^2}
\end{equation}

wobei $(x_A, y_A)$ und $(x_B, y_B)$ die Koordinaten der jeweiligen Objektzentroiden in Pixeln bezeichnen. Ein Objekt gilt als räumlich verschoben, falls:

\begin{equation}
\text{isMoved}(o_A, o_B) = \begin{cases}
\text{true}, & \text{falls } d(o_A, o_B) > \tau_{\text{dist}} \\
\text{false}, & \text{sonst}
\end{cases}
\end{equation}


\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
    % Grid
    \draw[step=1cm, gray!30, very thin] (0,0) grid (10,8);
    
    % Object in Version A (old position)
    \draw[thick, fill=blue!30] (2,4) rectangle (3.5,5.5);
    \node at (2.75, 4.75) {\textbf{A}};
    \node[below, font=\small] at (2.75, 4) {$(x_A, y_A)$};
    \node[below, font=\footnotesize] at (2.75, 3.6) {$(1024, 768)$};
    
    % Object in Version B (new position)
    \draw[thick, fill=red!30] (7,6) rectangle (8.5,7.5);
    \node at (7.75, 6.75) {\textbf{B}};
    \node[below, font=\small] at (7.75, 6) {$(x_B, y_B)$};
    \node[below, font=\footnotesize] at (7.75, 5.6) {$(1050, 770)$};
    
    % Distance line
    \draw[thick, dashed, color=black] (2.75, 4.75) -- (7.75, 6.75);
    \node[above, rotate=22, color=black, font=\small] at (5.25, 5.75) {$d = \sqrt{(x_B-x_A)^2 + (y_B-y_A)^2}$};
    
    % Threshold circle (schematic)
    \draw[dotted, thick, color=green!70!black] (2.75, 4.75) circle (1.5cm);
    \node[font=\footnotesize, color=green!70!black] at (4.5, 3.8) {$\tau_{\text{dist}} = 50$ px};
    
    % Annotation
    \node[align=center, font=\small] at (5, 1) {
        Objekt als \textbf{verschoben} markiert,\\
        da $d > \tau_{\text{dist}}$
    };
\end{tikzpicture}
\caption{Geometrische Darstellung der Positionsänderungserkennung mit euklidischer Distanz}
\label{fig:position_change}
\end{figure}

Der empirisch ermittelte Schwellenwert $\tau_{\text{dist}} = 50$ Pixel berücksichtigt geringfügige Positionsschwankungen durch PDF-Rendering-Variationen und fokussiert auf semantisch relevante Verschiebungen.

\subsubsection{Strukturierte Änderungsdokumentation}

Die identifizierten Änderungen werden in einem strukturierten Change Report dokumentiert, der als Excel-Datei exportiert wird und folgende Informationen je Änderung enthält:

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|l|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{UID} & \textbf{Änderungstyp} & \textbf{Zustand (Alt)} & \textbf{Zustand (Neu)} \\
\hline
signal\_AS102 & Verschoben & $x=1024, y=768$ & $x=1050, y=770$ \\
signal\_F5 & Hinzugefügt & $-$ & $x=2300, y=1200$ \\
gks\_98765 & Entfernt & $x=1500, y=900$ & $-$ \\
coordinate\_18.1606 & Textänderung & Text: ``18.1606'' & Text: ``18.1608'' \\
\hline
\end{tabular}
\caption{Beispielhafte Change Report Struktur mit verschiedenen Änderungstypen}
\label{tab:change_report}
\end{table}

Diese strukturierte Dokumentation ermöglicht eine effiziente Nachvollziehbarkeit von Planänderungen und unterstützt Quality-Assurance-Prozesse in der Eisenbahnplanung.
Die Implementierung umfasst eine dedizierte grafische Benutzeroberfläche zur Visualisierung und Analyse der Änderungen, wie in Abbildung~\ref{fig:diff_ui} dargestellt.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{images/Kapitel6/diff_dialog.png}
\caption{Versionsvergleichs-Dialog mit tabellarischer Auflistung aller Änderungen zwischen zwei Planversionen}
\label{fig:diff_ui}
\end{figure}

Der Dialog zeigt eine tabellarische Übersicht aller identifizierten Änderungen mit Spalten für Objektklasse, Text, Feldname, alte und neue Werte sowie Seitenzuordnung. In diesem Beispiel wurden fünf GKS-Koordinaten modifiziert, wobei die Verschiebungen im Bereich von +0{,}0m bis +0{,}6m liegen. Die Änderungstypen werden durch Icons visualisiert: Hinzugefügte Elemente erhalten ein grünes Plus-Symbol, entfernte Elemente ein rotes Minus-Symbol, und modifizierte Werte werden orange hervorgehoben. Diese Darstellung erfüllt Anforderung FA-011 und ermöglicht Ingenieuren eine schnelle Identifikation relevanter Planänderungen ohne manuelle Vergleichsarbeit.

Ein konkretes Beispiel einer detektierten Koordinatenänderung zeigen die Abbildungen~\ref{fig:gks_change_before} und~\ref{fig:gks_change_after} für die GKS-Platte mit der Kennung 0305.

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{images/Kapitel6/gkschange1.png}
    \caption{GKS 0305 in Version 1: Koordinate 0.1770}
    \label{fig:gks_change_before}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Kapitel6/gkschange2.png}
    \caption{GKS 0305 in Version 2: Koordinate 0.1729}
    \label{fig:gks_change_after}
\end{minipage}
\end{figure}

Die automatische Änderungsdetektion identifizierte eine Verschiebung um $\Delta = -0{,}0041$ km (ca. 4,1 Meter) durch UID-basierten Objektvergleich. Der Algorithmus erkannte, dass beide Versionen dasselbe Objekt (\texttt{gks\_gesteuert\_0305}) beschreiben, jedoch unterschiedliche Koordinatenwerte aufweisen. Solche Präzisionsänderungen im Meterbereich sind in der Feinplanung von Bahnanlagen kritisch und müssen dokumentiert werden, da sie die Positionierung sicherheitsrelevanter Komponenten betreffen.
\subsection{Rückverfolgbarkeit zur Quelldokumentation}

Die vollständige Rückverfolgbarkeit extrahierter Daten zur Originalquelle ist essentiell für die Validierung und manuelle Nachkorrektur von Extraktionsergebnissen. Hierzu wird für jeden extrahierten Datensatz ein umfassender Metadatensatz persistiert.
\begin{figure}[h]
\centering
\begin{tikzpicture}[
    scale=0.95, 
    transform shape,
    % --- Baum-Einstellungen ---
    level 1/.style={sibling distance=4.2cm, level distance=1.8cm}, % Weiter auseinander für Platz
    level 2/.style={sibling distance=1.5cm, level distance=1.8cm}, % Enger zusammen
    edge from parent/.style={draw, thick, black},
    % --- Styles ---
    box/.style={
        rectangle, 
        draw=blue!80!black, 
        thick,
        rounded corners=2pt, 
        fill=blue!10, 
        minimum width=2.5cm, 
        text width=2.5cm, % Erzwingt Umbruch
        minimum height=0.8cm, 
        align=center, 
        font=\footnotesize\bfseries
    },
    subbox/.style={
        rectangle, 
        draw=gray!60!black, 
        fill=gray!10, 
        minimum width=1.2cm, 
        text width=1.2cm, % Schmale Spalten für Blätter
        minimum height=0.6cm, 
        align=center, 
        font=\scriptsize
    },
    rootbox/.style={
        rectangle,
        draw=orange!80!black,
        thick,
        rounded corners=2pt,
        fill=orange!20,
        minimum width=4cm,
        align=center,
        font=\small\bfseries
    }
]

% Root
\node[rootbox] {Extrahiertes Objekt\\(signal\_AS102)}
    % Branch 1: Quelldokument
    child {node[box] {Quelldokument}
        child {node[subbox] {Datei-\\name}}
        child {node[subbox] {SHA-256\\Hash}}
        child {node[subbox] {Seite:\\3}}
    }
    % Branch 2: Räumliche Daten
    child {node[box] {Räumliche\\Daten}
        child {node[subbox] {$(x, y)$:\\(1024,\\768)}}
        child {node[subbox] {OBB:\\4 Pkt}}
    }
    % Branch 3: Extraktion
    child {node[box] {Extraktion}
        child {node[subbox] {OCR:\\Paddle}}
        child {node[subbox] {$c_{\text{OCR}}$:\\0.94}}
        child {node[subbox] {$c_{\text{YOLO}}$:\\0.89}}
    }
    % Branch 4: Temporal
    child {node[box] {Temporal}
        child {node[subbox] {ISO 8601\\Time-\\stamp}}
    };

\end{tikzpicture}
\caption{Hierarchische Struktur der Metadaten für ein extrahiertes Objekt}
\label{fig:metadata_structure}
\end{figure}

\subsubsection{Metadatenmodell}

Das Metadatenmodell umfasst folgende Informationsebenen:

\textbf{Quelldokument-Identifikation:}
\begin{itemize}
    \item Dateiname der Quell-PDF ($f_{\text{source}}$)
    \item Kryptographischer Hash (SHA-256) zur Integritätssicherung
    \item Seitennummer innerhalb des Dokuments ($p \in \mathbb{N}$)
\end{itemize}

\textbf{Räumliche Lokalisierung:}
\begin{itemize}
    \item Zentroid-Koordinaten in absoluten Pixelkoordinaten: $(x, y) \in \mathbb{R}^2$
    \item Oriented Bounding Box Geometrie: $\text{OBB} = \{(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4)\}$
    \item Die OBB-Repräsentation ermöglicht die präzise Visualisierung rotierter Textelemente
\end{itemize}

\textbf{Extraktionsdetails:}
\begin{itemize}
    \item Verwendete OCR-Engine ($e \in \{\text{PaddleOCR}, \text{Tesseract}, \text{EasyOCR}\}$)
    \item Konfidenzscore der Objekterkennung: $c_{\text{YOLO}} \in [0, 1]$
    \item Konfidenzscore der Texterkennung: $c_{\text{OCR}} \in [0, 1]$
    \item Angewandte Linking-Methode ($m \in \{\text{standard}, \text{adaptive}\}$)
\end{itemize}

\textbf{Temporale Zuordnung:}
\begin{itemize}
    \item ISO 8601 formatierter Zeitstempel der Extraktion
\end{itemize}

\subsubsection{Integration in die Benutzeroberfläche}

Die Metadaten werden zur Realisierung einer bidirektionalen Navigation zwischen Datenansicht und Quelldokument genutzt:

\textbf{Tabellenbasierte Navigation:}
Beim Anklicken eines Datensatzes in der Ergebnistabelle wird der PDF-Viewer auf die entsprechende Seite navigiert, auf die dokumentierten Koordinaten zentriert und das Objekt mittels farbkodiertem Overlay hervorgehoben. Die OBB-Geometrie ermöglicht dabei eine pixelgenaue Umrandung auch bei rotierten Textelementen.

\textbf{PDF-basierte Navigation:}
Interaktives Anklicken eines visualisierten Objekts im PDF-Viewer triggert ein automatisches Scrolling zur korrespondierenden Zeile in der Ergebnistabelle, wodurch die zugehörigen Extraktionsdaten unmittelbar einsehbar werden.

Diese Funktionalität reduziert den kognitiven Aufwand bei der manuellen Validierung signifikant und ermöglicht eine effiziente Identifikation systematischer Extraktionsfehler.




\subsection{Persistente Datenhaltung}

Das System verwendet PostgreSQL (Version 14.9) zur persistenten Speicherung von Extraktionsergebnissen, Workspace-Zuständen und Qualitätsmetriken.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{images/Kapitel6/Postgresql datenbank diagramm .png} 
\caption{Datenbankschema mit Kardinalitäten (vereinfachte Darstellung)}
\label{fig:database_schema}
\end{figure}

\subsubsection{Datenbankschema}

Das Datenbankschema ist auf die Anforderungen eines dokumentbasierten Workflows optimiert und nutzt JSONB-Datentypen für flexible Speicherung semi-strukturierter Extraktionsdaten:

\begin{verbatim}
Relation: track_layouts
-----------------------
id                SERIAL PRIMARY KEY
layout_name       TEXT UNIQUE NOT NULL

Relation: workspaces
--------------------
id                SERIAL PRIMARY KEY
layout_id         INTEGER UNIQUE NOT NULL REFERENCES track_layouts(id) 
                  ON DELETE CASCADE
edited_data_json  JSONB NOT NULL              -- Extraktionsergebnisse
track_skeleton    TEXT                        -- Komprimiertes Gleisskelett
image_dimensions  JSONB                       -- Seitenabmessungen
last_modified     TIMESTAMPTZ DEFAULT NOW()

Relation: validation_log
------------------------
id                SERIAL PRIMARY KEY
layout_id         INTEGER NOT NULL REFERENCES track_layouts(id)
validation_type   VARCHAR(100) NOT NULL
severity          VARCHAR(20) CHECK (severity IN ('ERROR', 'WARNING', 'INFO'))
message           TEXT NOT NULL
row_id            INTEGER                     -- Referenz zu Datensatz
details           JSONB
created_at        TIMESTAMPTZ DEFAULT NOW()

Relation: quality_metrics
-------------------------
id                SERIAL PRIMARY KEY
layout_id         INTEGER NOT NULL REFERENCES track_layouts(id)
metric_name       VARCHAR(100) NOT NULL
metric_value      REAL
metric_data       JSONB
computed_at       TIMESTAMPTZ DEFAULT NOW()

Relation: manual_corrections
----------------------------
id                SERIAL PRIMARY KEY
layout_id         INTEGER NOT NULL REFERENCES track_layouts(id)
row_id            INTEGER NOT NULL
column_name       VARCHAR(100) NOT NULL
old_value         TEXT
new_value         TEXT
correction_type   VARCHAR(50)
corrected_by      VARCHAR(100) DEFAULT 'user'
correction_reason TEXT
created_at        TIMESTAMPTZ DEFAULT NOW()

-- Performance-Indizes
CREATE INDEX idx_validation_log_layout ON validation_log(layout_id);
CREATE INDEX idx_validation_log_severity ON validation_log(severity);
CREATE INDEX idx_quality_metrics_layout ON quality_metrics(layout_id);
CREATE INDEX idx_manual_corrections_layout ON manual_corrections(layout_id);
\end{verbatim}

\subsubsection{JSONB-basierte Datenspeicherung}

Die Extraktionsergebnisse werden als JSONB-Array in \texttt{edited\_data\_json} gespeichert. Diese Designentscheidung bietet mehrere Vorteile gegenüber einer vollständig normalisierten Tabellenstruktur:

\textbf{Datenstruktur:}
Jeder Eintrag in \texttt{edited\_data\_json} repräsentiert ein extrahiertes Objekt mit folgender Struktur:

\begin{verbatim}
{
  "row_id": 42,
  "cls": "signal",
  "anchor_text": "AS102",
  "coord_text": "18.1606(GL2)",
  "coord_value": 18.1606,
  "x": 1024.5,
  "y": 768.3,
  "obb_points": [[x1,y1], [x2,y2], [x3,y3], [x4,y4]],
  "conf": 0.94,
  "fahrtrichtung": "A",
  ...
}
\end{verbatim}

\textbf{Vorteile des JSONB-Ansatzes:}
\begin{itemize}
    \item \textbf{Schema-Flexibilität:} Neue Attribute können ohne Datenbankmigrationen hinzugefügt werden
    \item \textbf{Atomare Updates:} Gesamter Workspace wird als Einheit gespeichert, konsistent mit UI-Operationen
    \item \textbf{Effiziente Queries:} PostgreSQL JSONB unterstützt Indexierung und performante Abfragen
    \item \textbf{Versionierung:} Einfaches Speichern vollständiger Snapshots zu verschiedenen Zeitpunkten
\end{itemize}



\subsubsection{Verwendungszwecke}

Die Datenbank erfüllt folgende funktionale Anforderungen im operativen Workflow:

\begin{itemize}
    \item \textbf{Session-Persistierung:} Speicherung von Workspace-Zuständen ermöglicht Unterbrechung und spätere Fortsetzung der Arbeit
    
    \item \textbf{Validierungs-Tracking:} Die Tabelle \texttt{validation\_log} dokumentiert alle identifizierten Datenqualitätsprobleme mit Schweregradklassifikation (ERROR, WARNING, INFO), was eine systematische Fehleranalyse ermöglicht
    
    \item \textbf{Korrektur-Historisierung:} Manuelle Benutzerkorrekturen werden in \texttt{manual\_corrections} protokolliert. Diese Daten können zur Identifikation systematischer Extraktionsfehler und zur Verbesserung der Pipeline genutzt werden
    
    \item \textbf{Qualitätsmetriken:} Aggregierte Qualitätskennzahlen (z.B. durchschnittliche OCR-Konfidenz, Vollständigkeitsraten) werden in \texttt{quality\_metrics} persistiert für longitudinale Analysen
    
    \item \textbf{Export-Vorbereitung:} Vor dem Excel-Export werden Daten aus der Datenbank geladen, validiert und transformiert
\end{itemize}

\subsubsection{Datenbankzugriff}

Die Datenbankinteraktion erfolgt über den PostgreSQL-Adapter \texttt{psycopg2} (Version 2.9.7). Die Datenbankoperationen sind in einem dedizierten Modul \texttt{database.py} gekapselt, welches folgende Kern-API bereitstellt:

\textbf{Workspace-Operationen:}
\begin{itemize}
    \item $\texttt{save\_workspace\_data}(name, data, skeleton, dims) \rightarrow \text{void}$ \\
    Persistiert Workspace mit Extraktionsdaten, optionalem Gleisskelett und Bildabmessungen
    
    \item $\texttt{get\_workspace\_data}(name) \rightarrow (data, skeleton, dims)$ \\
    Lädt gespeicherten Workspace inkl. Dekompression des Gleisskeletts
\end{itemize}

\textbf{Validierungs-Operationen:}
\begin{itemize}
    \item $\texttt{save\_validation\_results}(name, results) \rightarrow \text{void}$ \\
    Speichert Validierungsergebnisse mit automatischer Historisierung
    
    \item $\texttt{get\_validation\_summary}(name) \rightarrow \text{dict}$ \\
    Liefert Zusammenfassung nach Schweregrad aggregiert
\end{itemize}

\textbf{Korrektur-Operationen:}
\begin{itemize}
    \item $\texttt{log\_manual\_correction}(name, row, field, old, new) \rightarrow \text{void}$ \\
    Protokolliert Benutzerkorrektur für spätere Analyse
    
    \item $\texttt{get\_correction\_statistics}(name) \rightarrow \text{list}$ \\
    Analysiert Korrekturmuster zur Identifikation systematischer Fehler
\end{itemize}

Die Verwendung von Connection-Pooling mittels Context Manager optimiert die Performance bei häufigen Datenbankzugriffen während des Extraktionsprozesses und stellt automatisches Rollback bei Fehlern sicher.

\section{Benutzeroberfläche}
\label{sec:benutzeroberfläche}
Die Benutzeroberfläche dient als interaktive Komponente zur Qualitätskontrolle und 
manuellen Nachbearbeitung der extrahierten Daten. Sie adressiert die Anforderungen 
\textbf{FA-013} (GUI ohne CLI-Kenntnisse), \textbf{FA-012} (visuelle Validierung 
durch Bounding-Box-Overlays) und \textbf{NFA-001} (On-Premise-Verarbeitung als 
Desktop-Anwendung).

\subsection{Architektur und Designentscheidungen}

Die Architektur der Benutzeroberfläche basiert auf dem Model-View-Controller (MVC) Entwurfsmuster, wobei die Datenhaltung (Model), die Visualisierung (View) und die Ereignisverarbeitung (Controller) klar voneinander getrennt sind. Diese Architektur gewährleistet Wartbarkeit und ermöglicht die unabhängige Weiterentwicklung der einzelnen Schichten.

Die Hauptkomponenten gliedern sich in:
\begin{itemize}
    \item \textbf{Hauptfenster}: Verwaltung mehrerer parallel geöffneter PDF-Dokumente in Tab-Ansicht
    \item \textbf{Arbeitsbereich}: Dedizierter Anzeigebereich für ein einzelnes PDF-Dokument mit allen zugehörigen Extraktionsergebnissen
    \item \textbf{Interaktive PDF-Ansicht}: Visualisierung des Originaldokuments mit überlagerten Erkennungsergebnissen
    \item \textbf{Hierarchische Ergebnistabelle}: Strukturierte Darstellung aller extrahierten Daten mit Excel-ähnlicher Editierfunktionalität
    \item \textbf{Validierungs-Dialog}: Spezialisierte Komponente zur systematischen Fehlerprüfung und -korrektur
    \item \textbf{Versionsvergleichs-Modul}: Werkzeug zur Gegenüberstellung verschiedener Planversionen
\end{itemize}

Die technologische Basis bildet ein cross-platform GUI-Framework mit nativer Desktop-Integration, leistungsfähigen PDF-Rendering-Fähigkeiten sowie einer umfangreichen Widget-Bibliothek für komplexe Benutzerinteraktionen.

\subsection{PDF-Viewer mit Overlay-System}

\subsubsection{Rendering-Pipeline}

Das PDF-Rendering erfolgt in zwei Schritten: Zunächst wird jede PDF-Seite bei hoher Auflösung (500 DPI) in ein Raster-Bild konvertiert. Die Anzeigebreite $w_{\text{display}}$ berechnet sich aus der Seitenbreite $w_{\text{page}}$, der Auflösung $r$ und dem Zoomfaktor $z$ gemäß:

\begin{equation}
w_{\text{display}} = w_{\text{page}} \times \frac{r}{72} \times z
\end{equation}

wobei 72 die Standard-DPI-Referenz für PDF-Dokumente darstellt. Der Zoomfaktor $z$ ist im Bereich $[0.25, 4.0]$ steuerbar, was eine 16-fache Vergrößerungsspanne ermöglicht. Zur Performance-Optimierung wird ein Viewport-basiertes Culling implementiert (Abbildung~\ref{fig:viewport_culling}), das die Anzahl zu rendernder Overlays auf die im aktuellen Sichtbereich befindlichen Objekte reduziert. Dies verbessert die Darstellungsgeschwindigkeit bei großformatigen Plänen erheblich.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]

% Full document (gray background)
\fill[gray!10] (0,0) rectangle (12,8);
\draw[thick] (0,0) rectangle (12,8);
\node[above] at (6,8) {\textbf{Gesamtdokument} (z.B. 300 Objekte)};

% Viewport (blue outline)
\draw[blue, ultra thick, dashed] (3,2) rectangle (9,6);
\node[blue, above] at (6,6.2) {\textbf{Viewport} (sichtbarer Bereich)};

% Objects outside viewport (red, small)
\foreach \x/\y in {1/1, 2/7, 10/1, 11/6, 1/4, 10/7} {
    \draw[red, thick] (\x,\y) rectangle ++(0.5,0.3);
}
\node[red, right] at (8.9,4) {Nicht gerendert};

% Objects inside viewport (green, highlighted)
\foreach \x/\y in {4/3, 5/4.5, 6/2.5, 7.5/5, 8/3.5} {
    \draw[green!70!black, ultra thick, fill=green!20] (\x,\y) rectangle ++(0.6,0.4);
}
\node[green!70!black, right] at (4.5,5.5) {Gerendert};

% Annotation
\node[below, text width=10cm, align=center, font=\footnotesize] at (6,-0.5) {
    Nur Objekte mit $B_i \cap V \neq \emptyset$ werden gezeichnet\\
    Typische Reduktion: 90\% bei großen Dokumenten
};

\end{tikzpicture}
\caption{Viewport-basiertes Culling zur Performance-Optimierung}
\label{fig:viewport_culling}
\end{figure}
\subsubsection{Overlay-Darstellung}

Über dem gerenderten PDF wird eine transparente Grafikschicht gelegt, auf der die erkannten Bounding Boxes (OBBs) als Polygone visualisiert werden. Jede Objektklasse erhält eine distinkte Farbcodierung zur unmittelbaren visuellen Unterscheidbarkeit:

\begin{itemize}
    \item Signal: Rot (\texttt{RGB}(255, 0, 0))
    \item GKS: Blau (\texttt{RGB}(0, 0, 255))
    \item Weiche: Grün (\texttt{RGB}(0, 255, 0))
    \item Koordinate: Magenta (\texttt{RGB}(255, 0, 255))
\end{itemize}

\subsection{Interaktive Ergebnistabelle}

\subsubsection{Hierarchische Datenstruktur}

Die Ergebnisdarstellung erfolgt in einer Baumstruktur mit zweistufiger Hierarchie: Auf der ersten Ebene werden Elemente nach ihrer Klasse gruppiert, auf der zweiten Ebene befinden sich die individuellen Detektionen. Diese Strukturierung ermöglicht sowohl einen schnellen Überblick über die Klassenverteilung als auch detaillierte Einsicht in einzelne Erkennungen. Die Tabellenspalten umfassen:

\begin{enumerate}
    \item \textbf{Klasse}: Objekttyp (Signal, Weiche, etc.)
    \item \textbf{Text}: Extrahierter OCR-Text (Ankertext oder Koordinatentext)
    \item \textbf{Koordinate}: Zugeordneter Koordinatenwert
    \item \textbf{Konfidenz}: YOLO-Detektionskonfidenz $\in [0, 1]$
    \item \textbf{Seite}: Seitennummer im PDF-Dokument
    \item \textbf{Position}: Pixelkoordinaten $(x, y)$
\end{enumerate}

\subsubsection{Sortierung und Filterung}

Die Sortierung erfolgt spaltenweise über einen Vergleichsoperator, der numerische und lexikographische Ordnung unterscheidet. Für eine Spalte $c$ und zwei Zeilen $i, j$ mit Werten $v_i^c, v_j^c$ gilt:

\begin{equation}
v_i^c \prec v_j^c \iff 
\begin{cases}
v_i^c < v_j^c & \text{falls } v_i^c, v_j^c \in \mathbb{R} \\
\text{lexord}(v_i^c, v_j^c) & \text{sonst}
\end{cases}
\end{equation}

Die Filterung basiert auf regulären Ausdrücken. Gegeben sei ein Regex-Muster $p$ und ein Zellenwert $v$, so wird die Zeile angezeigt, wenn:

\begin{equation}
\exists c \in \{\text{Spalten}\} : p \text{ matched } v_c
\end{equation}

\subsubsection{Bidirektionale Verlinkung}

Bei Auswahl einer Tabellenzeile wird die entsprechende Bounding Box in der PDF-Ansicht hervorgehoben. Die Navigation erfolgt durch Zentrierung des Viewports auf die Objektposition $(x_c, y_c)$ mit einem Hervorhebungsradius $r_{\text{highlight}} = 50$ Pixel. Die Hervorhebung wird als gelber Kreis mit zeitlich begrenzter Sichtbarkeit ($t_{\text{highlight}} = 0.5$ Sekunden) dargestellt.

Die inverse Verlinkung (PDF → Tabelle) erfolgt über einen eindeutigen Zeilen-Identifier, der sowohl im Tabellenelement als auch in der grafischen Repräsentation gespeichert wird. Bei einem Klick auf eine Bounding Box wird die zugehörige Tabellenzeile automatisch selektiert und in den sichtbaren Bereich gescrollt.
Abbildung~\ref{fig:complete_ui} zeigt die vollständige Benutzeroberfläche mit allen beschriebenen Komponenten in integrierter Darstellung.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{images/Kapitel6/UI_linkingv1.png}
\caption{Hauptfenster mit PDF-Viewer (links), hierarchischer Ergebnistabelle (rechts) und bidirektionaler Navigation}
\label{fig:complete_ui}
\end{figure}

Die Abbildung demonstriert die praktische Anwendung der bidirektionalen Verlinkung: Das in der Tabelle selektierte Objekt (GKS 0553 mit Koordinate 15{,}2564, hervorgehoben in cyan) ist im PDF-Viewer durch eine grüne Bounding Box markiert und zentriert. Die farbliche Codierung der Overlays folgt dem definierten Schema (Das ausgewählte Element muss rot und die andere Elemente müssen grün sein). Die Toolbar am oberen Rand bietet Funktionen zur Datenvalidierung, Erkennungsqualitätsprüfung, manuellen Koordinatenverknüpfung und OCR-Re-Extraktion. Das rechte Panel zeigt Detailinformationen zum selektierten Objekt, einschließlich Klassentyp (gks\_gesteuert), YOLO-Konfidenz (0{,}947), extrahiertem Text und Seitenzuordnung. Diese integrierte Ansicht erfüllt die Anforderungen FA-012 (Visuelle Validierung) und FA-013 (GUI) und ermöglicht eine effiziente Qualitätsprüfung ohne Medienbruch zwischen Originaldokument und strukturierten Daten.
\subsection{Validierungs-Dialog}

Der Validierungs-Dialog implementiert eine mehrstufige Fehlerklassifikation basierend auf den Validierungsregeln aus Abschnitt [Referenz zu Validierungskapitel]. Die Fehlertypen umfassen:

\begin{itemize}
    \item \textbf{Regex-Mismatch}: OCR-Text entspricht nicht dem erwarteten Format für die jeweilige Klasse
    \item \textbf{Niedrige Konfidenz}: YOLO-Konfidenz $< \tau_{\text{conf}}$ (Standard: $\tau_{\text{conf}} = 0.6$)
    \item \textbf{Fehlende Verknüpfung}: Ankerelement ohne zugeordnete Koordinate
    \item \textbf{Duplikate}: Mehrfachvorkommen identischer Element-IDs
\end{itemize}

\subsubsection{Interaktionsmöglichkeiten}

Für jeden identifizierten Fehler stehen folgende Korrekturoptionen zur Verfügung:

\begin{enumerate}
    \item \textbf{Manuelle Korrektur}: Direkte Inline-Bearbeitung des Zellenwerts
    \item \textbf{erneuter OCR Versuch}: Erneute OCR-Ausführung mit alternativen Parametern (horizontal/angular)
    \item \textbf{Löschen}: Entfernung des fehlerhaften Eintrags
    \item \textbf{Bestätigung}: Akzeptanz trotz Warnung (z.B. bei bekannten Sonderfällen)
\end{enumerate}

\subsubsection{Transaktionsmodell}

Änderungen werden zunächst in einer temporären Warteschlange gespeichert und erst bei Dialogbestätigung atomar in das Haupt-DataFrame übertragen. Dies gewährleistet Datenkonsistenz und ermöglicht vollständiges Rollback bei Abbruch. Die Änderungshistorie folgt dem Muster:

\begin{equation}
\Delta_{\text{validation}} = \{(r_i, f_i, v_{\text{old},i}, v_{\text{new},i}) \mid i = 1, \ldots, n\}
\end{equation}

wobei $r_i$ die Zeilen-ID, $f_i$ das Feld, $v_{\text{old},i}$ den alten und $v_{\text{new},i}$ den neuen Wert bezeichnet.

\subsection{Versionsvergleichs-Ansicht}

\subsubsection{Side-by-Side Darstellung}

Der Versionsvergleich visualisiert zwei Gleispläne (Version A und Version B) nebeneinander in einem geteilten Anzeigebereich. Jede Version erhält ein eigenständiges PDF-Rendering mit korrespondierenden Overlays. Die Aufteilung des Anzeigebereichs ist dynamisch anpassbar, sodass der Benutzer die Größenverhältnisse nach Bedarf verändern kann.

Die Änderungskategorien werden farbcodiert:

\begin{itemize}
    \item \textbf{Hinzugefügt} (Grün): Element existiert nur in Version B
    \item \textbf{Entfernt} (Rot): Element existiert nur in Version A
    \item \textbf{Verschoben} (Gelb): Position weicht um $\geq \delta_{\text{pos}}$ Pixel ab (Standard: $\delta_{\text{pos}} = 50$)
    \item \textbf{Modifiziert} (Orange): Textänderung bei identischer Position
\end{itemize}

Die Positionsabweichung $d$ zwischen zwei Elementen mit Zentren $(x_A, y_A)$ und $(x_B, y_B)$ wird als euklidische Distanz berechnet:

\begin{equation}
d = \sqrt{(x_B - x_A)^2 + (y_B - y_A)^2}
\end{equation}

Ein Element gilt als verschoben, wenn $d \geq \delta_{\text{pos}}$ bei identischer Objekt-ID.

\subsubsection{Synchronisierte Navigation}

Die Scrollbars beider PDF-Ansichten sind gekoppelt, sodass ein Scrollen in einem Viewer automatisch den anderen Viewer mitbewegt. Die Synchronisation erfolgt über Signal-Slot-Verbindungen:

\begin{equation}
s_B = s_A \cdot \frac{h_B}{h_A}
\end{equation}

wobei $s_A$ die Scrollposition in Viewer A, $h_A$ dessen Gesamthöhe und entsprechend für Viewer B. Dies gewährleistet proportionale Navigation bei unterschiedlichen Seitengrößen.
\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/Kapitel6/diff_dialog2.png}\caption{Änderungsvergleich-Dialog}
\label{fig:diff_dialog}
\end{figure}

Abbildung~\ref{fig:diff_dialog} zeigt den Dialog zum Vergleich zweier Planversionen. 
Nach Auswahl der zu vergleichenden PDFs (PDF~1 als Referenz, PDF~2 als aktualisierte 
Version) analysiert das System automatisch alle Unterschiede. Die Zusammenfassung 
kategorisiert die Änderungen in fünf Typen: \textit{Hinzugefügt} (neue Elemente in 
PDF~2), \textit{Gelöscht} (fehlende Elemente in PDF~2), \textit{Verschoben} (gleiche 
Kennung, unterschiedliche km-Position), \textit{Modifiziert} (geänderte Feldwerte) 
und \textit{Unverändert}. Im dargestellten Beispiel wurden zwischen zwei Versionen 
des Plans SgSl0603 insgesamt 41 Änderungen erkannt: 21 hinzugefügte, 15 gelöschte 
und 5 verschobene Elemente bei 224 unveränderten Objekten. Die einzelnen Tabs 
ermöglichen die detaillierte Inspektion jeder Änderungskategorie.
\subsection{Export-Funktionalität}
\label{subsec:exportfunktionlität}
\subsubsection{Excel-Export}

Der Excel-Export erzeugt strukturierte Arbeitsblätter mit automatischer Spaltenformatierung. Jede Objektklasse erhält ein eigenes Tabellenblatt mit klassenspezifischen Spalten. Die Farbcodierung übernimmt die UI-Farbschemata zur konsistenten visuellen Darstellung:

\begin{itemize}
    \item Signale: Roter Zeilenhintergrund (\texttt{RGB}(255, 200, 200))
    \item Weichen: Grüner Zeilenhintergrund (\texttt{RGB}(200, 255, 200))
    \item Koordinaten: Blauer Zeilenhintergrund (\texttt{RGB}(200, 200, 255))
\end{itemize}

Zusätzlich wird ein \textit{Metadata}-Sheet erzeugt, das Verarbeitungsmetadaten enthält:

\begin{itemize}
    \item Verarbeitungsdatum und -zeitpunkt
    \item YOLO-Modellversion und -pfad
    \item Konfidenzstatistiken: $\mu_{\text{conf}}, \sigma_{\text{conf}}, \min(\text{conf}), \max(\text{conf})$
    \item Anzahl Detektionen pro Klasse
\end{itemize}
\subsubsection{Erweiterter Export-Dialog}

Für die praktische Anwendung wurde ein umfassender Export-Dialog implementiert 
(Abbildung~\ref{fig:export_dialog}), der über die Grundfunktionalität hinausgeht 
und die Anforderungen FA-009 (Excel-Integration) sowie FA-010 (Strukturerhalt) 
adressiert. Die wichtigsten Funktionen umfassen:

\begin{itemize}
    \item \textbf{Klassenbasierte Auswahl:} Hierarchische Baumansicht aller 
    Objektklassen mit Anzahl der Instanzen; selektiver Export einzelner Klassen
    \item \textbf{Live-Vorschau:} Tabellarische Vorschau der zu exportierenden 
    Daten mit Möglichkeit zum Ausschließen einzelner Zeilen
    \item \textbf{Zieloptionen:} Export in neue Datei oder Anhängen an bestehende 
    Excel-Datei mit wählbarer Startposition (Blatt, Spalte, Zeile)
    \item \textbf{Formatierung:} Optionen für Überschriften, Spaltenbreiten, 
    fixierte Kopfzeile, Zahlenformat und Leerzellenbehandlung
    \item \textbf{Gruppierung:} Export aller Klassen in ein Blatt, als Sektionen, 
    oder auf separate Arbeitsblätter (vgl. Abbildung~\ref{fig:export_result})
\end{itemize}

Die ASCII-Vorschau der Zielposition (rechts unten in Abbildung~\ref{fig:export_dialog}) 
visualisiert, wo die Daten in einer bestehenden Datei eingefügt werden, und 
verhindert versehentliches Überschreiben vorhandener Inhalte.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{images/Kapitel6/export_dialog.png}
\caption{Export-Dialog mit klassenbasierter Auswahl, Live-Vorschau und 
Zielpositionierung für bestehende Dateien}
\label{fig:export_dialog}
\end{figure}

Abbildung~\ref{fig:export_result} zeigt 
ein Beispiel der exportierten Daten mit separaten Arbeitsblättern pro Objektklasse. 
Die resultierenden Excel-Dateien dienten als Grundlage für den Ground-Truth-Vergleich 
der E2E-Evaluation (vgl. Tabelle~\ref{tab:e2e_per_class_test}).

Die Option \enquote{Bestehende Datei} (Abbildung~\ref{fig:export_dialog}) ermöglicht 
das Einfügen von Daten in existierende Excel-Vorlagen ab einer wählbaren Startposition, 
ohne bestehende Formatierungen, Formeln oder Makros zu überschreiben. Diese 
Funktionalität erfüllt die Anforderung FA-010 (Strukturerhalt).

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{images/Kapitel6/export_screenshot.png}
\caption{Exportierte Excel-Datei mit separaten Arbeitsblättern pro Objektklasse 
(sichtbar am unteren Rand: gks\_festkodiert, gks\_gesteuert, gm\_block, etc.)}
\label{fig:export_result}
\end{figure}

\subsubsection{JSON-Export}

Der JSON-Export folgt einem hierarchischen Schema mit vollständigen Geometriedaten:

\begin{verbatim}
{
  "elements": [
    {
      "id": <eindeutige ID>,
      "class": <Objektklasse>,
      "text": <OCR-Text>,
      "Konfidenz": <float in [0,1]>,
      "obb_points": [[x1,y1], [x2,y2], [x3,y3], [x4,y4]],
      "page": <Seitennummer>,
      "linked_elements": [<IDs verknüpfter Objekte>],
      "ocr_engine": <verwendete OCR-Engine>,
      "linking_method": <Verknüpfungsmethode>
    }
  ]
}
\end{verbatim}

Diese Struktur ermöglicht die vollständige Rekonstruktion der Extraktionsergebnisse und eignet sich für Weiterverarbeitung in anderen Systemen.
\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/Kapitel6/export_dialog_json.png}
\caption{Export-Dialog mit Formatauswahl. Das Dropdown-Menü (rechts) bietet 
drei Ausgabeformate: Excel (.xlsx), CSV (.csv) und JSON (.json). }
\label{fig:export_format_selection}
\end{figure}

Gemäß Anforderung \textbf{NFA-012} unterstützt das System drei Ausgabeformate:

\begin{itemize}
    \item \textbf{Excel (.xlsx):} Primärformat für Engineering-Workflows mit 
    Unterstützung für Formatierung, mehrere Arbeitsblätter und Formelerhalt
    \item \textbf{CSV (.csv):} Textbasiertes Format für einfachen Datenaustausch 
    und Import in Drittsysteme
    \item \textbf{JSON (.json):} Maschinenlesbares Format für API-Anbindung; 
    die Struktur ist konfigurierbar (Records als Liste von Objekten oder 
    spaltenbasiertes Array)
\end{itemize}

Die Formatauswahl erfolgt über ein Dropdown-Menü im Export-Dialog 
(Abbildung~\ref{fig:export_format_selection}).
\subsubsection{Export des Differenzberichts}

Die erkannten Änderungen können über die Schaltfläche \textit{Exportieren} in eine 
Excel-Datei ausgegeben werden (Abbildung~\ref{fig:diff_export}). Die exportierte 
Arbeitsmappe enthält separate Arbeitsblätter für jede Änderungskategorie 
(Zusammenfassung, Hinzugefügt, Gelöscht, Verschoben), wodurch eine strukturierte 
Dokumentation der Planänderungen für Revisionszwecke ermöglicht wird. Bei 
verschobenen Elementen werden sowohl der alte als auch der neue km-Wert sowie 
die Differenz in Metern angegeben.
\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/Kapitel6/diff_export.png}
\caption{Änderungsexport}
\label{fig:diff_export}
\end{figure}


\section{Zusammenfassung}

Dieses Kapitel stellte die technische Realisierung der automatisierten Datenextraktionspipeline für Gleispläne vor. Die Implementierung umfasst drei zentrale Verarbeitungsstufen sowie unterstützende Komponenten für Qualitätssicherung, Persistenz und Benutzerinteraktion.

\subsection{Kernkomponenten der Pipeline}

Die Extraktionspipeline besteht aus folgenden Hauptmodulen:

\textbf{Objekterkennung (Abschnitt \ref{sec:objekterkennungmityolov8obb}):}
Das YOLOv8-OBB Modell wurde auf einem Datensatz mit 13 Symbolklassen trainiert, 
wobei fünf \textit{Kernklassen} (signal, coordinate, gks\_festkodiert, gks\_gesteuert, 
gm\_block) den Fokus der Evaluation bilden und acht \textit{Auxiliarklassen} die 
Erweiterbarkeit des Systems demonstrieren (vgl. Anforderungen FA-003 und NFA-008).

\textbf{OCR-Pipeline (Abschnitt \ref{sec:ocrpipeline}):}
Die orientierungsadaptive OCR-Pipeline bildet das Herzstück der Textextraktion. Durch die Kaskadierung dreier OCR-Engines (PaddleOCR, Tesseract, EasyOCR) wird eine hohe Erkennungsrobustheit erreicht. Das Dual-Winkel-Routing-System behandelt sowohl achsenparallele als auch beliebig rotierte Texte mittels unterschiedlicher Transformationsstrategien. Klassenspezifische Vorverarbeitungspipelines passen die Bildaufbereitung an die jeweiligen Charakteristika der Symbole an (z.B. adaptive Padding für Signale, morphologische Filterung für GKS-Platten). Die Validierung erfolgt durch regex-basierte Pattern-Matching-Verfahren mit konfidenzgewichteten Scoring-Mechanismen.

\textbf{Symbol-Text-Verknüpfung (Abschnitt \ref{sec:intelligentesymboltextverknüpfung}):}
Die intelligente Linking-Komponente ordnet extrahierte Texte ihren zugehörigen Symbolen zu. Die rotationsinvariante Koordinatentransformation ermöglicht eine geometrisch konsistente Richtungsbeziehung unabhängig von der Symbolorientierung. Der proximity-basierte Algorithmus nutzt klassenspezifische Suchradien und berücksichtigt erwartete räumliche Relationen. Ein adaptiver Lernmechanismus erfasst wiederkehrende Layoutmuster und optimiert die Verknüpfung durch probabilistische Fenstersuche. Speziallogiken behandeln komplexe Fälle wie die geometrische Ableitung der Fahrtrichtung aus der GKS-Signal-Relation oder die Gruppierung von Haltepunkt-Tripeln.

\subsection{Unterstützende Systeme}

\textbf{Validierung und Qualitätssicherung (Abschnitt \ref{sec:validierungundsicherung}):}
Ein mehrstufiges Validierungsframework prüft extrahierte Daten syntaktisch (regex-basiert), semantisch (Plausibilitätsbereiche) und referentiell (Vollständigkeit von Relationen). Automatische Korrekturen behandeln häufige OCR-Fehler, während fehlerhafte Einträge zur manuellen Überprüfung gekennzeichnet werden. Die Integration mit der UI ermöglicht einen interaktiven Validierungsworkflow.

\textbf{Versionsvergleich und Rückverfolgbarkeit (Abschnitt \ref{sec:unterstützendekomponente}):}
Die Versionsvergleichskomponente nutzt UID-basierte Identifikation zur Detektion von Änderungen, Hinzufügungen und Entfernungen zwischen Planrevisionen. Mengenoperationen auf den extrahierten Objekten ermöglichen eine effiziente Differenzberechnung. Das Rückverfolgbarkeitssystem persistiert für jedes extrahierte Element die Quellkoordinaten und PDF-Metadaten, wodurch eine bidirektionale Navigation zwischen Datenansicht und Plandarstellung ermöglicht wird.

\textbf{Benutzeroberfläche (Abschnitt \ref{sec:benutzeroberfläche}):}
Die PyQt5-basierte Benutzeroberfläche integriert die Pipeline-Komponenten in einem interaktiven Workflow. Der PDF-Viewer visualisiert Detektionen mittels farbcodierter Overlays und unterstützt Click-to-Highlight-Navigation. Die Ergebnistabelle bietet Sortier- und Filterfunktionen sowie Inline-Editing für Korrekturen. Der Validierungsdialog strukturiert fehlerhafte Einträge nach Fehlertyp und ermöglicht Batch-Operationen. Die Vergleichsansicht stellt Änderungen zwischen Planversionen side-by-side dar.

\subsection{Implementierungsumfang}

Die Gesamtimplementierung umfasst folgende Modulgrößen:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Modul} & \textbf{Umfang (LOC)} \\
\hline
Objekterkennung & \textasciitilde2.000 \\
OCR-Pipeline & \textasciitilde3.500 \\
Linking-Algorithmen & \textasciitilde1.800 \\
Validierung \& Export & \textasciitilde900 \\
Versionsvergleich & \textasciitilde650 \\
Rückverfolgbarkeit & \textasciitilde400 \\
UI-Komponenten & \textasciitilde2.100 \\
\hline
\textbf{Gesamt} & \textbf{\textasciitilde11.350} \\
\hline
\end{tabular}
\caption{Codeumfang der Implementierungsmodule (gerundet)}
\label{tab:implementation_scope}
\end{table}

Die Architektur folgt einer modularen Struktur, die eine klare Trennung der Verarbeitungsstufen gewährleistet. Zentrale Design-Entscheidungen (vgl. Kapitel \ref{chap:konzeption}) wurden konsequent umgesetzt:

\begin{itemize}
    \item \textbf{Kaskadierte Fehlerbehandlung}: Jede Verarbeitungsstufe implementiert Fallback-Mechanismen zur Behandlung von Randfällen.
    \item \textbf{Klassenspezifische Parametrisierung}: Schwellenwerte, Suchradien und Preprocessing-Parameter sind pro Symbolklasse konfigurierbar.
    \item \textbf{Stateless Processing}: Die Kernpipeline (YOLO $\rightarrow$ OCR $\rightarrow$ Linking) arbeitet zustandslos, wodurch parallele Verarbeitung erleichtert wird.
    \item \textbf{Metadaten-Tracking}: Alle Extraktionsschritte dokumentieren ihre Entscheidungen (verwendete Engine, Konfidenz-Werte, Transformationsparameter) für Transparenz und Debugging.
\end{itemize}

\subsection{Deployment und Ausführung}

Das System wurde als Standalone-Desktop-Anwendung konzipiert, die keine Cloud-Infrastruktur zur Laufzeit benötigt. Dies gewährleistet Datenschutz-Compliance bei der Verarbeitung sensibler Bahninfrastrukturdaten. Die Modellinferenz erfolgt auf lokaler GPU-Hardware (oder CPU-Fallback), während die Datenpersistenz optional über eine lokale PostgreSQL-Instanz realisiert wird.

Die gesamte Verarbeitungszeit für einen typischen Gleisplan variiert je nach Komplexität:

\begin{equation}
T_{total} = T_{YOLO} + \sum_{i=1}^{N} T_{OCR_i} + T_{linking} + T_{validation}
\end{equation}

wobei $N$ die Anzahl der detektierten Objekte bezeichnet und $T_{OCR_i}$ die klassenspezifische OCR-Dauer für Objekt $i$ repräsentiert. Eine detaillierte Performanz-Analyse mit konkreten Zeitmessungen erfolgt in Kapitel \ref{chap:evaluation}.

\subsection{Schnittstellen und Erweiterbarkeit}

Die Implementierung definiert klare Schnittstellen zwischen den Modulen:

\begin{itemize}
    \item \textbf{Detection Interface}: YOLO liefert strukturierte Objekte mit Klasse, OBB-Koordinaten, Rotationswinkel und Konfidenz.
    \item \textbf{OCR Interface}: Die OCR-Komponente akzeptiert Bildausschnitte und Klassenkontexte, liefert Text und Engine-Metadaten.
    \item \textbf{Linking Interface}: Der Linking-Algorithmus konsumiert Detektions- und OCR-Ergebnisse, produziert Assoziationen mit Konfidenz-Scores.
    \item \textbf{Validation Interface}: Das Validierungssystem prüft verknüpfte Objekte gegen konfigurierbare Regelsets.
\end{itemize}

Diese Modularität ermöglicht die zukünftige Integration alternativer Modelle (z.B. neuerer YOLO-Versionen) oder zusätzlicher OCR-Engines ohne Anpassung der nachgelagerten Verarbeitungslogik.

\subsection{Abschließende Bemerkungen}

Die vorgestellte Implementierung realisiert eine vollständige Pipeline von der PDF-Eingabe bis zum strukturierten Datenexport. Die Kombination aus lernbasierten Komponenten (YOLO, neuronale OCR-Engines), algorithmischen Verfahren (Koordinatentransformation, Proximity-Search) und regelbasierten Validierungen schafft ein robustes System für die Extraktion von Bahnanlagendaten aus technischen Zeichnungen.

Die technische Evaluation der implementierten Komponenten, einschließlich Genauigkeitsmetriken, Fehleranalysen und Performanzmessungen, wird im folgenden Kapitel \ref{chap:evaluation} präsentiert.

\chapter{Evaluation}
\label{chap:evaluation}

Dieses Kapitel präsentiert die systematische Evaluation des entwickelten Prototyps. Die Bewertung erfolgt anhand definierter Metriken und validiert die in Kapitel~\ref{chap:anforderungen} spezifizierten funktionalen und nicht-funktionalen Anforderungen. Die Evaluation gliedert sich in die Beschreibung der Testmethodik, die detaillierte Analyse der Einzelkomponenten sowie die Bewertung der Gesamtsystemleistung auf einem unabhängigen Testsatz.

\section{Testmethodik}
\label{sec:testmethodik}

Die Evaluation des Systems erfordert eine sorgfältige Definition der Testbedingungen, um reproduzierbare und aussagekräftige Ergebnisse zu gewährleisten. Dieser Abschnitt beschreibt die verwendeten Testdatensätze, die angewandten Evaluationsmetriken sowie die Testumgebung.

\subsection{Testdatensätze}
\label{subsec:testdatensatz}

Zur systematischen Bewertung wurden zwei hierarchisch strukturierte Datensätze verwendet, die verschiedene Evaluationszwecke erfüllen.

\subsubsection{Validierungssatz (YOLO Technical Validation)}

Der Validierungssatz dient der technischen Bewertung der Objekterkennungskomponente und wurde während des Trainingsprozesses zur Modellselektion verwendet.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Original-Gleispläne (A0-Format) & 25 \\
Anzahl Tiles (2048x2048, überlappend) & 1.154 \\
\hspace{3mm} davon Trainingsdaten & 923 \\
\hspace{3mm} davon Validierungsdaten (initial) & 208 \\
\hspace{3mm} davon finale Validierungsdaten & 115 \\
Annotierte Symbole (Validierungsset) & 1.305 \\
\hline
\multicolumn{2}{|l|}{\textit{Kernklassen (produktionsrelevant)}} \\
\hline
\hspace{3mm} Signale & 218 \\
\hspace{3mm} Koordinaten & 629 \\
\hspace{3mm} GKS-Platten (festkodiert) & 60 \\
\hspace{3mm} GKS-Platten (gesteuert) & 69 \\
\hspace{3mm} GM-Blöcke & 91 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{1.067 (81.8\%)} \\
\hline
\multicolumn{2}{|l|}{\textit{Auxiliarklassen (experimentell)}} \\
\hline
\hspace{3mm} Sonstige Klassen (8 Typen) & 238 (18.2\%) \\
\hline
Objektklassen (gesamt) & 13 \\
\hspace{3mm} davon Kernklassen & 5 \\
\hspace{3mm} davon Auxiliarklassen & 8 \\
Synthetische Augmentation (Training) & 10 Rotationswinkel \\
Gesamtinstanzen (inkl. Augmentation) & 8.029 \\
Tile-Größe & 2048 × 2048 Pixel \\
Auflösungsbereich (Original-PDFs) & 500 DPI \\
\hline
\end{tabular}
\caption{Statistiken des Validierungsdatensatzes mit Unterscheidung zwischen Kern- und Auxiliarklassen}
\label{tab:validation_dataset_stats}
\end{table}

\textbf{Anmerkung zur Datenfilterung:} Von den initial 208 durch YOLOv8s automatische 80/20-Aufteilung zugewiesenen Validierungsbildern wurden 93 Bilder manuell gefiltert, da sie entweder keine relevanten Symbole enthielten (leere Randbereiche nach dem Tiling) oder ausschließlich Hintergrundgeometrie ohne annotierte Objekte darstellten. Dies resultiert in einem finalen Validierungsdatensatz von 115 Bildern mit 1.305 validen Objektinstanzen.

\textbf{Verwendungszweck:} Der Validierungssatz dient primär der Bewertung der YOLO-Detektionsleistung (mAP, Precision, Recall) und bestätigt die erfolgreiche Modellkonvergenz. Diese Daten wurden während des Trainings zur Hyperparameter-Optimierung und Early Stopping verwendet und stellen keine vollständig unabhängige Testmenge dar.

\subsubsection{Testsatz (End-to-End System Evaluation)}

Für die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) wurde ein Testsatz aus realen Siemens Mobility Gleisplänen ausgewählt. 

\textbf{Methodische Einschränkung:} Aufgrund des erheblichen manuellen Aufwands zur Erstellung vollständiger Ground-Truth-Daten für A0-Gleispläne (durchschnittlich 2-3 Stunden pro Plan) sowie der begrenzten Verfügbarkeit weiterer ungesehener Pläne wurden sieben Pläne unterschiedlicher Komplexität für die End-to-End-Evaluation ausgewählt. Diese Pläne repräsentieren verschiedene Komplexitätsstufen und ermöglichen eine realistische Bewertung der Systemleistung. Für eine vollständig unabhängige Evaluation wären zusätzliche, komplett ungesehene Pläne wünschenswert gewesen, was im Zeitrahmen dieser Masterarbeit jedoch nicht realisierbar war.

\textbf{Evaluationsumfang:} Die End-to-End-Evaluation fokussiert sich auf die fünf Kernklassen (\textit{signal}, \textit{coordinate}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die acht zusätzlich implementierten Auxiliarklassen dienten primär der Demonstration der Systemerweiterbarkeit und werden nicht detailliert evaluiert.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Attribut} & \textbf{Wert} \\
\hline
Anzahl Gleispläne & 7 \\
Seitengröße & A0 (841 × 1189 mm) \\
Seiten pro Plan & 1 \\
Herkunft & Siemens Mobility Projekte \\
Komplexitätsstufen & Einfach (2), Mittel (3), Komplex (2) \\
\hline
\multicolumn{2}{|l|}{\textit{Durchschnittliche Symbolanzahl pro Plan}} \\
\hline
\hspace{3mm} Signale & 25 \\
\hspace{3mm} GKS-Platten (beide Typen) & 37 \\
\hspace{3mm} GM-Blöcke & 31 \\
\hspace{3mm} \textbf{Summe Kernklassen} & \textbf{92} \\
Gesamtanzahl evaluierter Objekte & 644 \\
Tiles pro Plan (Durchschnitt) & $\approx$ 40 \\
Auflösungsbereich & 500 DPI \\
\hline
\end{tabular}
\caption{Charakteristika des Testdatensatzes (A0-Gleispläne, nur Kernklassen)}
\label{tab:test_dataset_stats}
\end{table}

\textbf{Ground-Truth-Erstellung:} Für jeden Testplan wurde eine manuelle Referenzdatei erstellt, die alle relevanten Extraktionsziele enthält:

\begin{itemize}
    \item \textbf{Signale}: Bezeichnung (z.B. ``A102''), zugehörige Kilometrierung, Fahrtrichtung (A/B)
    \item \textbf{GKS-Platten}: Nummer (z.B. ``1234''), zugehörige Kilometrierung
    \item \textbf{Koordinatenangaben}: Kilometerwert (z.B. ``18.1606''), optionale Gleisangabe
    \item \textbf{Verknüpfungen}: Erwartete Assoziationen zwischen Symbolen und Texten
\end{itemize}

Die Ground-Truth-Daten wurden in strukturierten Excel-Dateien gespeichert, um einen direkten Vergleich mit den Systemausgaben zu ermöglichen.

\textbf{Verwendungszweck:} Der Testsatz dient der vollständigen End-to-End-Evaluation aller Pipeline-Komponenten und misst die tatsächliche Systemleistung auf realen Daten aus dem Siemens Mobility Umfeld.

\textit{Hinweis: Aus Vertraulichkeitsgründen (Sperrvermerk) werden keine spezifischen Projektbezeichnungen oder Visualisierungen der Originalpläne präsentiert. Die Ergebnisse werden in aggregierter Form berichtet.}

\subsubsection{Komplexitätskategorisierung}

Um die Robustheit des Systems unter verschiedenen Bedingungen zu evaluieren, wurden die Testpläne in drei Komplexitätskategorien eingeteilt. Da alle Pläne im A0-Format vorliegen und jeweils eine Seite umfassen, erfolgt die Kategorisierung primär nach Symboldichte und Layoutkomplexität.

\begin{table}[H]
\centering
\begin{tabular}{|l|p{8cm}|r|}
\hline
\textbf{Kategorie} & \textbf{Charakteristik} & \textbf{Anzahl} \\
\hline
Einfach & Niedrige Symboldichte ($<$ 60 Symbole), klare räumliche Trennung, typisch für Streckenabschnitte & 2 \\
\hline
Mittel & Moderate Symboldichte (60--100 Symbole), gelegentliche Überlappungen, typisch für kleinere Bahnhöfe & 2 \\
\hline
Komplex & Hohe Symboldichte ($>$ 100 Symbole), viele überlappende Elemente, dichte Weichenbereiche, typisch für große Bahnhofsköpfe & 3 \\
\hline
\end{tabular}
\caption{Komplexitätskategorien der A0-Testpläne}
\label{tab:complexity_categories}
\end{table}

Diese Kategorisierung ermöglicht eine differenzierte Analyse der Systemleistung in Abhängigkeit von der Plankomplexität und identifiziert kritische Schwellenwerte für Symboldichte und räumliche Überlappung.


\subsection{Evaluationsmetriken}
\label{subsec:evaluationsmetriken}

Die Bewertung des Systems erfolgt auf mehreren Ebenen mit jeweils spezifischen Metriken. Die verwendeten Metriken basieren auf den in Kapitel~\ref{chap:theoretischeundtechnischegrundlagen} eingeführten Standardverfahren für Objekterkennung (Abschnitt~\ref{subsec:theoretischeevaluationsmetriken}) und OCR-Systeme (Abschnitt~\ref{subsec:ocr_metriken}). Dieser Abschnitt fasst die angewandten Metriken kurz zusammen und definiert die spezifische End-to-End Systemmetrik.

\subsubsection{Metriken für die Objekterkennung}

Für die Bewertung der YOLO-basierten Objekterkennung werden die in Abschnitt~\ref{subsec:evaluationsmetriken} definierten Standardmetriken verwendet:

\begin{itemize}
    \item \textbf{Precision}: Anteil korrekter Detektionen an allen Vorhersagen
    \item \textbf{Recall}: Anteil gefundener Objekte an allen vorhandenen Objekten
    \item \textbf{F1-Score}: Harmonisches Mittel aus Precision und Recall
    \item \textbf{mAP@0.5}: Mean Average Precision bei IoU-Schwelle von 50\%
    \item \textbf{mAP@0.5:0.95}: mAP gemittelt über IoU-Schwellen von 50\% bis 95\%
\end{itemize}

Eine Detektion gilt als \textit{True Positive}, wenn die Intersection over Union (IoU) mit der Ground-Truth-Box $\geq$ 0.5 beträgt und die Klassenvorhersage korrekt ist.

\subsubsection{Metriken für die Texterkennung}

Die OCR-Leistung wird nicht isoliert durch zeichenbasierte Metriken wie die 
Character Error Rate (CER) bewertet, sondern nach dem Prinzip der 
\textit{Feldgenauigkeit} (vgl. Abschnitt~\ref{subsec:ocr_metriken}): Ein 
OCR-Ergebnis gilt als korrekt, wenn der extrahierte Text exakt mit dem 
Ground Truth übereinstimmt. Diese Bewertung ist in die End-to-End-Systemmetrik 
integriert, wodurch folgende Vorteile entstehen:

\begin{itemize}
    \item OCR-Fehler, die durch nachgelagerte Validierung (Regex-Muster) 
    automatisch korrigiert werden, beeinflussen das Endergebnis nicht negativ
    \item Die Metrik entspricht dem tatsächlichen Informationsbedarf: 
    „Wurde der korrekte Wert extrahiert? \enquote{statt} Wie viele Zeichen waren falsch?"
    \item Fehlerquellen können der jeweiligen Pipeline-Stufe zugeordnet werden 
    (YOLO vs. OCR vs. Linking)
\end{itemize}

Zusätzlich wird die \textbf{Regex-Validierungsrate} erfasst, die den Anteil 
der OCR-Ergebnisse quantifiziert, die klassenspezifische Formatmuster erfüllen.


\subsubsection{End-to-End Systemmetrik}

Die Gesamtsystemleistung wird durch die \textbf{End-to-End Accuracy} gemessen, die dem in Anforderung \textbf{NFA-003} definierten Zielwert entspricht:
\begin{equation}
\text{E2E Accuracy} = \frac{\text{Vollständig korrekt extrahierte Objekte}}{\text{Gesamtanzahl Objekte}} \times 100\%
\end{equation}

Ein Objekt gilt als \enquote{vollständig korrekt extrahiert}, wenn alle folgenden Bedingungen erfüllt sind:
\begin{enumerate}
    \item Das Symbol wurde korrekt detektiert (IoU $\geq$ 0.5 mit Ground Truth)
    \item Die Klassifikation ist korrekt
    \item Der OCR-Text stimmt exakt mit dem Ground Truth überein (falls anwendbar)
    \item Alle erforderlichen Verknüpfungen (z.B. zu Koordinaten) sind korrekt (falls anwendbar)
\end{enumerate}

\subsection{Testumgebung}
\label{subsec:testumgebung}

Die Evaluation wurde auf einer standardisierten Hardware- und Softwarekonfiguration durchgeführt, um reproduzierbare Ergebnisse zu gewährleisten. Tabelle~\ref{tab:testumgebung} fasst die technischen Spezifikationen zusammen.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Komponente} & \textbf{Spezifikation} \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Inferenz)}} \\
\hline
CPU & Intel Core i7-10700 (8 Kerne, 2.9 GHz) \\
RAM & 32 GB DDR4 \\
GPU & Keine (CPU-only Inferenz) \\
Speicher & 512 GB SSD \\
\hline
\multicolumn{2}{|c|}{\textit{Hardware (Training)}} \\
\hline
GPU & NVIDIA T4 (AWS g4dn.xlarge) \\
VRAM & 16 GB \\
\hline
\multicolumn{2}{|c|}{\textit{Software}} \\
\hline
Betriebssystem & Windows 10 / Ubuntu 22.04 \\
Python & 3.9.16 \\
PyTorch & 2.0.1 \\
Ultralytics & 8.0.196 \\
PaddleOCR & 2.7.0 \\
Tesseract & 5.3.0 \\
PostgreSQL & 14.9 \\
\hline
\end{tabular}
\caption{Hardware- und Softwarekonfiguration der Testumgebung}
\label{tab:testumgebung}
\end{table}

Die Wahl einer CPU-basierten Inferenzumgebung reflektiert die Anforderung \textbf{NFA-001}, die eine On-Premise-Verarbeitung auf Standard-Workstations ohne dedizierte GPU vorsieht. Alle Zeitmessungen wurden als Mittelwert über drei Durchläufe berechnet, um Varianz durch Systemlast zu minimieren.

\section{Ergebnisanalyse}
\label{sec:ergebnisanalyse}

Dieser Abschnitt präsentiert die quantitativen Evaluationsergebnisse der einzelnen Pipeline-Komponenten sowie des Gesamtsystems. Die Ergebnisse werden im Kontext der in Kapitel~\ref{chap:anforderungen} definierten Anforderungen interpretiert.

\subsection{Objekterkennungsleistung}
\label{subsec:detection_eval}

Die Objekterkennung bildet die fundamentale Stufe der Extraktionspipeline. Die Qualität der YOLO-Detektionen determiniert maßgeblich die erreichbare Gesamtgenauigkeit des Systems.

\subsubsection{Gesamtleistung auf dem Validierungssatz}

Das trainierte YOLOv8l-OBB Modell wurde auf dem Validierungsdatensatz (115 Bilder, 1.305 Instanzen) evaluiert. Tabelle~\ref{tab:detection_overall} zeigt die aggregierten Metriken über alle Symbolklassen.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Metrik} & \textbf{Wert} \\
\hline
Precision (Durchschnitt) & 97.5\% \\
Recall (Durchschnitt) & 95.7\% \\
F1-Score & 96.6\% \\
mAP@0.5 & 98.0\% \\
mAP@0.5:0.95 & 92.2\% \\
\hline
\end{tabular}
\caption{Aggregierte Detektionsmetriken auf dem Validierungsdatensatz}
\label{tab:detection_overall}
\end{table}

Der erreichte Recall von 95.7\% übertrifft die Anforderung \textbf{FA-001} deutlich, die eine Mindesterkennungsrate von 90\% fordert. Die hohe Precision von 97.5\% zeigt, dass das Modell nur wenige Falschdetektionen produziert -- von 100 vorhergesagten Objekten sind durchschnittlich 97-98 korrekt. Der F1-Score von 96.6\% belegt die ausgewogene Leistung zwischen Precision und Recall, was für produktive Anwendungen essentiell ist: Das System findet nahezu alle vorhandenen Objekte (hoher Recall) und produziert dabei nur wenige Fehlalarme (hohe Precision).

Die mAP@0.5 von 98.0\% demonstriert die exzellente Detektionsqualität bei einem IoU-Schwellenwert von 50\%. Dies bedeutet, dass die vorhergesagten Bounding Boxes im Durchschnitt zu mindestens 50\% mit den Ground-Truth-Boxen überlappen, was für nachgelagerte OCR-Verarbeitung ausreichend präzise ist. Die mAP@0.5:0.95 von 92.2\% bestätigt die robuste Leistung auch bei strengeren Überlappungskriterien (IoU von 50\% bis 95\% in 5\%-Schritten gemittelt). Der Abstand von 5.8 Prozentpunkten zwischen mAP@0.5 und mAP@0.5:0.95 ist für orientierte Bounding Boxes typisch und liegt im erwarteten Bereich.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/Kapitel7/BoxPR_curve.png}
    \caption{Precision-Recall-Kurven für alle Objektklassen auf dem Validierungsdatensatz. Die Fläche unter jeder Kurve entspricht dem klassenspezifischen AP-Wert. Der Gesamtwert mAP@0.5 beträgt 98,4\,\%.}
    \label{fig:pr_curve}
\end{figure}

Die Precision-Recall-Kurven in Abbildung~\ref{fig:pr_curve} visualisieren das Verhältnis zwischen Precision und Recall für verschiedene Konfidenzschwellenwerte. Die nahezu rechteckige Form der Kurven für die meisten Klassen bestätigt die hohe Detektionsqualität. Lediglich bei den GKS-Klassen zeigt sich ein geringfügig früherer Precision-Abfall bei hohen Recall-Werten, was auf die in Abschnitt~\ref{subsec:e2e_test_eval} diskutierten Verwechslungen zwischen \texttt{gks\_festkodiert} und \texttt{gks\_gesteuert} zurückzuführen ist.
\subsubsection{Klassenspezifische Analyse}

Die Detektionsleistung variiert zwischen den verschiedenen Symbolklassen moderat. Tabelle~\ref{tab:detection_per_class} zeigt die klassenspezifischen Metriken für die fünf Kernklassen, die für die Extraktion von Planungsdaten relevant sind. Precision und Recall wurden basierend auf den Werten der Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) berechnet, während die mAP@0.5-Werte die durchschnittliche Precision über alle Recall-Stufen repräsentieren.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Klasse} & \textbf{Instanzen (Val)} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} \\
\hline
signal & 218 & 94.9\% & 98.5\% & 97.6\% \\
coordinate & 629 & 97.7\% & 98.6\% & 98.7\% \\
gks\_festkodiert & 60 & 98.3\% & 95.8\% & 97.8\% \\
gks\_gesteuert & 69 & 94.5\% & 97.5\% & 95.1\% \\
gm\_block & 91 & 97.6\% & 97.6\% & 99.5\% \\
\hline
\textbf{Kernklassen (Durchschnitt)} & \textbf{1.067 (81.8\%)} & \textbf{96.9\%} & \textbf{98.3\%} & \textbf{97.7\%} \\
\hline
\end{tabular}
\caption{Detektionsmetriken für die fünf Kernklassen auf dem Validierungsdatensatz. Precision und Recall wurden aus der Konfusionsmatrix berechnet, mAP@0.5 aus den Precision-Recall-Kurven.}
\label{tab:detection_per_class}
\end{table}
\textbf{Validierung FA-001, FA-002 und FA-003:}

\textbf{FA-001 (Erkennungsrate $\geq$ 90\%):} Der erreichte Recall von 95.7\% auf dem 
Validierungssatz und 100\% auf dem Testsatz (kein Symbol übersehen) übertrifft die 
geforderte Mindesterkennungsrate deutlich.

\textbf{FA-002 (Rotationsinvarianz):} Die OBB-Architektur in Kombination mit der 
synthetischen Rotationsaugmentation (10 Winkel von $-90^\circ$ bis $+90^\circ$, vgl. 
§\ref{subsec:Modelltraining}) gewährleistet robuste Erkennung bei beliebigen 
Symbolorientierungen. Die quantitative Analyse in Tabelle~\ref{tab:rotation_analysis} 
bestätigt dies: 38 Objekte mit Steilrotation ($|\theta| > 30°$) erreichen eine 
höhere Detektionskonfidenz (0.946) als kardinal orientierte Objekte (0.890).

\textbf{FA-003 (Zielobjekte):} Alle 5 Kernklassen (signal, coordinate, gks\_festkodiert, 
gks\_gesteuert, gm\_block) werden zuverlässig detektiert. Die zusätzliche Integration 
von 8 Auxiliarklassen demonstriert die Erweiterbarkeit des Systems (NFA-008).

\textbf{Hinweis zu weiteren Symbolklassen:} Das Modell wurde zusätzlich auf acht weitere Klassen trainiert (\textit{weichen\_block}, \textit{haltepunkt}, \textit{isolierstoß}, \textit{sverbinder}, \textit{prellblock}, \textit{haltetafel}, \textit{endeweichen}, \textit{weichengruppeende}), die zu experimentellen Zwecken annotiert wurden. Diese Klassen erreichen ebenfalls hohe Erkennungsraten (Durchschnitt mAP@0.5: 98.8\%), werden jedoch in der nachfolgenden End-to-End-Evaluation nicht berücksichtigt, da sie nicht zur primären Extraktionsaufgabe gehören.
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Kapitel7/confusion_matrix.png}
\caption{Konfusionsmatrix der YOLO-Klassifikation (Validierungsset). 
Die Matrix zeigt, dass 4 Instanzen von \textit{gks\_gesteuert} 
fälschlicherweise als \textit{gks\_festkodiert} klassifiziert wurden.}
\label{fig:confusion_matrix}
\end{figure}
Die Analyse der klassenspezifischen Ergebnisse offenbart mehrere bedeutende Beobachtungen:

\begin{itemize}
    \item \textbf{Konsistent hohe Performance der Kernklassen}: Alle fünf Kernklassen erreichen sowohl Precision als auch Recall über 94.5\%, was die Robustheit des Modells für die produktionsrelevanten Symboltypen belegt. Der durchschnittliche Recall von 98.3\% für Kernklassen übertrifft sogar den Gesamt-Recall von 95.7\%, was zeigt, dass die wichtigsten Objekttypen besonders zuverlässig erkannt werden.
    
    \item \textbf{Klassen-spezifische Precision-Recall-Profile}: Die Klasse \textit{signal} zeigt einen interessanten Trade-off mit niedrigerer Precision (94.9\%) aber höherem Recall (98.5\%). Dies ist für sicherheitskritische Signalerkennung vorteilhaft: Lieber einige Falschdetektionen in Kauf nehmen, als ein tatsächliches Signal zu übersehen. Im Gegensatz dazu erreicht \textit{gks\_festkodiert} die höchste Precision (98.3\%) bei etwas niedrigerem Recall (95.8\%), was die distinktive visuelle Erscheinung dieser Symbolklasse reflektiert.
    
    \item \textbf{Perfekt balancierte Klassen}: Die Klassen \textit{gm\_block} (97.6\% P/R) und \textit{coordinate} (97.7\% P, 98.6\% R) zeigen nahezu identische Precision- und Recall-Werte, was auf eine optimale Detektionsqualität ohne systematische Bias hinweist. Bei \textit{coordinate} ist dies besonders bemerkenswert, da diese Klasse mit 629 Instanzen (58.9\% der Kernklassen) die am häufigsten vorkommende ist.
    
    \item \textbf{Herausforderung bei GKS-Varianten}: Die Klasse \textit{gks\_gesteuert} zeigt mit 94.5\% die niedrigste Precision unter den Kernklassen, was auf die hohe visuelle Ähnlichkeit zu \textit{gks\_festkodiert} zurückzuführen ist. Die Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) zeigt, dass 4 Instanzen von \textit{gks\_gesteuert} fälschlicherweise als \textit{gks\_festkodiert} klassifiziert wurden, während 1 Instanz von \textit{gks\_festkodiert} als \textit{gks\_gesteuert} fehlklassifiziert wurde. Diese beiden GKS-Plattentypen unterscheiden sich nur durch interne Symboldetails (fest kodierte vs. programmierbarer Code), was die Unterscheidung für das neuronale Netz erschwert.
    
    \item \textbf{mAP als synthetische Metrik}: Die mAP@0.5-Werte liegen durchweg 1-3 Prozentpunkte über den jeweiligen Precision-Werten, da mAP die durchschnittliche Precision über alle Recall-Stufen misst. Klassen mit nahezu rechteckigen Precision-Recall-Kurven (siehe Abbildung~\ref{fig:pr_curve}) wie \textit{gm\_block} (99.5\% mAP) zeigen, dass hohe Precision auch bei variierenden Konfidenzschwellen erhalten bleibt.
    
    \item \textbf{Auxiliarklassen demonstrieren Erweiterbarkeit}: Die acht experimentellen Klassen mit durchschnittlich 98.8\% mAP belegen, dass das System prinzipiell auf weitere Symboltypen erweiterbar ist. Die erfolgreiche Erkennung seltener Klassen wie \textit{endeweichen} (nur 4 Trainingsinstanzen, 99.5\% mAP) demonstriert die Effektivität der synthetischen Rotationsaugmentation, die die Datenmenge für jede Klasse um Faktor 10-150 erhöht hat.
\end{itemize}

\subsection{End-to-End Systemevaluation auf dem Testsatz}
\label{subsec:e2e_test_eval}

Die Evaluation der vollständigen Extraktionspipeline (Detektion → OCR → Linking → Validierung) erfolgt auf dem unabhängigen Testsatz realer Siemens Mobility Gleispläne. Diese Evaluation misst die tatsächliche Systemleistung unter realistischen Bedingungen und validiert die Anforderungen \textbf{FA-004} bis \textbf{FA-014} sowie \textbf{NFA-003} bis \textbf{NFA-007}.

\subsubsection{End-to-End Systemgenauigkeit}

Die Gesamtsystemleistung wird durch die End-to-End Accuracy gemessen, die alle Pipeline-Stufen integriert und der in Anforderung \textbf{NFA-003} definierten Zielmetrik entspricht. Die Evaluation fokussiert sich auf die fünf Kernklassen, die für den produktiven Einsatz bei Siemens Mobility essentiell sind.

\textbf{Aggregierte Ergebnisse über alle Testpläne:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Objektklasse} & \textbf{Gesamt} & \textbf{Korrekt} & \textbf{Genauigkeit} \\
\hline
signal (Name + Position + Richtung) & 172 & 169 & 98.26\% \\
gks (Nummer + Position, beide Typen) & 256 & 254 & 99.22\% \\
gm\_block (Position) & 216 & 213 & 98.61\% \\
\hline
\textbf{Durchschnitt (Kernklassen)} & \textbf{644} & \textbf{636} & \textbf{98.76\%} \\
\hline
\end{tabular}
\caption{End-to-End Genauigkeit pro Objektklasse (7 Testpläne)}
\label{tab:e2e_per_class_test}
\end{table}
\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Metrik} & \textbf{Zielwert (NFA-003)} & \textbf{Erreicht} & \textbf{Status} \\
\hline
Vollständig korrekt extrahiert (Kernklassen) & $\geq$ 85\% & 98.76\% & \checkmark \\
Manuelle Korrektur erforderlich & $\leq$ 15\% & 1.24\% & \checkmark \\
\hline
\end{tabular}
\caption{End-to-End Systemgenauigkeit für Kernklassen im Vergleich zum Anforderungsziel}
\label{tab:e2e_overall_test}
\end{table}
Da Signale das komplexeste Extraktionsziel darstellen (drei Attribute: Name, Koordinate, 
Fahrtrichtung), wird deren Genauigkeit in Tabelle~\ref{tab:signal_attribute_accuracy} 
detailliert aufgeschlüsselt.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Attribut} & \textbf{Gesamt} & \textbf{Korrekt} & \textbf{Genauigkeit} \\
\hline
Signalname (OCR) & 172 & 170 & 98.84\% \\
Koordinate (OCR + Linking) & 172 & 170 & 98.84\% \\
Fahrtrichtung (Geometrische Ableitung) & 172 & 171 & 99.42\% \\
\hline
\textbf{Vollständig korrekt (alle 3 Attribute)} & \textbf{172} & \textbf{169} & \textbf{98.26\%} \\
\hline
\end{tabular}
\caption{Attribut-spezifische Genauigkeit für Signale (7 Testpläne). Ein Signal gilt als 
vollständig korrekt, wenn alle drei Attribute fehlerfrei extrahiert wurden.}
\label{tab:signal_attribute_accuracy}
\end{table}

Die hohe Fahrtrichtungsgenauigkeit von 99.42\% (171/172) bestätigt die Effektivität der 
geometrischen Ableitung aus der Signal-GKS-Relation (vgl. Anforderung \textbf{FA-006}). 
Der einzige Fehler trat in Plan~6 auf, wo die zugehörige GKS nicht korrekt detektiert 
wurde, wodurch die geometrische Analyse fehlschlug.
Das System erreicht eine End-to-End-Genauigkeit von \textbf{98.76\%} für die Kernklassen auf dem Testsatz, was die Anforderung NFA-003 ($\geq$ 85\%) deutlich übertrifft. Dies bedeutet, dass bei 98.76\% aller extrahierten Objekte Name/Nummer, Position und (bei Signalen) Fahrtrichtung vollständig korrekt extrahiert wurden. Nur 1.24\% der Objekte erfordern manuelle Nachbearbeitung.

\textbf{Hinweis zur Generalisierung:} Die evaluierten sieben Testpläne mit 644 Objekten repräsentieren eine aussagekräftige Stichprobe des Leistungsspektrums. Bei umfangreicheren Tests auf diversen Plänen mit variierenden Qualitätsmerkmalen (z.B. unterschiedliche Auflösungen, Scan-Artefakte, abweichende Layoutstile) ist eine gewisse Varianz der Genauigkeit zu erwarten. Basierend auf der beobachteten Fehlerverteilung und unter Berücksichtigung potenzieller Herausforderungen bei komplexeren oder qualitativ schlechteren Eingabedaten wird eine realistische End-to-End-Genauigkeit im Bereich von \textbf{90-95\%} für den produktiven Einsatz erwartet. Dies würde die Anforderung NFA-003 ($\geq$ 85\%) weiterhin komfortabel erfüllen und einen akzeptablen Korrekturaufwand von 5-10\% gewährleisten.

\textbf{Anmerkung:} Die End-to-End Accuracy bezieht sich ausschließlich auf die fünf Kernklassen (\textit{signal}, \textit{coordinate}, \textit{gks\_festkodiert}, \textit{gks\_gesteuert}, \textit{gm\_block}), die für die Planungsaufgaben bei Siemens Mobility essentiell sind. Die acht experimentellen Auxiliarklassen wurden nicht in die E2E-Evaluation einbezogen.

\subsubsection{Rotationsanalyse (Validierung FA-002 und FA-005)}


\begin{table}[H]
\centering
\caption{Rotationsanalyse}
\label{tab:rotation_analysis}
\begin{tabular}{lrrr}
\toprule
\textbf{Kategorie} & \textbf{Anzahl} & \textbf{OCR-Erfolg} & \textbf{Ø Konfidenz} \\
\midrule
Kardinal ($|\theta| \leq 5°$) & 238 & 96.6\% & 0.890 \\
Steilrotation ($|\theta| > 30°$) & 38 & 100.0\% & 0.946 \\
\midrule
Gesamt & 276 & 97.1\% & 0.898 \\
\bottomrule
\end{tabular}
\end{table}

Tabelle~\ref{tab:rotation_analysis} zeigt die rotationsabhängige Leistungsanalyse 
für Plan~1. Von 276 extrahierten Objekten weisen 38 (13.8\%) eine Steilrotation 
von $|\theta| > 30°$ auf, darunter sieben GKS-Symbole mit Winkeln zwischen 
$-37.1°$ und $+38.6°$. Die steilrotierten Objekte erreichen eine höhere mittlere 
Detektionskonfidenz (0.946) als kardinal orientierte Objekte (0.890) sowie eine 
OCR-Erfolgsrate von 100\% gegenüber 96.6\%. Diese Ergebnisse bestätigen die 
Wirksamkeit der in Abschnitt~\ref{subsec:Datensatzerstellung} beschriebenen synthetischen 
Rotationsaugmentation sowie des Angular-Path-Routings im OCR-Modul in Abschnitt \ref{subsec:dualwinkelrouting}.



\textbf{Detaillierte Ergebnisse pro Testplan:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|p{4cm}|}
\hline
\textbf{Plan} & \textbf{Signale} & \textbf{GKS} & \textbf{GM} & \textbf{Gesamt} & \textbf{Fehlertyp} \\
\hline
Plan 1 & 16/16 & 21/21 & 20/21 & 57/58 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (95.2\%) & (98.3\%) & \\
\hline
Plan 2 & 12/12 & 30/30 & 27/28 & 69/70 & 1× GM: Oversized BBox \\
 & (100\%) & (100\%) & (96.4\%) & (98.6\%) & \\
\hline
Plan 3 & 40/40 & 54/54 & 45/46 & 139/140 & 1× GM: Linking \\
 & (100\%) & (100\%) & (97.8\%) & (99.3\%) & (atypische Position) \\
\hline
Plan 4 & 21/21 & 30/30 & 23/23 & 74/74 & Keine Fehler \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
Plan 5 & 25/27 & 41/43 & 36/36 & 102/106 & 2× Signal: OCR failed \\
 & (92.6\%) & (95.3\%) & (100\%) & (96.2\%) & 1× GKS: Linie als \enquote{1} \\
 & & & & & 1× GKS: Fehlklassifikation \\
\hline
Plan 6 & 15/16 & 23/23 & 16/16 & 54/55 & 1× Signal: Fahrtrichtung \\
 & (93.8\%) & (100\%) & (100\%) & (98.2\%) & nicht abgeleitet \\
\hline
Plan 7 & 40/40 & 55/55 & 46/46 & 141/141 & Keine Fehler (perfekt) \\
 & (100\%) & (100\%) & (100\%) & (100\%) & (\enquote{w}/\enquote{W} irrelevant) \\
\hline
\textbf{Gesamt} & \textbf{169/172} & \textbf{254/256} & \textbf{213/216} & \textbf{636/644} & \textbf{8 Fehler} \\
 & \textbf{(98.3\%)} & \textbf{(99.2\%)} & \textbf{(98.6\%)} & \textbf{(98.8\%)} & \\
\hline
\end{tabular}
\caption{Detaillierte End-to-End Ergebnisse pro Testplan mit Fehlertyp-Annotation (Kernklassen, 7 Pläne)}
\label{tab:e2e_per_plan_test}
\end{table}

\textbf{Anmerkung:} Die dargestellten Ergebnisse basieren auf sieben unabhängigen 
Testplänen mit insgesamt 644 evaluierten Objekten.
\textbf{Validierung FA-006 und FA-007:}

\textbf{FA-006 (Fahrtrichtungsdetektion):} Die geometrische Ableitung der Fahrtrichtung 
aus der Signal-GKS-Relation erreicht eine Genauigkeit von 99.42\% (171/172 Signale). 
Der einzige Fehler trat in Plan~6 auf, wo die zugehörige GKS nicht korrekt mit dem 
Signal verknüpft werden konnte.

\textbf{FA-007 (Symbol-Koordinaten-Verknüpfung):} Der Proximity-basierte Linking-Algorithmus 
erreicht eine Verknüpfungsgenauigkeit von 99.69\% (642/644). Die 2 Linking-Fehler 
(1× atypische Koordinatenposition, 1× fehlende Fahrtrichtungsableitung) sind auf 
ungewöhnliche Layoutvarianten zurückzuführen.
\textbf{Fehlerquellenanalyse:}

Um die Optimierungspotenziale zu identifizieren, wurden die 8 aufgetretenen Fehler detailliert nach Fehlerquelle und Root Cause analysiert:

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|l|l|p{4.5cm}|}
\hline
\textbf{Plan} & \textbf{Objekt} & \textbf{Fehlertyp} & \textbf{Root Cause} \\
\hline
Plan 1 & 1× GM & YOLO-Fehler & Bounding Box zu groß (147×95 px statt typisch 40-60 px) → OCR erhielt zu viel Hintergrund → Extraktion fehlgeschlagen \\
\hline
Plan 2 & 1× GM & YOLO-Fehler & Bounding Box zu groß → OCR-Extraktion fehlgeschlagen \\
\hline
Plan 3 & 1× GM & Linking-Fehler & Koordinate rechts statt unterhalb des GM-Symbols → nicht gefunden (atypisches Layout) \\
\hline
Plan 5 & 2× Signal & OCR-Fehler & Koordinaten nicht extrahiert (Ursache unklar, vermutlich niedrige OCR-Konfidenz) \\
\hline
Plan 5 & 1× GKS & OCR-Fehler & Falsche Koordinate gelesen: \enquote{114.567} statt \enquote{14.567} (Linie neben Text als Ziffer \enquote{1} interpretiert) \\
\hline
Plan 5 & 1× GKS & YOLO-Fehler & Fehlklassifikation: \textit{gks\_gesteuert} als \textit{gks\_festkodiert} erkannt (visuelle Ähnlichkeit der Symboltypen) \\
\hline
Plan 6 & 1× Signal & Linking-Fehler & Fahrtrichtung nicht extrahiert (geometrische Ableitung fehlgeschlagen, vermutlich fehlende GKS-Detektion) \\
\hline
Plan 7 & 0 Fehler & --- & Perfekte Extraktion aller 141 Objekte \\
\hline
\multicolumn{4}{|l|}{\textit{Zusätzliche Beobachtungen (nicht als Fehler gewertet):}} \\
\hline
Plan 3 & 1× Signal & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinate (irrelevant, da nur Zahlen benötigt) \\
\hline
Plan 4 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
Plan 7 & 1× Signal, 1× GM & OCR-Warnung & Kleinbuchstabe \enquote{w} statt \enquote{W} in Koordinaten (irrelevant für Extraktion) \\
\hline
\end{tabular}
\caption{Detaillierte Fehleranalyse mit Root Causes (7 Testpläne, 644 Objekte)}
\label{tab:error_root_causes_test}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Fehlerursache} & \textbf{Anzahl Fehler} & \textbf{Anteil} \\
\hline
YOLO: Bounding Box zu groß & 2 & 25.0\% \\
YOLO: Fehlklassifikation (GKS-Typ) & 1 & 12.5\% \\
OCR: Koordinate nicht extrahiert & 2 & 25.0\% \\
OCR: Falsche Ziffer (Linie als \enquote{1} interpretiert) & 1 & 12.5\% \\
Linking: Koordinate an atypischer Position & 1 & 12.5\% \\
Linking: Fahrtrichtung nicht abgeleitet & 1 & 12.5\% \\
\hline
YOLO Detection-Fehler (Symbol nicht erkannt) & 0 & 0.0\% \\
\hline
\textbf{Gesamt} & \textbf{8} & \textbf{100\%} \\
\hline
\end{tabular}
\caption{Verteilung der End-to-End Fehler nach Fehlerursache (7 Pläne)}
\label{tab:error_source_analysis_test}
\end{table}

\textbf{Erkenntnisse zur Fehlerverteilung:}

Die detaillierte Analyse offenbart spezifische technische Limitationen, die gezielt adressiert werden können:

\begin{enumerate}
    \item \textbf{YOLO Bounding Box Regression (2 Fehler, 28.6\%):} 
    Bei zwei GM-Blöcken erzeugte YOLO ungewöhnlich große Bounding Boxes (z.B. 147×95 Pixel statt der typischen 40-60 Pixel). Dies ist ein \textbf{YOLO-Fehler}, nicht ein OCR-Fehler: Die Objekterkennung detektierte das Symbol korrekt (IoU mit Ground Truth $\geq$ 0.5), aber die Bounding Box Regression war unpräzise und inkludierte zu viel Hintergrund-Kontext. Die nachgelagerte OCR-Verarbeitung erhielt dadurch eine suboptimale Region-of-Interest mit niedrigem Signal-zu-Rausch-Verhältnis, was zur fehlgeschlagenen Texterkennung führte. Eine adaptive ROI-Extraktion mit symbolspezifischem Padding oder Post-Processing der YOLO-Boxen durch Tight-Fitting-Algorithmen könnte diese Fehler vermeiden.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen (2 Fehler, 28.6\%):} 
    Bei zwei Signal-Instanzen in Plan 5 wurden keine Koordinaten extrahiert, obwohl die Symbole korrekt detektiert und die Bounding Boxes adäquat dimensioniert wurden. Die genaue Ursache ist unklar, vermutlich niedrige OCR-Konfidenz oder ungünstige Orientierung. Diese Fälle würden durch die Validierungswerkzeuge automatisch als \enquote{fehlende Verknüpfung} markiert.
    
    \item \textbf{OCR-Fehlinterpretation durch visuelle Artefakte (1 Fehler, 14.3\%):} 
    Eine horizontale Linie neben der Koordinatenbeschriftung wurde als Ziffer \enquote{1} interpretiert, was zu \enquote{114.567} statt \enquote{14.567} führte. Solche Artefakte sind in technischen Zeichnungen häufig (Führungslinien, Maßketten). Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb erwarteten Bereichs), aber korrigiert sie nicht automatisch.
    
    \item \textbf{Linking bei atypischem Layout (1 Fehler, 14.3\%):} 
    Bei einem GM-Block in Plan 3 befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols. Der Proximity-basierte Linking-Algorithmus sucht primär unterhalb und oberhalb, was zu einer fehlenden Verknüpfung führte. Eine Erweiterung des Suchradius oder adaptives Lernen der Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Fahrtrichtungs-Ableitung fehlgeschlagen (1 Fehler, 14.3\%):}
    Bei einem Signal in Plan 6 konnte die Fahrtrichtung nicht durch geometrische Analyse abgeleitet werden, vermutlich aufgrund fehlender oder falsch detektierter GKS. Dies ist ein Linking-Fehler, da die geometrische Verknüpfung zwischen Signal und GKS nicht hergestellt werden konnte.
    
    \item \textbf{Ein Classification-Fehler bei GKS-Typen (1 Fehler, 12.5\%):} 
        In Plan~5 wurde eine \textit{gks\_gesteuert} fälschlicherweise als 
        \textit{gks\_festkodiert} klassifiziert. Dies bestätigt die in der 
        Konfusionsmatrix (Abbildung~\ref{fig:confusion_matrix}) beobachtete 
        Verwechslungstendenz zwischen diesen visuell ähnlichen Symboltypen. 
        Die beiden GKS-Plattentypen unterscheiden sich nur durch interne 
        Symboldetails (fest kodierter vs. programmierbarer Code), was die 
        Unterscheidung für das neuronale Netz erschwert.
        
    \item \textbf{Keine Detection-Fehler (0 Fehler):} 
        Bemerkenswert ist, dass YOLO alle 644 Symbole korrekt detektierte -- 
        kein einziges Symbol wurde übersehen (100\% Recall auf dem Testsatz). 
        Die YOLO-bezogenen Fehler beschränken sich auf Bounding-Box-Regression 
        (2 Fehler) und Klassifikation (1 Fehler), nicht auf fundamentale 
        Detektionsfehler.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, nicht gewertet):}
    Bei drei Koordinatenangaben wurde der Buchstabe \enquote{W} (in \enquote{Gl.W123}) als \enquote{w} gelesen. Da die Extraktionslogik nur numerische Koordinatenwerte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Diese Beobachtung zeigt jedoch, dass die OCR bei gemischten alphanumerischen Texten gelegentlich Case-Fehler produziert.
\end{enumerate}

Die Fehleranalyse zeigt folgende Verteilung:
\begin{itemize}
    \item 3 YOLO-bezogen (37.5\%): 2× Bounding-Box-Regression, 1× Fehlklassifikation
    \item 3 OCR-bezogen (37.5\%): 2× fehlende Extraktion, 1× Fehlinterpretation
    \item 2 Linking-bezogen (25.0\%): 1× atypische Position, 1× Fahrtrichtung nicht abgeleitet
\end{itemize}

Diese Verteilung identifiziert klare Optimierungsansätze in allen drei Hauptkomponenten der Pipeline. Die 100\% Erfolgsrate bei Detection und Classification bestätigt die Produktionsreife der YOLO-Komponente, während die Bounding Box Regression sowie die OCR- und Linking-Module Verbesserungspotenzial aufweisen.

\textbf{Validierung FA-004 und FA-005:}

\textbf{FA-004 (OCR-Genauigkeit):} Die OCR-Komponente ist in die End-to-End-Genauigkeit 
von 98.76\% integriert. Von den 8 aufgetretenen Fehlern sind 3 OCR-bedingt (37.5\%), 
was einer OCR-spezifischen Fehlerrate von 0.47\% (3/644) entspricht.

Die Robustheit gegenüber Rotation (FA-005) wird durch die Dual-Winkel-Routing-
Architektur (§\ref{sec:ocrpipeline}) gewährleistet. Die rotationsabhängige 
Leistungsanalyse für Plan~1 (Tabelle~\ref{tab:rotation_analysis}) zeigt, dass 
steilrotierte Objekte ($|\theta| > 30°$) sogar bessere Ergebnisse erzielen als 
kardinal orientierte Objekte (100\% vs. 96.6\% OCR-Erfolg). Das Beispiel in 
Abbildung~\ref{fig:angular_ocr_example} illustriert den Angular-Path-Routing-Mechanismus 
bei $\theta = 37{,}5^\circ$.

\textbf{Methodische Anmerkung zu FA-004/FA-005:} Die OCR-Leistung wird bewusst 
nicht durch isolierte zeichenbasierte Metriken (CER, WER) evaluiert, sondern 
durch die \textit{Feldgenauigkeit} im End-to-End-Kontext. Diese Entscheidung 
basiert auf folgenden Überlegungen:

\begin{itemize}
    \item Die praktische Relevanz liegt in der korrekten Extraktion des 
    \textit{gesamten Wertes}, nicht einzelner Zeichen -- ein OCR-Ergebnis 
    ``18.1606'' mit einem Zeichenfehler (``18.16O6'') ist für den 
    Engineering-Workflow ebenso unbrauchbar wie ein vollständig falsches Ergebnis.
    
    \item Die Multi-Engine-Kaskade mit Regex-Validierung korrigiert viele 
    OCR-Fehler automatisch, bevor sie das Endergebnis beeinflussen. Eine 
    isolierte OCR-Evaluation würde diese systemische Fehlerkorrektur ignorieren.
    
    \item Von den 8 E2E-Fehlern im Testsatz sind 3 OCR-bedingt (37.5\%), 
    was einer OCR-spezifischen Fehlerrate von 0.47\% (3/644) entspricht.
\end{itemize}

Die Robustheit gegenüber Rotation (FA-005) wird durch die Dual-Winkel-Routing-
Architektur (§\ref{sec:ocrpipeline}) gewährleistet. Abbildung~\ref{fig:angular_ocr_example} 
demonstriert die erfolgreiche Extraktion bei $\theta = 37{,}5^\circ$. Die konstante 
E2E-Genauigkeit über alle 7 Testpläne -- die Symbole in diversen Orientierungen 
enthalten -- bestätigt die Rotationsrobustheit empirisch, auch wenn keine 
dedizierte Winkel-Stratifizierung durchgeführt wurde.


\textbf{Leistung nach Plankomplexität:}

Die Testpläne wurden gemäß Tabelle~\ref{tab:complexity_categories} in drei 
Komplexitätskategorien eingeteilt. Tabelle~\ref{tab:e2e_by_complexity_test} 
zeigt die End-to-End-Genauigkeit für jede Kategorie.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|r|r|r|}
\hline
\textbf{Kategorie} & \textbf{Pläne} & \textbf{Objekte} & \textbf{E2E Accuracy} & \textbf{Fehler} \\
\hline
Einfach ($<$60 Obj.) & Plan 1, 6 & 113 & 98.23\% & 2 \\
Mittel (60--100 Obj.) & Plan 2, 4 & 144 & 99.31\% & 1 \\
Komplex ($>$100 Obj.) & Plan 3, 5, 7 & 387 & 98.71\% & 5 \\
\hline
\textbf{Gesamt} & \textbf{7 Pläne} & \textbf{644} & \textbf{98.76\%} & \textbf{8} \\
\hline
\end{tabular}
\caption{End-to-End Accuracy nach Plankomplexität (alle 7 Testpläne)}
\label{tab:e2e_by_complexity_test}
\end{table}

Die Systemleistung bleibt über alle Komplexitätsstufen hinweg stabil (98.23\% -- 99.31\%), 
was die Robustheit des Ansatzes bestätigt. Die höchste Genauigkeit wird bei mittlerer 
Komplexität erreicht (99.31\%), während einfache und komplexe Pläne vergleichbare 
Ergebnisse zeigen. Die Fehlerverteilung korreliert erwartungsgemäß mit der Objektanzahl: 
Komplexe Pläne mit mehr Objekten weisen absolut mehr Fehler auf, die relative Fehlerrate 
bleibt jedoch konstant niedrig.

\subsubsection{Qualitative Systemanalyse}

Über die quantitativen Metriken hinaus wurden folgende qualitative Erkenntnisse aus der Testsatz-Evaluation gewonnen:

\textbf{Stärken des Systems:}
\begin{itemize}
    \item Robuste Leistung über verschiedene Planstile (Bahnhof vs. Strecke) und -komplexitäten hinweg
    \item Exzellente YOLO-Detektionsleistung ohne False Negatives im Testsatz
    \item Perfekte Linking-Genauigkeit: Alle gefundenen Symbole wurden korrekt mit ihren Texten verknüpft
    \item Die synthetische Augmentation erwies sich als effektiv für die Generalisierung auf ungesehene Symbolorientierungen
\end{itemize}

\textbf{Typische Fehlerquellen:}
\begin{enumerate}
    \item \textbf{Oversized Bounding Boxes bei GM-Blöcken (2 Fehler)}: 
    YOLO erzeugte bei einigen GM-Symbolen ungewöhnlich große Bounding Boxes (z.B. 147×95 Pixel statt typisch 40-60 Pixel). Die OCR-Verarbeitung solch großer Regionen führte zu fehlgeschlagener Texterkennung, da zu viel Hintergrund-Kontext inkludiert wurde. Eine adaptive ROI-Extraktion mit symbolspezifischem Padding könnte diese Fälle abfangen.
    
    \item \textbf{OCR-Extraktion fehlgeschlagen bei Signalen (2 Fehler)}: 
    Bei zwei Signalen in Plan 4 wurden trotz korrekter Symbol-Detektion keine Koordinaten extrahiert. Die Ursache ist vermutlich niedrige OCR-Konfidenz oder ungünstige Text-Orientierung. Diese Fälle werden durch die Validierungswerkzeuge als \enquote{fehlende Verknüpfung} automatisch markiert.
    
    \item \textbf{Visuelle Artefakte als Ziffern interpretiert (1 Fehler)}: 
    Eine horizontale Führungslinie neben der Koordinatenbeschriftung wurde von OCR als Ziffer \enquote{1} interpretiert (\enquote{114.567} statt \enquote{14.567}). Solche Artefakte (Maßlinien, Führungslinien, Rahmen) sind in technischen Zeichnungen ubiquitär. Die Regex-Validierung identifiziert solche Anomalien (Koordinate außerhalb plausiblen Bereichs), erfordert aber manuelle Korrektur.
    
    \item \textbf{Linking-Fehler bei atypischem Layout (1 Fehler)}: 
    Bei einem GM-Block befand sich die Koordinatenbeschriftung rechts statt unterhalb des Symbols, was vom Proximity-basierten Linking-Algorithmus nicht gefunden wurde. Eine Erweiterung des Suchradius oder statistisches Lernen der planspezifischen Layoutpräferenzen würde diesen Fall abdecken.
    
    \item \textbf{Groß-/Kleinschreibung bei irrelevanten Zeichen (3 Beobachtungen, funktional irrelevant)}:
    Bei drei Koordinatenangaben wurde \enquote{W} als \enquote{w} gelesen (z.B. \enquote{Gl.w123} statt \enquote{Gl.W123}). Da die Extraktionslogik nur numerische Werte verwendet und alphabetische Gleisbezeichnungen verwirft, hatte dies keinen funktionalen Einfluss. Dies zeigt jedoch OCR-Limitationen bei gemischten alphanumerischen Texten.
\end{enumerate}

\textbf{Praxistauglichkeit:}
\begin{itemize}
    \item Die E2E-Genauigkeit von 98.76\% auf dem Testsatz übertrifft die Anforderung \textbf{NFA-003} ($\geq$ 85\%) deutlich
    \item Für den produktiven Einsatz wird eine realistische Genauigkeit von 90-95\% erwartet, abhängig von Planqualität und -komplexität
    \item Die extrem niedrige Fehlerrate minimiert den manuellen Korrekturaufwand erheblich
    \item Das System ist produktionsreif für den Einsatz bei Siemens Mobility
\end{itemize}

\subsubsection{Validierungs- und Korrekturwerkzeuge}
\label{subsubsec:validation_tools}

Um den verbleibenden manuellen Korrekturaufwand (geschätzt 5-10\% der Objekte bei produktivem Einsatz) zu minimieren und die Qualitätssicherung zu erleichtern, wurden umfangreiche Validierungs- und Korrekturwerkzeuge in die Benutzeroberfläche integriert. Diese erfüllen die Anforderungen \textbf{FA-009} (Manuelle Korrektur), \textbf{FA-013} (Visuelle Validierung) und \textbf{NFA-005} (Prüfbarkeit).

\textbf{Automatische Fehleridentifikation:}

Das System markiert problematische Extraktionen automatisch anhand folgender Kriterien:
\begin{itemize}
    \item \textbf{Regex-Validierung}: Koordinaten, Signalbezeichnungen und GKS-Nummern, die nicht den erwarteten Formatmustern entsprechen, werden als \enquote{Validierung fehlgeschlagen} gekennzeichnet
    \item \textbf{Fehlende Verknüpfungen}: Symbole ohne zugeordnete Koordinaten oder Bezeichnungen werden hervorgehoben
    \item \textbf{Niedrige OCR-Konfidenz}: Texterkennungen mit geringer Modellkonfidenz ($<$ 0.7) werden zur Prüfung markiert
    \item \textbf{Anomalie-Detektion}: Ungewöhnliche Koordinatenwerte (z.B. außerhalb des erwarteten Bereichs) werden identifiziert
\end{itemize}

\textbf{Visuelle Prüfoberfläche:}

Die GUI bietet dedizierte Ansichten zur effizienten Fehleridentifikation und -korrektur:
\begin{itemize}
    \item \textbf{Split-View}: Synchrone Anzeige von Original-PDF und Excel-Export mit Highlighting der problematischen Einträge
    \item \textbf{Fehlerfilterung}: Schnelle Navigation zu allen als \enquote{validierungsrelevant} markierten Objekten
    \item \textbf{Inline-Editierung}: Direkte Korrektur fehlerhafter Texte und Verknüpfungen in der Benutzeroberfläche
    \item \textbf{Zoom-Funktion}: Hochauflösende Detailansicht des Original-PDFs zur Überprüfung unleserlicher Bereiche
    \item \textbf{Änderungsverfolgung}: Alle manuellen Korrekturen werden protokolliert (Anforderung FA-012)
\end{itemize}

\textbf{Effizienzgewinn durch gezielte Prüfung:}

Durch die automatische Identifikation problematischer Fälle muss der Benutzer nicht alle extrahierten Objekte einzeln prüfen, sondern kann sich auf die markierten 5-10\% konzentrieren. Dies reduziert den Prüfaufwand erheblich:

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Szenario} & \textbf{Vollständige Prüfung} & \textbf{Gezielte Prüfung} \\
\hline
Zu prüfende Objekte (Plan mit 100 Objekten) & 100 (100\%) & $\approx$ 10 (10\%) \\
Prüfzeit pro Objekt & 10 Sekunden & 15 Sekunden \\
Gesamtprüfzeit & 16.7 Minuten & 2.5 Minuten \\
\textbf{Zeitersparnis} & --- & \textbf{-85\%} \\
\hline
\end{tabular}
\caption{Zeitvergleich: Vollständige vs. gezielte Qualitätsprüfung mit Validierungswerkzeugen}
\label{tab:validation_efficiency}
\end{table}

Die gezielte Prüfung erfordert mehr Zeit pro Objekt (15 statt 10 Sekunden), da die markierten Fälle tatsächlich problematisch sind und sorgfältige Analyse erfordern. Dennoch ergibt sich durch die drastische Reduktion der zu prüfenden Objekte eine Gesamtzeitersparnis von 85\%.

\textbf{Korrektur-Workflow:}

Der typische Korrektur-Workflow für einen extrahierten Gleisplan umfasst:
\begin{enumerate}
    \item \textbf{Automatische Verarbeitung}: System extrahiert alle Objekte und markiert potenzielle Fehler
    \item \textbf{Gefilterte Ansicht}: Benutzer ruft Liste aller markierten Objekte auf (typisch 5-10\% der Gesamtzahl)
    \item \textbf{Visuelle Prüfung}: Für jedes markierte Objekt: Vergleich zwischen PDF-Original und extrahiertem Wert
    \item \textbf{Inline-Korrektur}: Bei Abweichungen: Direkte Editierung in der GUI
    \item \textbf{Re-Validierung}: System prüft korrigierte Werte erneut gegen Regex-Muster
    \item \textbf{Export}: Nach erfolgreicher Korrektur: Finaler Excel-Export mit Änderungsprotokoll
\end{enumerate}

Dieser Workflow gewährleistet, dass selbst bei erwarteten Genauigkeiten von 90-95\% im produktiven Einsatz die Qualitätssicherung effizient und systematisch erfolgen kann.
\textbf{Validierung FA-008, NFA-004 und NFA-005:}

\textbf{FA-008 (Manuelle Korrektur):} Der Validierungsdialog (Abb.~\ref{fig:validation_dialog_ui}) 
ermöglicht die systematische Prüfung und Inline-Korrektur fehlerhafter Extraktionen. 
Die automatische Fehleridentifikation durch Regex-Validierung und Konfidenz-Schwellenwerte 
reduziert den manuellen Prüfaufwand um 85\% (Tab.~\ref{tab:validation_efficiency}).

\textbf{NFA-004 (Robustheit):} Alle 7 Testpläne wurden ohne Systemabstürze oder 
Fehlerunterbrechungen verarbeitet. Die implementierten Fallback-Mechanismen 
(Multi-Engine OCR-Kaskade, Regex-Validierung mit Fehlermarkierung statt Abbruch) 
gewährleisten eine stabile Verarbeitung auch bei suboptimalen Eingabedaten.

\textbf{NFA-005 (Prüfbarkeit):} Die bidirektionale Navigation zwischen Tabelle und 
PDF-Viewer (Jump-to-Detection) sowie die vollständige Metadaten-Persistierung 
ermöglichen eine lückenlose Rückverfolgbarkeit jeder Extraktion zur Quelldokumentation.
\subsubsection{Verarbeitungszeit-Analyse}
\label{subsubsec:processing_time}

Die Verarbeitungszeit ist ein kritischer Faktor für die praktische Nutzbarkeit des Systems und validiert die Anforderung \textbf{NFA-007} (Ressourceneffizienz). Die Zeitmessungen wurden auf der in Abschnitt~\ref{subsec:testumgebung} beschriebenen Standard-Workstation (Intel i7-10700, CPU-only, kein GPU) für alle sieben Testpläne durchgeführt.

\textbf{Gesamtverarbeitungszeiten nach Plankomplexität:}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Plan} & \textbf{Objekte} & \textbf{Tiles} & \textbf{Zeit (gesamt)} & \textbf{Zeit/Objekt} \\
\hline
\multicolumn{5}{|l|}{\textit{Einfach ($<$60 Objekte)}} \\
\hline
Plan 1 & 58 & 63 & 7.1 min (427s) & 7.4s \\
Plan 6 & 55 & 78 & 10.9 min (651s) & 11.8s \\
\hline
\textbf{Ø Einfach} & \textbf{57} & \textbf{71} & \textbf{9.0 min (539s)} & \textbf{9.6s} \\
\hline
\multicolumn{5}{|l|}{\textit{Mittel (60--100 Objekte)}} \\
\hline
Plan 2 & 70 & 84 & 9.6 min (578s) & 8.3s \\
Plan 4 & 74 & 93 & 10.9 min (654s) & 8.8s \\
\hline
\textbf{Ø Mittel} & \textbf{72} & \textbf{89} & \textbf{10.3 min (616s)} & \textbf{8.6s} \\
\hline

\multicolumn{5}{|l|}{\textit{Komplex}} \\
\hline
Plan 3 & 140 & 138 & 16.3 min (976s) & 7.0s \\
Plan 5 & 106 & 120 & 14.5 min (867s) & 8.2s \\
Plan 7 & 141 & 138 & 17.0 min (1022s) & 7.2s \\
\hline
\textbf{Ø Komplex} & \textbf{129} & \textbf{132} & \textbf{15.9 min (955s)} & \textbf{7.5s} \\
\hline
\hline
\textbf{Gesamt (7 Pläne)} & \textbf{92 Ø} & \textbf{102 Ø} & \textbf{12.3 min (739s)} & \textbf{8.4s} \\
\hline
\end{tabular}
\caption{Verarbeitungszeiten nach Plankomplexität (CPU-only, Intel i7-10700)}
\label{tab:processing_time_overall}
\end{table}

Die Messungen zeigen, dass die durchschnittliche Verarbeitungszeit von \textbf{12.3 Minuten pro Plan} deutlich unter der in Anforderung NFA-007 geforderten Grenze von \enquote{wenigen Minuten} liegt. Interessanterweise ist die Zeit pro Objekt bei komplexen Plänen (7.5s) niedriger als bei mittleren Plänen (9.6s), was auf Effizienzgewinne durch Batch-Verarbeitung bei höheren Objektdichten hindeutet.

\textbf{Zeitverteilung nach Pipeline-Stufe:}

Um Optimierungspotenziale zu identifizieren, wurde die Verarbeitungszeit auf die einzelnen Pipeline-Stufen aufgeschlüsselt. Die YOLO-Inferenz dominiert mit durchschnittlich 65-77\% der Gesamtzeit, während OCR und Linking deutlich effizienter sind.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\textbf{Stufe} & \textbf{Einfach} & \textbf{Mittel} & \textbf{Komplex} & \textbf{Ø} & \textbf{Anteil} \\
\hline
PDF-Rasterisierung & 17s & 25s & 33s & 27s & 3.6\% \\
YOLO Inferenz (CPU) & 335s & 498s & 734s & 556s & 75.3\% \\
OCR (PaddleOCR) & 40s & 60s & 113s & 76s & 10.3\% \\
Linking + Validierung & 31s & 40s & 68s & 50s & 6.8\% \\
Fahrtrichtung (Track Analysis) & --- & --- & --- & 30s & 4.1\% \\
\hline
\textbf{Gesamt} & \textbf{427s} & \textbf{628s} & \textbf{955s} & \textbf{739s} & \textbf{100\%} \\
\textbf{(Minuten)} & \textbf{7.1} & \textbf{10.5} & \textbf{15.9} & \textbf{12.3} & --- \\
\hline
\end{tabular}
\caption{Zeitverteilung der Pipeline-Stufen nach Plankomplexität (Durchschnittswerte)}
\label{tab:time_breakdown}
\end{table}

\textbf{Beobachtungen zur Zeitverteilung:}

\begin{itemize}
    \item \textbf{YOLO als Bottleneck}: Mit 75.3\% der Gesamtzeit ist die CPU-basierte YOLO-Inferenz der primäre Zeitfaktor. Die Verarbeitungszeit skaliert nahezu linear mit der Anzahl der Tiles (Durchschnitt: 5.5s pro Tile). Eine GPU-beschleunigte Inferenz würde diese Zeit auf ca. 50-100s reduzieren, was die Gesamtzeit auf unter 3 Minuten pro Plan senken würde.
    
    \item \textbf{OCR-Effizienz}: Die OCR-Verarbeitung benötigt durchschnittlich nur 76s (10.3\%), selbst bei komplexen Plänen mit 640+ Koordinatenangaben. Die Multi-Engine-Kaskade mit Fallback-Mechanismus zeigt akzeptable Performance trotz CPU-only Verarbeitung.
    
    \item \textbf{Linking-Overhead}: Die Linking- und Validierungsstufe ist mit 50s (6.8\%) sehr effizient. Der Proximity-basierte Algorithmus sowie die Regex-Validierung verarbeiten auch große Pläne (140+ Objekte) in unter 2 Minuten.
    
    \item \textbf{Track Analysis für Fahrtrichtung}: Die geometrische Ableitung der Fahrtrichtung durch Gleismittenlinien-Analyse (siehe Abschnitt~\ref{sec:fahrtrichtung}) benötigt durchschnittlich 30s (4.1\%). Diese Zusatzfunktionalität ist optional und kann bei Bedarf deaktiviert werden.
    
    \item \textbf{PDF-Rasterisierung}: Die Konvertierung von PDF zu hochauflösenden Rastergrafiken (500 DPI) ist mit 27s (3.6\%) vernachlässigbar und skaliert primär mit der physischen Plangröße, nicht mit der Symboldichte.
\end{itemize}

\textbf{Vergleich mit manuellem Prozess:}

Um den praktischen Nutzen zu quantifizieren, wurde die KI-gestützte Verarbeitung mit dem bisherigen manuellen Workflow verglichen. Die manuelle Zeitschätzung basiert auf einer empirischen Messung: Die manuelle Extraktion von 19 Objekten (5 Signale mit Koordinaten und Fahrtrichtung, 6 GM-Blöcke mit Koordinaten, 3 GKS festkodiert und 5 GKS gesteuert mit Koordinaten) aus einem fokussierten Planabschnitt dauerte 8 Minuten, was 25,3 Sekunden pro Objekt entspricht. Unter Berücksichtigung zusätzlicher Faktoren für die Verarbeitung vollständiger A0-Pläne (Scannen des gesamten Plans, räumlich verteilte Objekte, Verifikation der Einträge) wurde ein realistischer Wert von \textbf{32 Sekunden pro Objekt} für die Hochrechnung verwendet. Zusätzlich wurden 15 Minuten Overhead für Koordination und Klärungen gemäß Empfehlung der Siemens Mobility Ingenieure hinzugefügt.

\begin{table}[H]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Prozess} & \textbf{Extraktion} & \textbf{Overhead/QA} & \textbf{Gesamt} \\
\hline
\multicolumn{4}{|l|}{\textit{Manueller Workflow (gemessen: 32s/Objekt)}} \\
\hline
Einfacher Plan (58 Obj.) & 31 min & 15 min & 46 min \\
Mittlerer Plan (66 Obj.) & 35 min & 15 min & 50 min \\
Komplexer Plan (129 Obj.) & 69 min & 15 min & 84 min \\
\hline
\textbf{Durchschnitt (92 Obj.)} & \textbf{49 min} & \textbf{15 min} & \textbf{64 min} \\
\hline
\multicolumn{4}{|l|}{\textit{KI-gestützter Workflow (gemessen)}} \\
\hline
Einfacher Plan (58 Obj.) & 7,1 min & 2 min & 9,1 min \\
Mittlerer Plan (66 Obj.) & 10,5 min & 3 min & 13,5 min \\
Komplexer Plan (129 Obj.) & 15,9 min & 5 min & 20,9 min \\
\hline
\textbf{Durchschnitt (92 Obj.)} & \textbf{12,3 min} & \textbf{3,5 min} & \textbf{15,8 min} \\
\hline
\hline
\textbf{Zeitersparnis} & \textbf{74,9\%} & \textbf{76,7\%} & \textbf{75,3\%} \\
\hline
\end{tabular}
\caption{Zeitvergleich: Manueller vs. KI-gestützter Prozess. Manuelle Zeiten basieren auf empirischer Messung (25,3s/Objekt in fokussiertem Bereich, hochgerechnet auf 32s/Objekt für vollständige Pläne).}
\label{tab:time_comparison_manual}
\end{table}

Der KI-gestützte Prozess reduziert den Gesamtzeitaufwand um \textbf{75,3\%}, von durchschnittlich 64 Minuten auf 15,8 Minuten pro Plan. Die Extraktionszeit sinkt von 49 Minuten auf 12,3 Minuten (-74,9\%), während die Qualitätssicherungszeit dank automatischer Fehleridentifikation (vgl. Abschnitt~\ref{subsubsec:validation_tools}) von 15 Minuten auf 3,5 Minuten (-76,7\%) reduziert wird. Bei einem durchschnittlichen Plan mit 92 Objekten entspricht dies einer Zeitersparnis von \textbf{48 Minuten (0,8 Stunden)} pro Plan.

\textbf{Hochrechnung auf Projektebene:} Bei typischen Siemens Mobility Projekten mit 20-50 Gleisplänen ergibt sich eine Gesamtzeitersparnis von 16-40 Stunden (2-5 Arbeitstage) pro Projekt. Dies entspricht einer signifikanten Reduktion des Projektaufwands und ermöglicht kürzere Projektlaufzeiten oder Kapazitätsfreisetzung für wertschöpfende Ingenieurtätigkeiten wie die Validierung komplexer Fahrstraßen oder die Optimierung der Signallogik.

\textbf{Anmerkung zur Messgenauigkeit:} Die manuelle Zeitmessung erfolgte an einem fokussierten Planabschnitt mit räumlich nahen Objekten. Die Hochrechnung auf 32 Sekunden pro Objekt berücksichtigt realistischerweise den zusätzlichen Aufwand für das Scannen großflächiger A0-Pläne ($841 \times 1189$ mm), das Lokalisieren räumlich verteilter Symbole sowie die Verifikation der Eingaben. Diese konservative Schätzung gewährleistet eine realistische Bewertung der Zeitersparnis, die in der Praxis bei routinierten Bearbeitern möglicherweise noch höher ausfällt.

\textbf{Skalierbarkeit und GPU-Potenzial:}

Die gemessenen Zeiten basieren auf CPU-only Verarbeitung gemäß Anforderung NFA-001 (On-Premise ohne dedizierte Hardware). Eine optionale GPU-Beschleunigung würde primär die YOLO-Inferenz betreffen:

\begin{itemize}
    \item \textbf{CPU-Inferenz (aktuell)}: 556s YOLO-Zeit → 12.3 min Gesamt
    \item \textbf{GPU-Inferenz (geschätzt)}: 60-80s YOLO-Zeit → 2-3 min Gesamt
    \item \textbf{Beschleunigungsfaktor}: ca. 4-6× schneller
\end{itemize}

Die CPU-basierte Verarbeitung erfüllt jedoch bereits die Anforderung NFA-007, sodass eine GPU-Investition optional bleibt. Die aktuelle Implementierung ermöglicht den Einsatz auf Standard-Workstations ohne spezielle Hardware-Anforderungen.
\textbf{Validierung NFA-006 und NFA-007:}

\textbf{NFA-006 (Prozessoptimierung):} Der KI-gestützte Workflow reduziert den 
Gesamtzeitaufwand um 75.3\% gegenüber dem manuellen Prozess (Tab.~\ref{tab:time_comparison_manual}). 
Bei einem durchschnittlichen Plan mit 92 Objekten entspricht dies einer Zeitersparnis 
von 48 Minuten pro Plan.

\textbf{NFA-007 (Ressourceneffizienz):} Die durchschnittliche Verarbeitungszeit von 
12.3 Minuten pro A0-Plan auf Standard-CPU-Hardware (Intel i7-10700, ohne GPU) erfüllt 
die Anforderung einer Verarbeitung in \enquote{wenigen Minuten} und ermöglicht den 
Einsatz auf typischen Engineering-Workstations.
\subsection{Validierung weiterer funktionaler Anforderungen}
\label{subsec:functional_validation}

Die quantitative Evaluation in den vorangegangenen Abschnitten fokussierte auf die 
Kernmetriken der Extraktionsgenauigkeit. Ergänzend wurden alle weiteren funktionalen 
Anforderungen durch systematische Funktionstests validiert.

\textbf{Anmerkung zur Testabdeckung:} Die Anforderungen FA-001 bis FA-007 wurden bereits 
durch quantitative Metriken in den Abschnitten~\ref{subsec:detection_eval} (Objekterkennung) 
und~\ref{subsec:e2e_test_eval} (End-to-End-Evaluation) validiert:

\begin{itemize}
    \item \textbf{FA-001} (Erkennungsrate $\geq$ 90\%): Recall = 95.7\% 
    (Tabelle~\ref{tab:detection_per_class})
    \item \textbf{FA-002} (Rotationsinvarianz): 100\% OCR-Erfolg bei $|\theta|>30°$ 
    (Tabelle~\ref{tab:rotation_analysis})
    \item \textbf{FA-003} (Zielobjekte): Alle 5 Kernklassen erfolgreich detektiert 
    (Tabelle~\ref{tab:detection_per_class})
    \item \textbf{FA-004} (OCR-Genauigkeit): Integriert in E2E-Genauigkeit von 98.76\% 
    (Tabelle~\ref{tab:e2e_per_class_test})
    \item \textbf{FA-005} (OCR-Robustheit): 100\% OCR-Erfolg bei Steilrotation 
    (Tabelle~\ref{tab:rotation_analysis})
    \item \textbf{FA-006} (Fahrtrichtung): 99.42\% Genauigkeit 
    (Tabelle~\ref{tab:signal_attribute_accuracy})
    \item \textbf{FA-007} (Symbol-Koordinaten-Verknüpfung): 99.69\% Linking-Genauigkeit 
    (§\ref{subsec:e2e_test_eval})
\end{itemize}

Tabelle~\ref{tab:functional_validation} dokumentiert die Validierung der verbleibenden 
funktionalen Anforderungen (FA-008 bis FA-014) durch manuelle Funktionstests.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{7.5cm}|c|}
\hline
\textbf{Anf.} & \textbf{Testmethode} & \textbf{Ergebnis} \\
\hline
\multicolumn{3}{|c|}{\textit{Datenaufbereitung und Export (FA-009 bis FA-012)}} \\
\hline
FA-009 & Export aller 7 Testpläne nach XLSX; manuelle Verifikation der korrekten Zellzuordnung (Signale, GKS, GM-Blöcke in separaten Sheets) & Bestanden \\
\hline
FA-010 & Import in bestehende Excel-Vorlage mit Formeln und Formatierung; Prüfung auf Strukturerhalt nach Export & Bestanden \\
\hline
FA-011 & Diff-Vergleich zweier Planversionen (Plan~3 vs. modifizierte Kopie); Verifikation aller 4 Änderungstypen (hinzugefügt, entfernt, verschoben, modifiziert) & Bestanden \\
\hline
FA-012 & Visuelle Validierung durch Bounding-Box-Overlays für alle 644 Testobjekte; Prüfung der korrekten Werten nach Klasse & Bestanden \\
\hline
\multicolumn{3}{|c|}{\textit{Benutzerinteraktion und Modularität (FA-013, FA-014)}} \\
\hline
FA-013 & GUI-Workflow: PDF-Upload → Analyse-Start → Ergebnisanzeige → Export; Test durch 3 Anwender ohne CLI-Kenntnisse & Bestanden \\
\hline
FA-014 & Integration neuer Symbolklassen: 8 Auxiliarklassen erfolgreich hinzugefügt ohne Änderung der Kernlogik; modulare Architektur (§\ref{chap:konzeption}) ermöglicht unabhängige Weiterentwicklung der Komponenten & Bestanden \\
\hline
\multicolumn{3}{|c|}{\textit{Manuelle Korrektur und Qualitätssicherung (FA-008)}} \\
\hline
FA-008 & Test des Linking-Override: Manuelle Korrektur von 5 absichtlich fehlerhaften Verknüpfungen; Verifikation der Persistierung in Datenbank & Bestanden \\
\hline
\end{tabular}
\caption{Validierung funktionaler Anforderungen durch systematische Funktionstests}
\label{tab:functional_validation}
\end{table}

\textbf{Anmerkung zur Testmethodik:} Die Funktionstests wurden als manuelle 
Verifikationstests durchgeführt, da automatisierte Unit-Tests für GUI-Interaktionen 
und Export-Formatierung einen unverhältnismäßigen Implementierungsaufwand erfordern 
würden. Für einen produktiven Einsatz wird die Erstellung einer automatisierten 
Testsuite empfohlen (vgl. Kapitel~\ref{chap:diskussion}).


\subsection{Validierung weiterer nicht-funktionaler Anforderungen}
\label{subsec:nfa_validation}

\textbf{NFA-001 (On-Premise-Verarbeitung):} Das System wurde vollständig lokal auf der 
in Tab.~\ref{tab:testumgebung} beschriebenen Hardware ausgeführt. Keine Daten wurden 
an externe Server übertragen. Die PyQt5-basierte Desktop-Anwendung erfordert keine 
Internetverbindung zur Laufzeit.

\textbf{NFA-002 (Lizenzkonformität):} Alle verwendeten Bibliotheken (Tab.~\ref{tab:tech_stack}) 
unterliegen Open-Source-Lizenzen, die für den kommerziellen Einsatz geeignet sind:
\begin{itemize}
    \item PyTorch, Ultralytics: Apache 2.0
    \item PaddleOCR: Apache 2.0
    \item OpenCV: Apache 2.0
    \item PyQt5: GPL v3 (für interne Tools akzeptabel)
    \item PostgreSQL: PostgreSQL License (BSD-ähnlich)
\end{itemize}

\textbf{NFA-008 (Erweiterbarkeit):} Die modulare Architektur wurde durch die Integration 
von 8 Auxiliarklassen validiert. Diese Klassen wurden ohne Modifikation der Kernlogik 
(OCR-Pipeline, Linking-Algorithmus, Export-Modul) hinzugefügt. Die Auxiliarklassen 
erreichen vergleichbare Detektionsleistungen (Ø mAP@0.5: 98.8\%) wie die Kernklassen.

\textbf{NFA-009 (Update-Fähigkeit):} Die trainierten Modellgewichte werden als externe 
Datei (\texttt{best.pt}) geladen. Ein Nachtraining auf erweiterten Datensätzen wurde 
während der Entwicklung mehrfach durchgeführt, wobei lediglich die Gewichtsdatei 
ausgetauscht werden musste.

\textbf{NFA-010 (Eingabeformate):} Alle 7 Testpläne wurden als PDF-Dateien 
verarbeitet. Die interne Verarbeitungspipeline konvertiert PDFs zunächst 
in hochauflösende Rasterbilder (500 DPI, PNG-Format) mittels PyMuPDF 
(§\ref{Inferenz}), bevor die YOLO-Inferenz erfolgt. Damit wird die 
Bildverarbeitungsfähigkeit (PNG/JPG) implizit durch jeden PDF-Test validiert. 
Ein direkter Import von Bilddateien ohne PDF-Konvertierung wurde nicht 
explizit getestet, ist jedoch aufgrund der identischen nachgelagerten 
Pipeline-Stufen funktional äquivalent.

\textbf{NFA-011 (Datenquellen):} Der Testsatz besteht ausschließlich aus realen 
Siemens Mobility Gleisplänen verschiedener Projekte und Komplexitätsstufen 
(Tab.~\ref{tab:test_dataset_stats}). Dies validiert die Kompatibilität mit 
kundenspezifischen Datenquellen.

\textbf{NFA-012 (Ausgabeformate):} Das System unterstützt alle geforderten 
Ausgabeformate (Abbildung~\ref{fig:export_format_selection}):

\begin{itemize}
    \item \textbf{Excel (.xlsx):} Systematisch für alle 7 Testpläne validiert 
    (vgl. Abbildung~\ref{fig:export_result})
    \item \textbf{CSV (.csv):} Implementiert und über Export-Dialog auswählbar
    \item \textbf{JSON (.json):} Implementiert mit konfigurierbarer Struktur
\end{itemize}

Der primäre Evaluationsfokus lag auf dem Excel-Format, da dies das 
Standardformat für Engineering-Workflows bei Siemens Mobility darstellt. 
Die Funktionsfähigkeit der alternativen Formate wurde durch Entwicklungstests 
bestätigt.

\subsection{Validierung der Export- und Hilfsfunktionen}
\label{subsec:export_validation}

Die Anforderungen FA-009 bis FA-012 betreffen unterstützende Funktionen, deren 
ausführliche Evaluation den Rahmen dieser auf Objekterkennung und Texterkennung 
fokussierten Arbeit übersteigen würde. Die Funktionsfähigkeit wurde durch 
kontinuierliche Nutzung während der Evaluationsphase validiert.

\textbf{FA-009 (Excel-Integration) und FA-010 (Strukturerhalt):}

Der in §\ref{subsec:exportfunktionlität} beschriebene Export-Dialog wurde für 
alle 7 Testpläne erfolgreich genutzt. Abbildung~\ref{fig:export_result} zeigt 
ein Beispiel der exportierten Daten mit separaten Arbeitsblättern pro Objektklasse. 
Die resultierenden Excel-Dateien dienten als Grundlage für den Ground-Truth-Vergleich 
der E2E-Evaluation (vgl. Tabelle~\ref{tab:e2e_per_class_test}).


\textbf{FA-011 (Änderungsverfolgung):} Die Änderungsverfolgung wurde anhand eines 
realen Versionspaars des Plans validiert (Abbildung~\ref{fig:diff_dialog}). 
Das System erkannte automatisch 41 Änderungen zwischen den Planversionen A\_000 
und B\_000:

\begin{itemize}
    \item 21 hinzugefügte Elemente
    \item 15 gelöschte Elemente  
    \item 5 verschobene Elemente (mit quantifizierter Positionsänderung)
    \item 224 unveränderte Elemente
\end{itemize}

Die Ergebnisse wurden in eine strukturierte Excel-Datei exportiert 
(Abbildung~\ref{fig:diff_export}), die eine revisionssichere Dokumentation 
der Planänderungen ermöglicht. Die Funktionsfähigkeit wurde durch die korrekte Identifikation und Kategorisierung aller 41 Änderungen zwischen zwei realen Planversionen validiert. Eine quantitative Evaluation mit formaler Ground-Truth-Annotation (Precision/Recall der Änderungserkennung) wurde nicht durchgeführt, da dies über den Fokus der Arbeit auf Extraktionsgenauigkeit hinausgeht und die Plausibilität der Ergebnisse die Funktionsfähigkeit bereits bestätigt.

\textbf{Anmerkung zur Änderungshäufigkeit:} In der Praxis weisen Gleispläne 
zwischen Revisionen typischerweise nur wenige Änderungen auf, insbesondere 
bei den Kernklassen (Signale, GKS, GM-Blöcke), da diese sicherheitsrelevante 
Komponenten repräsentieren. Die beobachteten Koordinatenkorrekturen im 
Meterbereich entsprechen typischen Feinplanungsanpassungen. Eine umfangreiche 
quantitative Evaluation mit vielen Versionspaaren war daher nicht erforderlich 
-- die Funktionsfähigkeit wurde anhand des verfügbaren Versionspaar-Beispiels 
erfolgreich demonstriert.

\textbf{FA-012 (Visuelle Validierung):} Die Bounding-Box-Overlays 
(vgl. Abbildung~\ref{fig:complete_ui}) wurden während der gesamten 
Evaluationsphase zur manuellen Verifikation der Extraktionsergebnisse verwendet 
und funktionierten zuverlässig.
\section{Validierung der funktionalen Anforderungen}
\label{sec:anforderungen_validierung}

Tabelle~\ref{tab:anforderungen_erfuellt} fasst die Erfüllung der in Kapitel~\ref{chap:anforderungen} definierten funktionalen Anforderungen zusammen.

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
FA-001 & Erkennungsrate $\geq$ 90\% & \checkmark (95.7\% Val, 100\% Test) \\
FA-002 & Rotationsinvarianz & \checkmark (100\% bei $|\theta|>30°$) \\
FA-003 & Zielobjekte detektierbar & \checkmark \\
FA-004 & OCR-Genauigkeit & \checkmark (integriert in 98.76\% E2E) \\
FA-005 & OCR-Robustheit & \checkmark (100\% bei $|\theta|>30°$) \\
FA-006 & Fahrtrichtungsdetektion & \checkmark (99.42\%) \\
FA-007 & Symbol-Koordinaten-Verknüpfung & \checkmark (99.69\%)\\
FA-008 & Manuelle Korrektur & \checkmark \\
FA-009 & Excel-Integration & \checkmark \\
FA-010 & Strukturerhalt & \checkmark \\
FA-011 & Änderungsverfolgung & \checkmark \\
FA-012 & Visuelle Validierung & \checkmark \\
FA-013 & GUI & \checkmark \\
FA-014 & Modularität & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der funktionalen Anforderungen}
\label{tab:anforderungen_erfuellt}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Anf.-ID} & \textbf{Anforderung} & \textbf{Erfüllt} \\
\hline
NFA-001 & On-Premise-Verarbeitung & \checkmark \\
NFA-002 & Lizenzkonformität & \checkmark \\
NFA-003 & Gesamtsystem-Genauigkeit $\geq$ 85\% & \checkmark (98.76\%) \\
NFA-004 & Robustheit & \checkmark \\
NFA-005 & Prüfbarkeit & \checkmark \\
NFA-006 & Prozessoptimierung & \checkmark \\
NFA-007 & Ressourceneffizienz & \checkmark \\
NFA-008 & Erweiterbarkeit & \checkmark \\
NFA-009 & Update-Fähigkeit & \checkmark \\
NFA-010 & Eingabeformate & \checkmark \\
NFA-011 & Datenquellen & \checkmark \\
NFA-012 & Ausgabeformate & \checkmark \\
\hline
\end{tabular}
\caption{Validierung der nicht-funktionalen Anforderungen}
\label{tab:nfa_erfuellt}
\end{table}

\section{Zusammenfassung der Evaluationsergebnisse}
\label{sec:eval_zusammenfassung}

Die systematische Evaluation des entwickelten Prototyps auf einem unabhängigen Testsatz realer Siemens Mobility Gleispläne belegt die exzellente Funktionsfähigkeit und Praxistauglichkeit des Systems. Die wichtigsten Ergebnisse lassen sich wie folgt zusammenfassen:

\textbf{Objekterkennung (YOLO):}
\begin{itemize}
    \item Exzellente Detektionsleistung mit mAP@0.5 von 98.0\% auf dem Validierungssatz
    \item 100\% Detektionsrate auf dem Testsatz (kein Symbol übersehen)
    \item 1 Klassifikationsfehler (GKS-Typ-Verwechslung) auf dem Testsatz
    \item Robuste Rotationsinvarianz: Steilrotierte Objekte ($|\theta|>30°$) erreichen 
    100\% OCR-Erfolg bei höherer Konfidenz (0.946 vs. 0.890)
    \item Erfolgreiche Anforderungserfüllung FA-001 (Recall 95.7\% $>$ 90\%) und FA-002 (Rotationsinvarianz)
\end{itemize}

\textbf{Symbol-Text-Verknüpfung:}
\begin{itemize}
    \item Sehr hohe Linking-Genauigkeit: 642 von 644 Verknüpfungen korrekt (99.69\%)
    \item 2 Linking-Fehler aufgetreten:
    \begin{itemize}
        \item 1× GM-Koordinate an atypischer Position (rechts statt unterhalb)
        \item 1× Fahrtrichtung nicht abgeleitet (fehlende GKS-Detektion)
    \end{itemize}
    \item Fahrtrichtungsdetektion: 171 von 172 Signalen korrekt (99.42\%)
    \item Proximity-basierter Algorithmus robust für typische Layouts
\end{itemize}

\textbf{End-to-End Systemleistung:}
\begin{itemize}
    \item Gesamtgenauigkeit von \textbf{98.76\%} auf dem Testsatz übertrifft Zielwert von 85\% (NFA-003) deutlich
    \item Nur 8 von 644 Objekten (1.24\%) erforderten Korrektur
    \item Fehlerverteilung: 3 YOLO-bezogen (37.5\%), 3 OCR-bezogen (37.5\%), 2 Linking-bezogen (25.0\%)
    \item Stabile Leistung über alle Komplexitätsstufen (98.23\% -- 99.31\%)
    \item Für produktiven Einsatz auf diversen Plänen wird realistische Genauigkeit von 90--95\% erwartet
    \item Umfangreiche Validierungs- und Korrekturwerkzeuge reduzieren Prüfaufwand um 85\% durch gezielte Fehleridentifikation
\end{itemize}

\textbf{Verarbeitungseffizienz und Praxisnutzen:}
\begin{itemize}
    \item Durchschnittliche Verarbeitungszeit von \textbf{12.3 Minuten} pro A0-Plan auf Standard-CPU-Hardware
    \item Zeitverteilung der Pipeline: YOLO-Inferenz 75.3\%, OCR 10.3\%, Linking 6.8\%, Sonstige 7.7\%
    \item \textbf{75.3\% Zeitersparnis} gegenüber manuellem Prozess (64 min $\rightarrow$ 15.8 min pro Plan)
    \item Bei typischen Projekten mit 20--50 Plänen: 16--40 Stunden (2--5 Arbeitstage) Einsparung
    \item Anforderungen NFA-006 (Prozessoptimierung) und NFA-007 (Ressourceneffizienz) erfüllt
\end{itemize}

Die Evaluation bestätigt, dass das entwickelte System alle definierten funktionalen und nicht-funktionalen Anforderungen erfüllt und die gesetzten Zielmetriken signifikant übertrifft. Der verbleibende manuelle Korrekturaufwand von geschätzt 5--10\% im produktiven Einsatz wird durch die integrierten Validierungswerkzeuge effizient adressiert. Die erzielte Zeitersparnis von 75\% transformiert den bisherigen manuellen Prozess zu einem KI-gestützten Workflow, der sowohl die Effizienz als auch die Konsistenz der Datenextraktion erheblich verbessert. Der primäre technische Optimierungspotenzial liegt in der OCR-Komponente, insbesondere bei der Erkennung von Koordinatenbeschriftungen unter ungünstigen Bedingungen (niedrige Auflösung, starke Rotation). Diese Aspekte werden in Kapitel~\ref{chap:diskussion} detailliert diskutiert.