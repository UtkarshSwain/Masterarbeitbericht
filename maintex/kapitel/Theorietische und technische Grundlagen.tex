\chapter{Theoretische und technische Grundlagen}

Im Rahmen dieses Kapitels werden die maßgeblichen Technologien und Konzepte dargelegt, auf denen das entwickelte Modell basiert. Der Fokus liegt auf der automatisierten Objekterkennung, Textextraktion und strukturierten Informationsverarbeitung aus Gleisplänen.
\section{Grafische Gleispläne im Bahnkontext}
Die Modellierung eines Bahnhofs stellt eine komplexe Aufgabe dar, da ein solcher in der Regel aus Hunderten von Gleisen und Weichen sowie zahlreichen Signalen besteht. 
Die manuelle Erstellung eines Bahnhofsmodells, das den physischen Details eines Bahnhofs entspricht, erfordert in der Regel mehrere Wochen Arbeit. Dies hat signifikante 
Auswirkungen auf die Anwendbarkeit und den Umfang vieler Methoden.\cite{railroadstationdrawing}


Diese zeitintensive manuelle Arbeit motiviert den Einsatz automatisierter Verfahren zur Datenextraktion. Die folgenden Abschnitte legen die theoretischen Grundlagen dar, auf denen eine solche Automatisierung aufbauen kann.

\subsection{Grundlagen des Gleisplans}
Der Gleisplan bildet die fundamentale Datengrundlage für die Planung, den Bau und den operativen Betrieb von Bahnanlagen. Er repräsentiert die technische Infrastruktur und dient als visuelle Schnittstelle zwischen der physischen Außenanlage und der sicherungstechnischen Logik (Stellwerk).
\\
Gleispläne existieren in zwei Darstellungsformen \cite{Pachl}: maßstäbliche 
Lagepläne (topografisch, bautechnisch) und schematische Übersichtspläne 
(topologisch, sicherungstechnisch). Diese Arbeit fokussiert letztere, da sie 
die logischen Fahrwegbeziehungen priorisieren und die Grundlage für die Stellwerksplanung bilden.

Gleispläne enthalten drei Symbolkategorien: (1) Fahrwegelemente (Gleise, Weichen), 
(2) Sicherungstechnik (Signale, Achszähler), (3) Metadaten (Bezeichner, 
Kilometrierung). Jedes Element trägt eindeutige Labels zur Identifikation.

Obwohl diese Pläne in der heutigen Ingenieurspraxis mittels CAD-Systemen (Computer Aided Design) erstellt werden, liegt die primäre Herausforderung für eine automatisierte Auswertung nicht im Dateiformat, sondern in der korrekten semantischen Interpretation dieser abstrahierten Symbolik und ihrer logischen Verknüpfung \cite{Pachl}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/gleisplandrawio.png} 
    \caption{Ausschnitt eines schematischen Gleisplans mit Darstellung von Signalen, Weichen und Gleisbezeichnungen (Beispielhafte Darstellung) aus Quelle \cite{railroadstationdrawing}}
    \label{fig:gleisplan_schema}
\end{figure}

\subsection{Bahntechnische Symbole und ihre Klassifikation}
\label{sec:Bahntechnischesymbole}
In den Gleisplänen kommen zahlreiche Symbole zum Einsatz, um die verschiedenen technischen Objekte, Anlagenkomponenten und zusätzlichen Informationen übersichtlich darzustellen. Diese Symboldarstellung dient nicht nur der Visualisierung der Gleiskomponenten, sondern bildet auch die Grundlage für Planung, Projektierung, Dokumentation und spätere Anwendung. \cite{signallayout} Da der Gleisplan eine zentrale Schnittstelle zwischen technischer Planung, 
betrieblicher Umsetzung und sicherheitstechnischen Systemen darstellt, ist eine präzise, klare und einheitliche Symbolklassifizierung notwendig. Die für diese Arbeit relevanten Symbolklassen gliedern sich in drei funktionale 
Kategorien (siehe Tabelle \ref{tab:symbol_categories}):

\begin{table}[h]
\centering
\begin{tabular}{|p{3cm}|p{12cm}|}
\hline
\textbf{Kategorie} & \textbf{Elemente und Funktion} \\
\hline
Infrastruktur & \textbf{Signale}: Fahrterlaubnis und Geschwindigkeitsvorgabe. 
\textbf{Weichen}: Fahrwegverzweigungen. \textbf{Isolierstöße}: Elektrische 
Gleistrennung. \textbf{Achszähler}: Belegungserkennung. \\
\hline
Zugbeeinflussung & \textbf{Balisen}: Punktförmige Datenübertragung an Zug. 
\textbf{Gleismagnete}: Induktive Informationsübertragung. \textbf{Gleiskoppelspulen}: 
Elektromagnetische Kopplung. \\
\hline
Metadaten & \textbf{Positionsangaben}: Kilometrierung, Lagepunkte. 
\textbf{Gleisabschnittsnamen}: Eindeutige Gleisidentifikation. \\
\hline
\end{tabular}
\caption{Klassifikation bahntechnischer Symbole}
\label{tab:symbol_categories}
\end{table}

Die detaillierte visuelle Darstellung dieser Symbole erfolgt in Abschnitt 4.1.
\section{Objekterkennung in Bildern}
\label{sec:objekterkennung}

Die automatisierte Analyse visueller Informationen bildet das Fundament zahlreicher technischer Anwendungen von der Qualitätskontrolle in der industriellen Fertigung über die medizinische Bildanalyse bis hin zur autonomen Navigation. Während Menschen mühelos in der Lage sind, Objekte in Bildern zu identifizieren und zu lokalisieren, erfordert die Replikation dieser Fähigkeit in Maschinen die Lösung komplexer Teilprobleme. Historisch wurde diese Aufgabe durch explizite Programmierung von Erkennungsregeln adressiert. Der konzeptionelle Durchbruch gelang durch maschinelles Lernen, insbesondere durch tiefe neuronale Netze, die relevante Merkmale selbstständig aus Trainingsdaten extrahieren können \cite{LeCun1998_GradientBasedLearning,Krizhevsky2012_AlexNet}.

Für technische Anwendungen wie die Analyse von Konstruktionszeichnungen oder Gleisplänen manifestieren sich spezifische Herausforderungen, die über die Verarbeitung natürlicher Fotografien hinausgehen. Im Folgenden werden zunächst die domänenspezifischen Anforderungen technischer Zeichnungen erläutert, bevor die grundlegenden methodischen Ansätze der Objekterkennung systematisch hergeleitet werden. Das Kapitel folgt dabei einer Progression vom Allgemeinen zum Spezifischen: Ausgehend von universellen Erkennungsprinzipien wird schrittweise die Anwendung auf bahntechnische Gleispläne konkretisiert.

\subsection{Objekterkennung in technischen CAD-Zeichnungen}
\label{subsec:cad_spezifisch}

Im Gegensatz zu natürlichen Szenen weisen technische Konstruktionszeichnungen spezifische Charakteristika auf, die dedizierte Lösungsansätze erfordern. Diese domänenspezifischen Besonderheiten haben direkten Einfluss auf die Wahl geeigneter Erkennungsverfahren und definieren die in Kapitel \ref{chap:anforderungen} formulierten funktionalen Anforderungen an das System.

\subsubsection{Domänenspezifische Eigenschaften}

Technische Zeichnungen unterscheiden sich fundamental von natürlichen Bildern in ihrer Entstehung und Struktur. CAD-Systeme (Computer-Aided Design) erzeugen vektorbasierte Repräsentationen, die geometrische Primitive wie Linien, Kreise und Polylinien sowie zugeordnete Metadaten enthalten. Die Konvertierung in Rasterformate (PDF, PNG) für ML-basierte Analyse führt jedoch zu einem Informationsverlust hinsichtlich topologischer Beziehungen und semantischer Layer-Informationen. Topologische Beziehungen beschreiben die Verbindungsstruktur zwischen Elementen -- etwa welche Gleisabschnitte durch eine Weiche verbunden sind oder welches Signal zu welchem Gleis gehört. Semantische Layer-Informationen kodieren die funktionale Zuordnung von Objekten zu Kategorien wie Gleisführung, Signaltechnik oder Oberleitung. Bei der Rasterisierung werden diese Strukturinformationen eliminiert; das Bild wird zu einer Ansammlung von Pixeln ohne inhärente Bedeutung. Diese Transformation von strukturierten zu unstrukturierten Daten motiviert den Einsatz von Deep-Learning-Verfahren, die robuste Mustererkennung auch ohne explizite topologische Information ermöglichen.

Bahntechnische Symbole folgen standardisierten Bibliotheken gemäß EN-Normen oder kundenspezifischen Richtlinien. Die Variabilität innerhalb einer Symbolklasse ist somit deutlich geringer als bei natürlichen Objekten wie Fahrzeugen oder Personen, jedoch erschweren kundenspezifische Anpassungen die Generalisierung über verschiedene Planungsunternehmen hinweg. Ein durchschnittlicher Bahnhofsgleisplan enthält zwischen 500 und 2000 individuelle Symbole auf einer Fläche von wenigen Quadratmetern, was eine deutlich höhere Objektdichte als in typischen Computer-Vision-Benchmark-Datensätzen wie COCO \cite{Lin2014_COCO} oder Pascal VOC \cite{Everingham2010_PascalVOC} darstellt. Zum Vergleich: COCO-Bilder enthalten im Durchschnitt 7 bis 10 Objekte, während ein Gleisplanausschnitt von vergleichbarer Pixelgröße 50 bis 100 Symbole aufweisen kann.

Anders als bei Objekten mit kanonischer Orientierung -- etwa Straßenverkehrsschilder (typischerweise frontal) oder Fußgänger (vertikal stehend) -- können bahntechnische Symbole und Textannotationen entlang der Gleisachsen in beliebigen Winkeln orientiert sein. Ein Weichensymbol kann beispielsweise in 16 verschiedenen Rotationen auftreten, die jeweils 22,5° Schritte zwischen 0° und 360° abdecken. Signale werden parallel zur Gleisachse positioniert, Gleisnummern folgen der Krümmung des Gleises. Diese Rotationsvariabilität ohne bevorzugte Ausrichtung erfordert rotationsinvariante Detektionsarchitekturen, deren Notwendigkeit in Abschnitt \ref{sec:obb} detailliert erläutert wird.

\subsubsection{Stand der Forschung}

Die Anwendung von Machine Learning auf technische Zeichnungen ist ein aktives Forschungsfeld mit Anwendungen in verschiedenen Ingenieurdisziplinen. Ahmed et al. \cite{Ahmed2011} entwickelten frühe Ansätze zur automatischen Segmentierung von architektonischen Grundrissen unter Verwendung morphologischer Operationen und Hough-Transformation. Moreno-García et al. \cite{Moreno2019} entwickelten ein umfassendes Framework zur Digitalisierung komplexer technischer Zeichnungen und demonstrierten die Überlegenheit von Deep-Learning-Ansätzen gegenüber klassischen Feature-Engineering-Methoden. Rahul et al. \cite{Rahul2019} extrahierten Informationen aus P\&ID-Diagrammen der Prozessindustrie mittels Faster R-CNN und erreichten dabei mean Average Precision-Werte von über 85\%.

Jamieson et al. \cite{Jamieson2024} liefern eine umfassende Übersicht über Deep-Learning-Methoden für Engineering-Diagramme und identifizieren als zentrale Herausforderungen die hohe Variabilität kundenspezifischer Symbolbibliotheken, den Mangel an annotierten Trainingsdaten sowie die Notwendigkeit rotationsinvarianter Detektionsverfahren. Für den bahntechnischen Bereich existieren bisher hauptsächlich proprietäre Lösungen einzelner Eisenbahnunternehmen ohne publizierte wissenschaftliche Evaluierung. Die vorliegende Arbeit adressiert diese Forschungslücke durch Entwicklung und Evaluation eines prototypischen Systems auf Basis moderner One-Stage-Detektoren.

\subsection{Paradigmen der Objekterkennung}
\label{subsec:paradigmen}

Die historische Entwicklung der Objekterkennung lässt sich in drei methodische Hauptströmungen unterteilen, die sich hinsichtlich ihrer theoretischen Fundierung und praktischen Anwendbarkeit signifikant unterscheiden \cite{Zou2019_ObjectDetectionSurvey,Liu2020_DeepLearningDetectionSurvey}. Diese Evolution reflektiert den Übergang von expliziter Programmierung über datengetriebenes Lernen hin zu hierarchischer Merkmalsrepräsentation.

\subsubsection{Regelbasierte Verfahren}

Frühe Ansätze basierten auf der Anwendung manuell entwickelter Algorithmen. Template Matching schiebt ein Referenzmuster pixelweise über das Zielbild und quantifiziert die Ähnlichkeit mittels Korrelationsmetriken \cite{Brunelli2009}. Fortgeschrittenere Varianten wie SIFT (Scale-Invariant Feature Transform) oder SURF (Speeded Up Robust Features) extrahieren lokale Merkmale mit gewisser Robustheit gegenüber Skalierungs- und Beleuchtungsvariationen \cite{Lowe2004,Bay2008}.

Die fundamentale Limitierung liegt in der mangelnden Generalisierungsfähigkeit. Da die Merkmalsdefinition a priori erfolgen muss, versagen diese Ansätze bei unvorhergesehenen Variationen. Für Gleispläne mit mehr als 13 Symbolklassen in beliebigen Rotationszuständen ist eine erschöpfende Enumeration aller Template-Varianten computationell nicht handhabbar. Ein Weichensymbol in 16 Rotationen bei 3 Maßstäben würde bereits 48 Templates erfordern; für 13 Klassen summiert sich dies auf über 600 Varianten. Zudem erfordern kundenspezifische Anpassungen der Symbolbibliothek eine manuelle Neudefinition der Templates.

\subsubsection{Maschinelles Lernen mit konstruierten Merkmalen}

Verfahren wie Support Vector Machines (SVM) \cite{Cortes1995_SVM} in Kombination mit Histogram of Oriented Gradients (HOG) Features \cite{Dalal2005} lernen Erkennungslogik aus Trainingsdaten. Der fundamentale Unterschied zu regelbasierten Ansätzen liegt in der Adaptivität: Das System lernt selbstständig relevante Merkmalskombinationen aus annotierten Beispielen. Dennoch verbleibt die Merkmalsextraktion (Feature Engineering) als manuelle Aufgabe. Ein Ingenieur muss entscheiden, welche Bildcharakteristika -- etwa Gradientenrichtungen, Farbhistogramme oder Texturstatistiken -- für die jeweilige Domäne relevant sind \cite{Lowe2004,Dalal2005}. Die Güte des Gesamtsystems ist direkt an die Qualität dieser handdefinierten Merkmalsrepräsentation gekoppelt. Bei technischen Zeichnungen mit ihrer Kombination aus geometrischer Präzision, variabler Linienstärke und dichtem Layout erreichen diese Verfahren schnell ihre Leistungsgrenzen.

\subsubsection{Deep Learning und End-to-End-Lernen}

Moderne Ansätze basieren auf Convolutional Neural Networks (CNNs), die hierarchische Repräsentationen direkt aus Rohdaten lernen \cite{LeCun1998_GradientBasedLearning,Krizhevsky2012_AlexNet}. Das fundamentale Designprinzip ist die automatische Merkmalsentdeckung: Anstelle expliziter Regeln oder vordefinierter Feature-Deskriptoren extrahiert das Netz Features selbstständig durch wiederholte Anwendung von Faltungsoperationen. 

Die Funktionsweise basiert auf hierarchischer Merkmalskomposition. Die ersten Schichten eines CNN reagieren auf einfache visuelle Muster wie Linien in unterschiedlichen Orientierungen. Diese werden durch kleine Faltungsfilter (typischerweise 3×3 oder 5×5 Pixel) detektiert \cite{Zeiler2014_Visualizing}. Höhere Schichten kombinieren diese Basismuster zu komplexeren Strukturen: aus Linien werden Ecken und Kurven, aus diesen werden geometrische Formen, und schließlich emergieren symbolspezifische Muster wie die charakteristische T-Form einer Weiche oder die kreisförmige Struktur eines Signals. Das Netz lernt diese Hierarchie selbstständig, indem es während des Trainings annotierte Beispiele analysiert und seine internen Parameter (Filtergewichte) so anpasst, dass Vorhersagefehler minimiert werden \cite{LeCun1998_GradientBasedLearning}.

Abbildung \ref{fig:cnn_feature_hierarchy} verdeutlicht diese progressive Abstraktion. Die erste Schicht detektiert lediglich Linienorientierungen mittels kleiner 3×3 Filter. Die zweite Schicht kombiniert diese Basismuster zu geometrischen Elementen wie Ecken, Kurven und Kreisen. In der dritten Schicht entstehen komplexere Formen wie Rechtecke, T-Formen und Pfeile. Schließlich emergieren in der finalen Schicht symbolspezifische Repräsentationen wie Weichen, Signale oder Gleisabschnitte. Diese zunehmende Komplexität der Features wird durch wiederholte Faltungsoperationen erreicht, wobei jede Schicht auf den Ausgaben der vorherigen aufbaut.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.8cm and 1.2cm,
    % STYLES
    layer/.style={
        rectangle, draw, thick, 
        minimum width=2.0cm, minimum height=4.0cm, 
        align=center
    },
    feature/.style={
        rectangle, draw, 
        minimum width=1.6cm, minimum height=0.6cm, 
        align=center, font=\tiny,
        fill=white!90!gray, 
        drop shadow 
    },
    layer_title/.style={font=\bfseries\scriptsize, align=center},
    layer_sub/.style={font=\tiny, align=center, color=black!70},
    arrow/.style={->, >=stealth, thick}
]

    % ==========================
    % 1. INPUT (Kleiner als die anderen)
    % ==========================
    \node[layer, fill=blue!15, minimum height=3cm] (input) {};
    \node[above=0.2cm of input.center] (in_img) {
        \tikz{\draw[thick, fill=white] (0,0) rectangle (0.8,0.8); \draw[thick, red] (0.4,0.4) circle (0.2);}
    };
    \node[above=0.1cm of in_img, font=\small] {Eingabe};
    \node[below=0.1cm of in_img, font=\tiny, align=center] {Gleisplan\\1024$\times$1024};

    % ==========================
    % 2. LAYER 1
    % ==========================
    \node[layer, fill=yellow!20, right=of input] (layer1) {};
    \node[layer_title, below=0.1cm of layer1.north] (t1) {Schicht 1};
    \node[layer_sub, below=0.0cm of t1] (st1) {Einfache\\Muster};
    
    \node[feature, below=0.2cm of st1] (f1) {Horizontal};
    \node[feature, below=0.1cm of f1] (f2) {Vertikal};
    \node[feature, below=0.1cm of f2] (f3) {Diagonal};

    % ==========================
    % 3. LAYER 2
    % ==========================
    \node[layer, fill=orange!20, right=of layer1] (layer2) {};
    \node[layer_title, below=0.1cm of layer2.north] (t2) {Schicht 2};
    \node[layer_sub, below=0.0cm of t2] (st2) {Geometrie};
    
    \node[feature, below=0.2cm of st2] (f4) {Ecken};
    \node[feature, below=0.1cm of f4] (f5) {Kurven};
    \node[feature, below=0.1cm of f5] (f6) {Kreise};

    % ==========================
    % 4. LAYER 3
    % ==========================
    \node[layer, fill=red!20, right=of layer2] (layer3) {};
    \node[layer_title, below=0.1cm of layer3.north] (t3) {Schicht 3};
    \node[layer_sub, below=0.0cm of t3] (st3) {Formen};
    
    \node[feature, below=0.2cm of st3] (f7) {Rechteck};
    \node[feature, below=0.1cm of f7] (f8) {T-Form};
    \node[feature, below=0.1cm of f8] (f9) {Pfeil};

    % ==========================
    % 5. OUTPUT
    % ==========================
    \node[layer, fill=green!20, right=of layer3] (output) {};
    \node[layer_title, below=0.1cm of output.north] (t4) {Ausgabe};
    \node[layer_sub, below=0.0cm of t4] (st4) {Symbole};
    
    \node[feature, below=0.2cm of st4] (s1) {Weiche};
    \node[feature, below=0.1cm of s1] (s2) {Signal};
    \node[feature, below=0.1cm of s2] (s3) {Gleis};

    % ==========================
    % ARROWS & LABELS
    % ==========================
    \draw[arrow] (input) -- (layer1);
    \draw[arrow] (layer1) -- (layer2);
    \draw[arrow] (layer2) -- (layer3);
    \draw[arrow] (layer3) -- (output);

    \node[above=0.1cm of layer1, font=\scriptsize] {3$\times$3 Filter};
    \node[above=0.1cm of layer2, font=\scriptsize] {Kombination};
    \node[above=0.1cm of layer3, font=\scriptsize] {Abstraktion};

    % --- HIER IST DER FIX ---
    % Wir nutzen (input |- output.south): X von Input, Y von Output.south
    \draw[->, >=stealth, very thick, blue!60!black] 
        ([yshift=-0.5cm] input |- output.south) -- ([yshift=-0.5cm] output.south)
        node[midway, below, font=\small] {Zunehmende Komplexität der Features};

\end{tikzpicture}
\caption{Hierarchische Merkmalsextraktion in Convolutional Neural Networks.}
\label{fig:cnn_feature_hierarchy_straight}
\end{figure}
Diese End-to-End-Lernfähigkeit ermöglicht eine Robustheit, die mit konventionellen Verfahren nicht erreichbar ist. Für technische Zeichnungen erweisen sich Deep-Learning-Architekturen als überlegen, da sie sowohl exakte Lokalisierung als auch semantische Klassifikation integriert durchführen können, ohne dass ein Ingenieur domänenspezifische Features manuell definieren muss \cite{Girshick2014_RCNN,Ren2015_FasterRCNN}.

\subsection{Architekturen für Deep-Learning-basierte Objekterkennung}
\label{subsec:dl_architekturen}

Die Implementierung von CNN-basierten Objektdetektoren lässt sich in zwei Architekturparadigmen unterteilen, die sich in ihrer Balance zwischen Detektionsgenauigkeit und Recheneffizienz unterscheiden. Die Wahl der Architektur hat direkte Auswirkungen auf die Praxistauglichkeit des Systems.

\subsubsection{Zweistufige vs. einstufige Detektoren}

Zweistufige Detektoren wie Faster R-CNN \cite{Ren2015_FasterRCNN} separieren den Erkennungsprozess konzeptionell in zwei aufeinanderfolgende Phasen. Zunächst generiert ein Region Proposal Network (RPN) zwischen 100 und 300 Kandidatenregionen, die potenziell Objekte enthalten könnten. Diese Regionen werden anschließend durch ein separates Klassifikationsnetzwerk evaluiert, das sowohl die Objektklasse bestimmt als auch die Bounding-Box-Koordinaten verfeinert. Der Vorteil liegt in hoher Detektionspräzision, insbesondere bei kleinen oder teilweise verdeckten Objekten. Die sequenzielle Verarbeitung hunderter Kandidatenregionen führt jedoch zu Bildraten von lediglich 5 bis 10 Bildern pro Sekunde (Frames Per Second, FPS) auf moderner GPU-Hardware (Graphics Processing Unit).

Für die Gleisplananalyse erweist sich diese Geschwindigkeit als kritisch. Ein durchschnittlicher Plan umfasst nach der Kachelungsstrategie (siehe Abschnitt \ref{subsec:tiling_strategie}) zwischen 150 und 300 Bildausschnitte. Während YOLO-Modelle typischerweise mit Eingangsgrößen von 640×640 oder 1024×1024 Pixeln arbeiten, werden für die Gleisplananalyse aufgrund der sehr kleinen Symbolgrößen Kacheln von 2048×2048 Pixeln verwendet (Details zur Kachelungsstrategie in Abschnitt \ref{subsec:tiling_strategie}). Bei einer Verarbeitungszeit von 100 bis 200 Millisekunden pro Kachel würde die Gesamtanalyse zwischen 15 Sekunden und 4 Minuten dauern. Dies widerspricht der in Kapitel \ref{chap:anforderungen} definierten Anforderung einer interaktiven Benutzeroberfläche, die Analyseergebnisse innerhalb von maximal 10 Sekunden präsentieren soll.

Einstufige Detektoren wie YOLO (You Only Look Once) \cite{redmon2016lookonceunifiedrealtime} adressieren diese Geschwindigkeitsproblematik durch eine fundamental andere Strategie. YOLO unterteilt das Eingabebild in ein regelmäßiges Raster von Zellen (beispielsweise 13×13 oder 26×26 Zellen bei kleineren Eingabegrößen, bzw. entsprechend größere Raster bei höheren Auflösungen). Für jede Zelle sagt das Netz in einem einzigen Vorwärtsdurchlauf direkt mehrere Aspekte voraus: Liegt in dieser Zelle ein Objekt? Wenn ja, welcher Objektklasse gehört es an? Wie hoch ist die Konfidenz dieser Vorhersage? Und wo genau innerhalb der Zelle befindet sich das Objekt? Diese parallele Verarbeitung aller Bildbereiche ermöglicht Inferenzgeschwindigkeiten von 40 bis über 100 Bildern pro Sekunde \cite{yolov8_ultralytics}. Moderne Varianten wie YOLOv8 erreichen durch architektonische Verbesserungen -- insbesondere Feature Pyramid Networks (FPN) \cite{Lin2017_FPN} für Multi-Scale-Detektion und verbesserte Loss-Funktionen -- vergleichbare oder sogar überlegene Präzision gegenüber zweistufigen Detektoren bei Beibehaltung der Echtzeitfähigkeit.

Die Abwägung zwischen beiden Paradigmen muss im Kontext der Anwendungsanforderungen erfolgen. Für technische Gleispläne mit gut sichtbaren, scharf gerenderten Symbolen rechtfertigt die hohe Verarbeitungsgeschwindigkeit marginale Genauigkeitseinbußen. Die in Kapitel \ref{chap:evaluation} dargestellten Ergebnisse bestätigen, dass eine mean Average Precision von über 98\% mit einstufigen Architekturen erreichbar ist, bei gleichzeitiger Gesamtverarbeitungszeit von etwa 5 Sekunden pro Plan.

Abbildung \ref{fig:two_stage_vs_one_stage} illustriert den fundamentalen Unterschied zwischen beiden Architekturparadigmen. Faster R-CNN generiert zunächst mehrere hundert Kandidatenregionen mittels RPN, die anschließend sequenziell klassifiziert werden müssen. YOLO hingegen sagt in einem einzigen Vorwärtsdurchlauf direkt die Bounding Boxes und Klassen voraus, was zu einer Beschleunigung um den Faktor 5 führt. Für die Analyse eines typischen Gleisplans mit 200 gekachelten Bildausschnitten (je 2048×2048 Pixel) reduziert sich die Gesamtverarbeitungszeit von etwa 4 Minuten (Faster R-CNN) auf unter 5 Sekunden (YOLO).

% ABBILDUNG 3.2: Two-Stage vs One-Stage
\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, thick, minimum width=2.8cm, minimum height=1cm, align=center, font=\footnotesize},
    arrow/.style={->, >=stealth, thick}
]
    % LEFT: Two-Stage
    \node[box, fill=blue!15] (img1) {Eingabebild\\$1024 \times 1024$ px};
    \node[box, fill=yellow!25, below=of img1] (rpn) {Region Proposal\\Network\\$\sim$300 Kandidaten};
    \node[box, fill=orange!20, below=of rpn] (class1) {Klassifikation\\pro Region};
    \node[box, fill=green!20, below=of class1] (out1) {Detektionen};
    
    \draw[arrow] (img1) -- (rpn);
    \draw[arrow] (rpn) -- (class1);
    \draw[arrow] (class1) -- (out1);
    
    \node[left=0.6cm of rpn, align=right, font=\scriptsize\bfseries] (label1) {Two-Stage\\(Faster R-CNN)};
    \node[below=0.15cm of label1, align=right, font=\tiny, text width=2.2cm] {$\sim$8 FPS\\Hohe Präzision\\4 Min/Gleisplan};
    
    % RIGHT: One-Stage
    \begin{scope}[xshift=6.5cm]
        \node[box, fill=blue!15] (img2) {Eingabebild\\$1024 \times 1024$ px};
        \node[box, fill=green!20, below=3.2cm of img2] (direct) {Direkte Vorhersage\\(Grid-basiert)};
        \node[box, fill=green!20, below=of direct] (out2) {Detektionen};
        
        \draw[arrow] (img2) -- (direct);
        \draw[arrow] (direct) -- (out2);
        
        \node[right=0.6cm of direct, align=left, font=\scriptsize\bfseries] (label2) {One-Stage\\(YOLO)};
        \node[below=0.15cm of label2, align=left, font=\tiny, text width=2.2cm] {$\sim$40 FPS\\Echtzeitfähig\\5 Sek/Gleisplan};
    \end{scope}
    
    % Comparison arrow
    \draw[<->, >=stealth, thick, red!70!black] (out1.south) ++ (0,-0.4) -- ++ (6.5,0) 
        node[midway, below, font=\scriptsize, align=center] {Faktor 5 schneller};
\end{tikzpicture}
\caption{Architekturvergleich zwischen zweistufigen und einstufigen Objektdetektoren.}
\label{fig:two_stage_vs_one_stage}
\end{figure}

\subsection{Training und Optimierung}
\label{subsec:training_optimization}

Die Leistungsfähigkeit neuronaler Netze hängt kritisch vom Trainingsprozess ab, durch den die internen Parameter (Gewichte) des Netzes aus annotierten Beispieldaten gelernt werden. Dieser Abschnitt erläutert die fundamentalen Mechanismen des Trainings und deren Implikationen für die Gleisplanerkennung.

\subsubsection{Supervised Learning und Loss-Funktionen}

Das Training erfolgt im Supervised-Learning-Paradigma \cite{Goodfellow2016_DeepLearning}: Dem Netz werden Eingabebilder zusammen mit korrekten Annotationen (Ground Truth) präsentiert. Für Objektdetektion umfassen diese Annotationen sowohl die Bounding-Box-Koordinaten als auch die Klassenbezeichnung jedes Objekts. Das Netz generiert Vorhersagen, die mit der Ground Truth verglichen werden. Die Diskrepanz wird durch eine Loss-Funktion quantifiziert, die typischerweise mehrere Komponenten kombiniert:

Der Klassifikations-Loss misst, wie gut die vorhergesagte Objektklasse mit der tatsächlichen Klasse übereinstimmt. Üblich ist die Cross-Entropy-Loss \cite{Goodfellow2016_DeepLearning}, die hohe Konfidenz für korrekte Klassen belohnt und niedrige Konfidenz für falsche Klassen fordert. Die Lokalisierungs-Loss quantifiziert die Abweichung zwischen vorhergesagter und tatsächlicher Bounding Box, häufig implementiert als Smooth L1-Loss über die Box-Koordinaten \cite{Ren2015_FasterRCNN}. Moderne Detektoren wie YOLOv8 verwenden zusätzlich IoU-basierte Loss-Komponenten \cite{Rezatofighi2019_GIoU}, die direkt den Überlappungsgrad optimieren anstatt nur Koordinatenabweichungen zu minimieren.

Der Trainingsprozess iteriert über den Datensatz in kleinen Batches (typischerweise 8 bis 32 Bilder). Für jedes Batch wird der Loss berechnet und mittels Backpropagation \cite{Rumelhart1986_Backpropagation} -- einem effizienten Algorithmus zur Berechnung von Gradienten -- ermittelt, wie jedes Gewicht im Netz angepasst werden muss, um den Loss zu reduzieren. Ein Optimierer wie SGD (Stochastic Gradient Descent) \cite{Robbins1951_SGD} oder Adam \cite{Kingma2015_Adam} führt diese Gewichtsanpassungen durch. Die Lernrate steuert die Schrittgröße dieser Anpassungen; typische Werte liegen zwischen 0.001 und 0.01. Das Training erstreckt sich über mehrere hundert Epochen, wobei eine Epoche einen vollständigen Durchlauf durch alle Trainingsdaten bezeichnet.

Abbildung \ref{fig:training_loop} visualisiert diesen iterativen Prozess. Ein Batch von Trainingsbildern wird zunächst durch das Netz propagiert (Forward Pass), wobei für jedes Bild Vorhersagen generiert werden. Diese Vorhersagen werden mit den korrekten Annotationen verglichen, und die Diskrepanz wird als Loss quantifiziert. Mittels Backpropagation werden anschließend die Gradienten für alle Gewichte berechnet. Der Optimizer nutzt diese Gradienten, um die Gewichte schrittweise anzupassen. Dieser Zyklus wiederholt sich für jeden Batch im Datensatz. Nach einem vollständigen Durchlauf durch alle Trainingsdaten (eine Epoche) beginnt die nächste Epoche. Das Training konvergiert typischerweise nach mehreren hundert Epochen, wobei der Loss kontinuierlich abnimmt.

% ABBILDUNG 3.4: Training Loop
\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.2cm,
    box/.style={rectangle, draw, thick, minimum width=3cm, minimum height=1cm, align=center, font=\small},
    roundbox/.style={rectangle, rounded corners, draw, thick, minimum width=3cm, minimum height=1cm, align=center, font=\small},
    arrow/.style={->, >=stealth, thick}
]
    % Training Data
    \node[box, fill=blue!15] (data) {Trainingsdaten\\Bilder + Annotationen};
    
    % Forward Pass
    \node[box, fill=yellow!20, below=of data] (forward) {Forward Pass\\Vorhersagen generieren};
    
    % Loss Calculation
    \node[box, fill=orange!20, below=of forward] (loss) {Loss-Berechnung\\Fehler quantifizieren};
    
    % Backpropagation
    \node[roundbox, fill=red!20, below=of loss] (backprop) {Backpropagation\\Gradienten berechnen};
    
    % Weight Update
    \node[roundbox, fill=green!20, below=of backprop] (update) {Gewichte anpassen\\Optimizer (SGD/Adam)};
    
    % Arrows
    \draw[arrow] (data) -- (forward);
    \draw[arrow] (forward) -- (loss);
    \draw[arrow] (loss) -- (backprop);
    \draw[arrow] (backprop) -- (update);
    
    % Loop back
    \draw[arrow, dashed, thick, blue!60!black] (update.east) -- ++(1.5,0) |- (forward.east)
        node[midway, right, align=left, font=\scriptsize] {Nächster\\Batch};
    
    % Side annotations
    \node[left=0.8cm of forward, align=right, font=\scriptsize, text width=2.5cm] 
        {CNN generiert\\Bounding Boxes\\+ Klassen};
    \node[left=0.8cm of loss, align=right, font=\scriptsize, text width=2.5cm] 
        {Vergleich mit\\Ground Truth\\$\mathcal{L} = \mathcal{L}_{\text{cls}} + \mathcal{L}_{\text{loc}}$};
    \node[left=0.8cm of backprop, align=right, font=\scriptsize, text width=2.5cm] 
        {$\frac{\partial \mathcal{L}}{\partial w_i}$\\für alle Gewichte};
    \node[left=0.8cm of update, align=right, font=\scriptsize, text width=2.5cm] 
        {$w_i \leftarrow w_i - \eta \frac{\partial \mathcal{L}}{\partial w_i}$};
    
    % Epoch indicator
    \node[below=1.5cm of update, font=\small, align=center] 
        {Wiederholung über mehrere hundert Epochen\\bis Konvergenz};
\end{tikzpicture}
\caption{Trainingszyklus eines CNN-basierten Objektdetektors.}
\label{fig:training_loop}
\end{figure}

\subsubsection{Data Augmentation und Generalisierung}

Eine zentrale Herausforderung beim Training neuronaler Netze ist Overfitting \cite{Goodfellow2016_DeepLearning}: Das Netz memoriert die Trainingsbeispiele anstatt generalisierbare Muster zu lernen. Datenerweiterung (Data Augmentation) \cite{dataugmentation} adressiert dieses Problem durch künstliche Erhöhung der Datensatzvielfalt. Während des Trainings werden Bilder on-the-fly transformiert durch Operationen wie horizontales Spiegeln, Rotation, Skalierung, Helligkeitsanpassung oder Hinzufügen von Rauschen. Für Gleispläne ist insbesondere Rotation essenziell, da Symbole in beliebigen Orientierungen auftreten können. Mosaic Augmentation \cite{Bochkovskiy2020_YOLOv4}, eine Technik, die vier Trainingsbilder zu einem zusammenfügt, erhöht zusätzlich die Robustheit gegenüber variablen Objektdichten.

Abbildung \ref{fig:data_augmentation} zeigt typische Augmentation-Techniken im Überblick. Ein Originalbild wird während des Trainings zufällig transformiert. Rotation um ±30° simuliert die beliebige Orientierung von Symbolen in Gleisplänen. Horizontale Spiegelung verdoppelt effektiv die Datensatzgröße ohne zusätzliche Annotation. Skalierung zwischen 0.8× und 1.2× erhöht die Robustheit gegenüber variierenden Symbolgrößen. Helligkeitsanpassung um ±20\% kompensiert unterschiedliche Scan- oder Digitalisierungsqualitäten. Mosaic Augmentation kombiniert vier Trainingsbilder zu einem einzigen Bild und erhöht die Exposition gegenüber hohen Objektdichten -- besonders relevant für Gleispläne mit bis zu 100 Symbolen pro Bildausschnitt. Diese Transformationen werden on-the-fly während des Trainings angewendet, sodass das Netz in jeder Epoche leicht unterschiedliche Versionen derselben Trainingsbilder sieht.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.8cm and 0.4cm,
    imgbox/.style={
        rectangle, 
        draw, 
        thick, 
        minimum width=3.2cm, 
        minimum height=2.8cm, 
        align=center, 
        font=\scriptsize
    },
    % HIER IST DIE ÄNDERUNG: \boldmath hinzugefügt
    label/.style={font=\scriptsize\bfseries\boldmath}
]

    % --- REIHE 1 ---

    % 1. Original
    \node[imgbox, fill=blue!10] (orig) {Original\\[0.2cm]
    \begin{tikzpicture}[scale=0.5]
        \draw[very thick, fill=white] (0,0) rectangle (2,1);
        \draw[very thick, red] (0.5,0.5) circle (0.3);
        \draw[thick] (1.5, 0.2) -- (1.5, 0.8);
    \end{tikzpicture}};
    \node[label, below=0.1cm of orig] (l_orig) {Original};
    
    % 2. Rotation
    \node[imgbox, fill=yellow!15, right=of orig] (rot) {Rotation\\[0.2cm]
    \begin{tikzpicture}[scale=0.5, rotate=30]
        \draw[very thick, fill=white] (0,0) rectangle (2,1);
        \draw[very thick, red] (0.5,0.5) circle (0.3);
        \draw[thick] (1.5, 0.2) -- (1.5, 0.8);
    \end{tikzpicture}};
    % Jetzt fett durch \boldmath im Style
    \node[label, below=0.1cm of rot] (l_rot) {$\pm 30^\circ$};
    
    % 3. Spiegelung
    \node[imgbox, fill=green!15, right=of rot] (flip) {Spiegelung\\[0.2cm]
    \begin{tikzpicture}[scale=0.5]
        \draw[very thick, fill=white] (0,0) rectangle (2,1);
        \draw[very thick, red] (1.5,0.5) circle (0.3);
        \draw[thick] (0.5, 0.2) -- (0.5, 0.8);
    \end{tikzpicture}};
    \node[label, below=0.1cm of flip] (l_flip) {Horizontal};
    
    
    % --- REIHE 2 ---
    
    % 4. Skalierung
    \node[imgbox, fill=orange!15, below=of l_orig] (scale) {Skalierung\\[0.2cm]
    \begin{tikzpicture}[scale=0.3]
        \draw[very thick, fill=white] (0,0) rectangle (2,1);
        \draw[very thick, red] (0.5,0.5) circle (0.3);
        \draw[thick] (1.5, 0.2) -- (1.5, 0.8);
    \end{tikzpicture}};
    \node[label, below=0.1cm of scale] (l_scale) {0.5--1.5$\times$};
    
    % 5. Helligkeit
    \node[imgbox, fill=purple!15, right=of scale] (bright) {Helligkeit\\[0.2cm]
    \begin{tikzpicture}[scale=0.5]
        \draw[very thick, black!60, fill=gray!40] (0,0) rectangle (2,1);
        \draw[very thick, red!60] (0.5,0.5) circle (0.3);
        \draw[thick, black!60] (1.5, 0.2) -- (1.5, 0.8);
    \end{tikzpicture}};
    % Jetzt fett durch \boldmath im Style
    \node[label, below=0.1cm of bright] (l_bright) {$\pm 20\%$};
    
    % 6. Mosaic
    \node[imgbox, fill=red!15, right=of bright] (mosaic) {Mosaic\\[0.2cm]
    \begin{tikzpicture}[scale=0.22]
        \foreach \x/\y in {0/0, 2.2/0, 0/2.2, 2.2/2.2} {
            \draw[fill=white] (\x,\y) rectangle ++(2,1);
            \draw[red] (\x+0.5,\y+0.5) circle (0.3);
        }
    \end{tikzpicture}};
    \node[label, below=0.1cm of mosaic] (l_mosaic) {4 Bilder};
    
    
    % --- PFEIL ---
    \draw[->, >=stealth, very thick, blue!60!black] 
        ([yshift=-0.3cm]l_scale.south west) -- ([yshift=-0.3cm]l_mosaic.south east)
        node[midway, below, font=\small] {Data Augmentation während Training};

\end{tikzpicture}
\caption{Data Augmentation-Strategien: Vergrößerte Darstellung mit fetten Beschriftungen.}
\label{fig:data_augmentation_bold}
\end{figure}

Transfer Learning \cite{transferlearning} ist eine weitere Strategie zur Verbesserung der Generalisierung. Anstatt ein Netz von Grund auf zu trainieren, wird ein auf einem großen Datensatz (etwa ImageNet \cite{Deng2009_ImageNet} oder COCO \cite{Lin2014_COCO}) vortrainiertes Modell als Startpunkt verwendet. Die unteren Schichten, die generische Features wie Kanten und Texturen gelernt haben, werden beibehalten; lediglich die oberen Schichten werden auf die spezifische Domäne (Gleispläne) angepasst. Dies reduziert den Bedarf an domänenspezifischen Trainingsdaten signifikant \cite{NIPS2014_532a2f85}. Für die in Kapitel \ref{chap:implementation} beschriebene Implementierung wird YOLOv8 mit auf COCO vortrainierten Gewichten initialisiert und anschließend auf bahntechnischen Daten fine-tuned.

\subsection{Evaluationsmetriken}
\label{subsec:evaluationsmetriken}

Die Bewertung von Objektdetektoren erfordert standardisierte Metriken, die sowohl Lokalisierungsgenauigkeit als auch Klassifikationsgüte quantifizieren. Im Gegensatz zu einfachen Klassifikationsproblemen müssen Detektoren beide Aspekte simultan korrekt vorhersagen. Die nachfolgenden Metriken bilden die Grundlage für die in Kapitel \ref{chap:evaluation} präsentierte Systemevaluation.

\subsubsection{Intersection over Union}

Die Intersection over Union (IoU) \cite{Everingham2010_PascalVOC} quantifiziert den Überlappungsgrad zwischen vorhergesagter Box $B_{\text{pred}}$ und Ground-Truth-Box $B_{\text{gt}}$:

\begin{equation}
\text{IoU}(B_{\text{pred}}, B_{\text{gt}}) = 
\frac{\text{Area}(B_{\text{pred}} \cap B_{\text{gt}})}
{\text{Area}(B_{\text{pred}} \cup B_{\text{gt}})}
\end{equation}

Der IoU-Wert liegt im Intervall $[0, 1]$, wobei 0 keine Überlappung und 1 perfekte Übereinstimmung signalisiert. In der Praxis gilt eine Detektion als korrekt, wenn $\text{IoU} \geq 0.5$ -- das heißt, die Überlappung muss mindestens die Hälfte der Gesamtfläche betragen. Dieser Schwellenwert reflektiert einen Kompromiss zwischen Forderung nach Präzision und Toleranz gegenüber geringfügigen Positionsabweichungen. Für präzisionskritische Anwendungen werden teilweise höhere Schwellen (0.75 oder 0.9) verwendet \cite{Lin2014_COCO}.

Abbildung \ref{fig:iou_visualization} veranschaulicht diese Berechnung anhand eines konkreten Beispiels. Die grüne Box repräsentiert die Ground-Truth-Position eines Symbols, wie sie von einem menschlichen Annotator markiert wurde. Die blaue Box stellt die Vorhersage des Detektors dar. Die Schnittfläche (rot schraffiert) entspricht der Fläche, in der beide Boxen überlappen. Die Vereinigungsfläche (durch die gestrichelte schwarze Linie umrandet) umfasst alle Bereiche, die von mindestens einer der beiden Boxen abgedeckt werden. Im dargestellten Fall beträgt die IoU etwa 0.28, was unterhalb der üblichen Schwelle von 0.5 liegt. Obwohl visuell erkennbar ist, dass der Detektor das richtige Objekt identifiziert hat, wird die Detektion aufgrund der ungenügenden Lokalisierung als Fehldetektion gewertet. Dies verdeutlicht die Strenge der IoU-Metrik: Nicht nur die korrekte Objektklasse, sondern auch eine präzise Lokalisierung ist erforderlich.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=1.1]
    % Ground Truth (grün)
    \draw[thick, green!70!black, fill=green!20, opacity=0.7] (1,1) rectangle (5,3.5);
    \node[green!70!black, font=\small\bfseries] at (0.5, 3.8) {Ground Truth};
    
    % Prediction (blau)
    \draw[thick, blue!70!black, fill=blue!20, opacity=0.7] (2.5,1.8) rectangle (6.5,4.3);
    \node[blue!70!black, font=\small\bfseries] at (4.5, 4.6) {Vorhersage};
    
    % Intersection (rot)
    \begin{scope}
        \clip (1,1) rectangle (5,3.5);
        \fill[pattern=north east lines, pattern color=red!80!black, opacity=0.8] 
            (2.5,1.8) rectangle (6.5,4.3);
    \end{scope}
    \draw[thick, red!80!black] (2.5,1.8) rectangle (5,3.5);
    \node[red!80!black, font=\small\bfseries] at (3.75, 2.65) {Schnitt};
    
    % Union (gestrichelt)
    \draw[thick, dashed, black] (1,1) -- (1,3.5) -- (2.5,3.5) -- (2.5,4.3) -- 
        (6.5,4.3) -- (6.5,1.8) -- (5,1.8) -- (5,1) -- cycle;
    
    \node[below, font=\small] at (3.75, 0.3) {$\text{IoU} \approx 0.28$ (zu niedrig, gilt als Fehldetektion)};
\end{tikzpicture}
\caption{Visualisierung der Intersection over Union (IoU)-Metrik.}
\label{fig:iou_visualization}
\end{figure}

\subsubsection{Precision, Recall und Mean Average Precision}

Basierend auf der IoU-Schwelle werden Detektionen klassifiziert: True Positives (TP, korrekte Detektionen mit IoU $\geq$ 0.5), False Positives (FP, Falschdetektionen oder korrekte Klasse aber IoU $<$ 0.5) und False Negatives (FN, übersehene Objekte). Daraus ergeben sich Precision und Recall \cite{Everingham2010_PascalVOC}:

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}, \quad 
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

Die Precision gibt an, welcher Anteil der vom Detektor gemeldeten Objekte tatsächlich korrekt ist. Ein hoher Precision-Wert bedeutet wenige Fehlalarme. Der Recall misst, welcher Anteil der im Bild tatsächlich vorhandenen Objekte vom Detektor gefunden wurde. Ein hoher Recall-Wert bedeutet, dass wenige Objekte übersehen werden. Beide Metriken stehen typischerweise in einem Trade-off: Durch Erhöhung der Konfidenzschwelle kann Precision verbessert werden, jedoch auf Kosten des Recalls.

Für Gleisplanerkennung ist insbesondere der Recall kritisch. Ein übersehenes Signal oder eine fehlende Weichenkennzeichnung kann zu gravierenden Fehlern in der nachfolgenden Planung führen -- etwa falsche Berechnung von Gleiskapazitäten oder Sicherheitsrisiken. Falschdetektionen (niedrige Precision) sind weniger kritisch, da sie in der Validierungsphase (siehe Abschnitt \ref{sec:validation}) durch regelbasierte Filter und Plausibilitätsprüfungen eliminiert werden können. Ein falsch detektiertes Signal an einer unmöglichen Position (etwa mitten auf einem Gleis ohne Haltemöglichkeit) wird durch die semantische Validierung verworfen.

Die Mean Average Precision (mAP) \cite{Everingham2010_PascalVOC,Lin2014_COCO} aggregiert die Erkennungsgüte über alle Objektklassen und verschiedene Konfidenzschwellen. Für jede Klasse wird zunächst die Average Precision (AP) als Fläche unter der Precision-Recall-Kurve berechnet. Diese Kurve entsteht, indem die Konfidenzschwelle variiert wird: Bei niedriger Schwelle werden viele Objekte detektiert (hoher Recall, niedrige Precision), bei hoher Schwelle nur sehr sichere Detektionen (niedriger Recall, hohe Precision). Die mAP ist der Mittelwert über alle Klassen:

\begin{equation}
\text{mAP} = \frac{1}{N} \sum_{i=1}^{N} \text{AP}_i
\end{equation}

wobei $N$ die Anzahl der Objektklassen bezeichnet. Üblich ist die Angabe von mAP@0.5 (IoU-Schwelle 50\%) sowie mAP@[0.5:0.95] (Mittelwert über IoU-Schwellen von 50\% bis 95\% in 5\%-Schritten). Die letztere Metrik ist deutlich strenger und fordert präzisere Lokalisierung. Für diese Arbeit wird primär mAP@0.5 verwendet (siehe Kapitel \ref{chap:evaluation}), da leichte Ungenauigkeiten in der Boxlokalisierung durch nachgelagerte Validierung kompensiert werden können. Ein Signal, das mit 45° statt exakt 47° Rotation detektiert wird, ist für die Planungsaufgabe dennoch korrekt identifiziert.

Die in diesem Abschnitt dargestellten Verfahren -- von zweistufigen Detektoren wie Faster R-CNN bis zu einstufigen Architekturen wie YOLO -- verwenden achsenparallele Begrenzungsrahmen (Axis-Aligned Bounding Boxes, AABB). Diese Boxen sind durch vier Parameter definiert und ihre Kanten verlaufen stets horizontal und vertikal zum Bildrand. Für viele Anwendungen wie Fußgängererkennung oder Fahrzeugdetektion, bei denen Objekte typischerweise aufrechte oder horizontale Orientierung aufweisen, ist dies ausreichend. Technische Zeichnungen mit beliebig rotierten Symbolen und Textannotationen stellen jedoch besondere Anforderungen, die achsenparallele Boxen nur unzureichend erfüllen. Abschnitt \ref{sec:obb} führt daher orientierte Begrenzungsrahmen (Oriented Bounding Boxes, OBB) ein, die durch einen zusätzlichen Rotationsparameter präzisere Lokalisierung rotierter Objekte ermöglichen.
\section{Oriented Object Detection für rotierte Objekte}

Die in Abschnitt 3.2 vorgestellten objekterkennenden Modelle basieren überwiegend auf achsenparallelen Bounding Boxes (Axis-Aligned Bounding Boxes, AABB)\cite{redmon2016lookonceunifiedrealtime}. Bei der Detektion rotierter Objekte, wie sie in technischen Zeichnungen und Gleisplänen vorliegen, stoßen AABB-basierte Ansätze jedoch an fundamentale Grenzen. Oriented Bounding Boxes (OBB) bieten hier eine geometrisch präzisere Alternative, indem sie zusätzlich zum Zentrum und den Abmessungen auch den Rotationswinkel des Objekts modellieren.

\subsection{Limitierungen achsenparalleler Bounding Boxes}

Achsenparallele Bounding Boxes werden durch vier Parameter definiert: Zentrumskoordinaten $(c_x, c_y)$ sowie Breite $w$ und Höhe $h$. Die Orientierung dieser Boxen ist fest an die Bildachsen gekoppelt, was bei rotierten Objekten zu erheblichen Nachteilen führt.

\subsubsection{Hintergrundanteil bei Rotation}

Der fundamentale Nachteil von AABB zeigt sich im Verhältnis zwischen der vom Objekt tatsächlich belegten Fläche und der durch die Bounding Box umschlossenen Gesamtfläche. Bei rotierten Objekten umschließen achsenparallele Bounding Boxes einen erheblichen Anteil irrelevanten Hintergrunds\cite{Ding2019_LearningRoI}. Die AABB muss alle Ecken des rotierten Objekts umschließen und umfasst dabei große Bereiche außerhalb des eigentlichen Objekts. Die OBB hingegen liegt eng am Objekt an und minimiert den Hintergrundanteil. Dieser Unterschied ist in Abbildung \ref{fig:aabb_obb_comparison} für ein um 45° rotiertes Textlabel dargestellt.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.9]
    % LEFT: AABB
    \begin{scope}
        \node[above, font=\large\bfseries] at (2, 3.5) {AABB};
        
        % Rotated text at 45 degrees
        \begin{scope}[rotate around={45:(2,1.5)}]
            \draw[thick, fill=blue!20] (0.5,1) rectangle (3.5,2);
            \node[font=\small] at (2,1.5) {Signal A12};
        \end{scope}
        
        % AABB box (red, axis-aligned)
        \draw[thick, red, dashed] (0.086,0.086) rectangle (3.914,2.914);
        
        % Annotations
        \node[below, red, font=\small] at (2, 3.5) {Hintergrund: 41\%};
    \end{scope}
    
    % RIGHT: OBB
    \begin{scope}[xshift=6cm]
        \node[above, font=\large\bfseries] at (2, 3.5) {OBB};
        
        % Rotated text at 45 degrees
        \begin{scope}[rotate around={45:(2,1.5)}]
            \draw[thick, fill=blue!20] (0.5,1) rectangle (3.5,2);
            \node[font=\small] at (2,1.5) {Signal A12};
        \end{scope}
        
        % OBB box (green, rotated with object)
        \begin{scope}[rotate around={45:(2,1.5)}]
            \draw[thick, green!70!black, dashed] (0.5,1) rectangle (3.5,2);
        \end{scope}
        
        % Annotations
        \node[below, green!70!black, font=\small] at (2, 3.5) {Hintergrund: 2\%};
    \end{scope}
\end{tikzpicture}
\caption{AABB vs OBB Vergleich bei rotiertem Textlabel.}
\label{fig:aabb_obb_comparison}
\end{figure}

Dieser hohe Hintergrundanteil hat zwei wesentliche Konsequenzen für die Objekterkennung: Erstens verschlechtert sich das Signal-Rausch-Verhältnis der Eingabedaten für nachgelagerte Verarbeitungsschritte wie OCR\cite{Ma2018ArbitraryOrientedST}. Zweitens steigt die Wahrscheinlichkeit falscher Detektionen, da irrelevante Bildelemente innerhalb der Bounding Box liegen\cite{Ding2019_LearningRoI}.

\subsubsection{Non-Maximum Suppression bei dichten Objektgruppen}

Ein weiteres Problem ergibt sich bei der Nachbearbeitung von Detektionen mittels Non-Maximum Suppression (NMS)\cite{Ren2015_FasterRCNN}. NMS eliminiert redundante Detektionen durch Unterdrückung überlappender Boxen basierend auf ihrer Intersection over Union (IoU):

\begin{equation}
\text{IoU}(B_1, B_2) = \frac{\text{Area}(B_1 \cap B_2)}{\text{Area}(B_1 \cup B_2)}
\end{equation}

Bei dicht platzierten rotierten Objekten führen AABB zu hohen IoU-Werten, obwohl die Objekte selbst nicht überlappen\cite{Ding2019_LearningRoI}. Dies resultiert in fälschlicher Unterdrückung korrekter Detektionen. Abbildung \ref{fig:nms_problem} demonstriert dieses Fehlverhalten anhand zweier nahe beieinanderliegender Textlabels. Die beiden Textobjekte sind um 30° bzw. -30° rotiert und überlappen sich nicht. Ihre achsenparallelen Bounding Boxes hingegen weisen eine signifikante Überlappung mit IoU = 0.42 auf. Bei einem typischen NMS-Schwellenwert von $\tau = 0.4$\cite{Lin2017_FocalLoss} würde eine der beiden korrekten Detektionen fälschlicherweise unterdrückt. Im Gegensatz dazu berechnet sich die IoU zwischen den orientierten Bounding Boxes korrekt zu 0.0, da die Objekte tatsächlich nicht überlappen.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.9]
    % Two rotated text boxes close together

    % --- Text 1 (30° rotation) ---
    \begin{scope}[rotate around={30:(1.5,2)}]
        \draw[very thick, green!70!black, fill=blue!20] (0.5,1.5) rectangle (2.5,2.5);
        \node[font=\small, rotate=30] at (1.5,2) {Text 1};
    \end{scope}

    % --- Text 2 (-30° rotation) ---
    \begin{scope}[rotate around={-30:(4,2)}]
        \draw[very thick, green!70!black, fill=blue!20] (3,1.5) rectangle (5,2.5);
        \node[font=\small, rotate=-30] at (4,2) {Text 2};
    \end{scope}

    % --- AABBs (red and dashed) ---
    % AABB for Text 1
    \draw[thick, red, dashed] (0.232,1.134) rectangle (2.768,2.866);

    % AABB for Text 2
    \draw[thick, red, dashed] (2.732,1.134) rectangle (5.268,2.866);

    % Overlap region (AABB overlap)
    \fill[red, opacity=0.2] (2.732,1.134) rectangle (2.768,2.866);

    % Annotation
    \node[below, red, font=\small] at (2.75, 0.8) {IoU(AABB) = 0.42};
    \node[below, green!70!black, font=\small] at (2.75, 0.3) {IoU(OBB) = 0.0};
\end{tikzpicture}
\caption{NMS-Problem bei dicht platzierten rotierten Objekten.}
\label{fig:nms_problem}
\end{figure}

\subsection{Oriented Bounding Boxes als Lösung}

Oriented Bounding Boxes erweitern die AABB-Repräsentation um einen zusätzlichen Freiheitsgrad: den Rotationswinkel $\theta$. Eine OBB wird durch fünf Parameter definiert:

\begin{equation}
\text{OBB} = (c_x, c_y, w, h, \theta)
\end{equation}

wobei $(c_x, c_y)$ das Zentrum, $w$ und $h$ die Dimensionen entlang der lokalen Achsen und $\theta \in [-90^\circ, 90^\circ)$ den Rotationswinkel relativ zur horizontalen Bildachse beschreiben. Diese Repräsentation ermöglicht eine eng anliegende Umhüllung rotierter Objekte und reduziert den Hintergrundanteil auf das geometrische Minimum.

\subsubsection{Vorteile für rotierte Objekte}

Die präzisere geometrische Modellierung durch OBB bietet mehrere Vorteile. Bei perfekter Ausrichtung der OBB mit dem Objekt minimiert sich der Hintergrundanteil, während AABB bei starker Rotation erhebliche Hintergrundbereiche umschließen. Die IoU zwischen OBB reflektiert die tatsächliche Objektüberlappung, wodurch Fehlunterdrückungen bei NMS vermieden werden. Dies ist besonders wichtig bei dicht gruppierten Objekten, wo AABB fälschlicherweise hohe Überlappungen anzeigen würden. Zudem kodiert der Parameter $\theta$ explizit die Objektorientierung, was für nachgelagerte Prozesse wie orientierungsabhängige OCR wertvoll ist. Für die Domäne der Gleisplan-Analyse sind diese Eigenschaften besonders relevant, da Symbole entlang der Gleisachsen in beliebigen Winkeln angeordnet sind und oft dicht gruppiert auftreten.

\subsection{Herausforderungen bei der Verwendung von OBB}

Trotz der geometrischen Vorteile bringen OBB zusätzliche Komplexität in die Detektionspipeline, die sich auf Training, Inferenz und Nachbearbeitung auswirkt. Abbildung \ref{fig:obb_challenges} fasst die drei Hauptherausforderungen zusammen: erhöhte Berechnungskomplexität bei der IoU-Berechnung, die Periodizität des Rotationswinkels, die zu Trainingsinstabilität führt, und der erhöhte Trainingsaufwand gegenüber AABB-Modellen.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.85, every node/.style={font=\small}]
    % Challenge 1: Computational Complexity
    \begin{scope}
        \node[above, font=\bfseries] at (2, 3.5) {Berechnungskomplexität};
        
        % AABB: Simple rectangles
        \draw[thick, red] (0.5, 2) rectangle (1.5, 3);
        \draw[thick, red] (2, 2) rectangle (3, 3);
        \node at (1.5, 1.5) {\textcolor{red}{AABB: $\mathcal{O}(1)$}};
        
        % OBB: Rotated rectangles with intersection
        \begin{scope}[shift={(0,-1.5)}]
            \begin{scope}[rotate around={20:(1,0.5)}]
                \draw[thick, blue] (0.5, 0) rectangle (1.5, 1);
            \end{scope}
            \begin{scope}[rotate around={-15:(2.5,0.5)}]
                \draw[thick, blue] (2, 0) rectangle (3, 1);
            \end{scope}
            \node at (1.5, -0.7) {\textcolor{blue}{OBB: $\mathcal{O}(n)$}};
        \end{scope}
    \end{scope}
    
    % Challenge 2: Angle Periodicity
    \begin{scope}[xshift=5.5cm]
        \node[above, font=\bfseries] at (2, 3.5) {Winkel-Periodizität};
        
        % Angle representation
        \draw[->] (1, 2.5) arc (0:360:0.5);
        \node at (2.5, 2.5) {$\theta \in [0^\circ, 360^\circ)$};
        
        % Problem illustration
        \node[align=center] at (2, 1.3) {$0^\circ$ und $89^\circ$:\\geometrisch 1° apart\\numerisch 89 apart};
        
        \draw[thick, red, <->] (0.5, 0.3) -- (3.5, 0.3);
        \node[below, font=\footnotesize] at (2, 0.3) {Trainingsinstabilität};
    \end{scope}
    
    % Challenge 3: Training Overhead
    \begin{scope}[xshift=11cm]
        \node[above, font=\bfseries] at (2, 3.5) {Trainingsaufwand};
        
        % Training progress bars
        \draw[thick, fill=red!30] (0.5, 2.5) rectangle (3, 2.8);
        \node[right] at (3.2, 2.65) {AABB: 100 Epochen};
        
        \draw[thick, fill=blue!30] (0.5, 1.5) rectangle (3.5, 1.8);
        \node[right] at (3.7, 1.65) {OBB: 120-130 Epochen};
        
        % Arrow indicating increase
        \draw[thick, red, ->] (3.3, 2.2) -- (3.3, 1.9);
        \node[right, font=\footnotesize] at (3.5, 2.05) {+20-30\%};
    \end{scope}
\end{tikzpicture}
\caption{Hauptherausforderungen bei OBB-Verwendung.}
\label{fig:obb_challenges}
\end{figure}

\subsubsection{Erhöhte Berechnungskomplexität}

Die Berechnung der Intersection over Union zwischen zwei OBB ist algorithmisch aufwendiger als bei AABB. Während die AABB-IoU durch einfache Min-Max-Operationen in konstanter Zeit $\mathcal{O}(1)$ berechnet werden kann, erfordert die OBB-IoU die Lösung eines Polygon-Clipping-Problems\cite{Zhou2017EastAE}:

\begin{equation}
\text{IoU}_{\text{OBB}}(B_1, B_2) = \frac{\text{Area}(P_1 \cap P_2)}{\text{Area}(P_1 \cup P_2)}
\end{equation}

wobei $P_1$ und $P_2$ die als Vierecke repräsentierten OBB sind. Die Schnittpunktberechnung zwischen den Kantensegmenten skaliert mit $\mathcal{O}(n^2)$ für $n$-Ecke, was in der Praxis zu einem Faktor 3-5 langsameren NMS-Durchläufen führt\cite{Xie2021OrientedRD}.

\subsubsection{Winkel-Periodizität und Trainingsinstabilität}

Der Rotationswinkel $\theta$ weist eine zyklische Periodizität auf: Eine Rotation um 180° (bzw. bei symmetrischen Objekten um 90°) resultiert in einer geometrisch identischen Bounding Box\cite{Yang2021RSDETR,Zhao2023ABFL}. Dies führt zu Ambiguitäten in der Netzwerk-Ausgabe. Betrachtet man beispielsweise zwei Vorhersagen $\theta_1 = 0^\circ$ und $\theta_2 = 89^\circ$ für dasselbe Objekt, so sind diese geometrisch nur 1° voneinander entfernt, numerisch jedoch 89 Einheiten. Standard-Regressionsverluste (z.B. Smooth L1) behandeln diese Diskrepanz linear, was zu instabilen Gradienten führt.


\subsubsection{Erhöhter Trainingsaufwand}

Die zusätzliche Parameterregression für $\theta$ erhöht die Komplexität des Lernproblems. Empirische Studien zeigen, dass OBB-Modelle typischerweise 20-30\% mehr Trainingsepochen benötigen als vergleichbare AABB-Modelle, um ähnliche Konvergenzniveaus zu erreichen\cite{Xie2021OrientedRD}. Dies resultiert aus der Notwendigkeit, sowohl die präzise Lokalisierung als auch die korrekte Orientierungsprädiktion zu erlernen.

\subsection{YOLOv8-OBB Architektur}

Die in Kapitel 5 motivierte Wahl von YOLOv8 für die Symbolerkennung basiert auf dessen spezialisierter OBB-Variante. YOLOv8-OBB erweitert die Standard-Architektur um die Regression des Rotationsparameters $\theta$, während die grundlegende Netzwerkstruktur aus Backbone, Neck und Detection Heads erhalten bleibt\cite{yolov8_ultralytics}. Die schematische Architektur ist in Abbildung \ref{fig:yolov8_architecture} dargestellt und zeigt den Informationsfluss vom Eingabebild über die Feature-Extraktion bis zu den finalen OBB-Vorhersagen. Der Backbone extrahiert hierarchische Merkmale auf fünf Auflösungsstufen, das Neck fusioniert diese über Skalen hinweg, und die drei spezialisierten Detection Heads produzieren Vorhersagen für kleine, mittlere und große Objekte.

\begin{figure}[H]
\centering
\hspace*{1.5cm} 
\begin{tikzpicture}[
    scale=0.9,
    node distance=1.2cm and 0.5cm,
    module/.style={
        rectangle, 
        draw, 
        minimum width=3.2cm, 
        minimum height=1cm, 
        align=center, 
        fill=blue!10,
        font=\small
    },
    arrow/.style={->, >=stealth, thick, rounded corners}
]

    % 1. Input
    \node[module, fill=gray!20] (input) {Input\\$1024 \times 1024 \times 3$};
    
    % 2. Backbone
    \node[module, below=of input, fill=blue!20] (backbone) {CSPNet Backbone\\(C2f Modules)};
    
    % 3. Multi-scale features
    \node[module, below=of backbone, fill=green!20, minimum width=8cm] (features) {Multi-Scale Features\\(P3, P4, P5)};
    
    % 4. Neck
    \node[module, below=of features, fill=orange!20] (neck) {PANet Neck\\(FPN + PAN)};
    
    % 5. Detection heads
    \node[module, below=1.5cm of neck, fill=red!20, minimum width=2.8cm] (head2) {Head P4\\(Medium)};
    \node[module, left=0.2cm of head2, fill=red!20, minimum width=2.8cm] (head1) {Head P3\\(Small)};
    \node[module, right=0.2cm of head2, fill=red!20, minimum width=2.8cm] (head3) {Head P5\\(Large)};
    
    % 6. Outputs
    \node[module, below=1.2cm of head2, fill=purple!20, minimum width=8cm] (output) {OBB Predictions\\$(c_x, c_y, w, h, \theta, \text{cls})$};
    
    % Arrows
    \draw[arrow] (input) -- (backbone);
    \draw[arrow] (backbone) -- (features);
    \draw[arrow] (features) -- (neck);
    
    \draw[arrow] (neck.south) -- (head2.north); 
    \draw[arrow] (neck.south) -| (head1.north); 
    \draw[arrow] (neck.south) -| (head3.north); 
    
    \draw[arrow] (head2.south) -- (output.north);
    \draw[arrow] (head1.south) -- (head1.south |- output.north); 
    \draw[arrow] (head3.south) -- (head3.south |- output.north); 
    
    % Annotations
    \node[right=0.8cm of backbone, font=\footnotesize, align=left] {
        \textbf{Feature Extraction:}\\
        5 Resolution Levels
    };
    
    \node[right=0.8cm of neck, font=\footnotesize, align=left] {
        \textbf{Feature Fusion:}\\
        Top-Down + Bottom-Up
    };
    
    \node[right=0.2cm of head3, font=\footnotesize, align=left, text width=3.5cm] {
        \textbf{Decoupled Heads:}\\
        Separate Classification\\
        \& Regression
    };

\end{tikzpicture}
\caption{YOLOv8-OBB Architektur.}
\label{fig:yolov8_architecture}
\end{figure}

\subsubsection{Netzwerkkomponenten}

Die Architektur gliedert sich in drei funktionale Blöcke. Der Backbone extrahiert hierarchische Merkmale aus den Eingabebildern und verwendet eine auf CSPNet basierende Architektur mit Cross Stage Partial Connections\cite{Wang2020_CSPNet}. Die sogenannten C2f-Module (CSP Bottleneck with 2 Convolutions, faster) ersetzen die C3-Module früherer Versionen und bieten eine effizientere Gradientenfluss-Charakteristik bei gleichzeitiger Reduktion der Parameteranzahl. Das Neck-Modul fusioniert Merkmale unterschiedlicher Auflösungsstufen durch eine Kombination aus Top-Down- und Bottom-Up-Pfaden. Die Path Aggregation Network (PANet) Architektur\cite{Liu2018_PANet} ermöglicht eine effektive Informationspropagation zwischen den Skalen, was für die Detektion sowohl kleiner als auch großer Objekte essentiell ist. YOLOv8 folgt einem Anchor-Free-Ansatz\cite{Tian2019FCOS}, bei dem Objektzentren direkt vorhergesagt werden, anstatt vordefinierte Anker-Boxen zu verwenden. Die Heads sind dezentralisiert (decoupled): Separate Zweige regredieren die Lokalisierung (inklusive $\theta$) und die Klassifikation. Diese Entkopplung verbessert die Konvergenz, da die beiden Aufgaben unterschiedliche Repräsentationen erfordern\cite{Ge2021_YOLOX}.

\subsubsection{Verlustfunktion und Training}

Das Training von YOLOv8-OBB optimiert eine kombinierte Verlustfunktion, die drei Komponenten umfasst:

\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_{\text{box}} \mathcal{L}_{\text{box}} + \lambda_{\text{cls}} \mathcal{L}_{\text{cls}} + \lambda_{\text{dfl}} \mathcal{L}_{\text{dfl}}
\end{equation}

Die Box-Loss $\mathcal{L}_{\text{box}}$ quantifiziert die Lokalisierungsgenauigkeit der vorhergesagten OBB gegenüber der Ground Truth. Die Classification-Loss $\mathcal{L}_{\text{cls}}$ verwendet Binary Cross-Entropy für die Klassenzuordnung\cite{Lin2017_FocalLoss}. Die Distribution Focal Loss (DFL)\cite{Li2020GeneralizedFocal} verbessert die Bounding-Box-Regression durch Modellierung der Koordinaten als Verteilungen anstelle von Punktschätzungen.

Die konkreten Trainingsparameter und -ergebnisse für den im Rahmen dieser Arbeit trainierten Detektor werden in Kapitel 6 detailliert dokumentiert. Die Auswahl von YOLOv8-OBB in Kapitel 5 basiert auf der Kombination aus Echtzeitfähigkeit, Rotationsinvarianz und dem Anchor-Free-Design, welches die Adaption an domänenspezifische Symbolklassen erleichtert.

\subsection{Relevanz für Gleisplan-Analyse}

Die beschriebenen OBB-Eigenschaften sind besonders relevant für die automatisierte Analyse von Gleisplänen. Technische Zeichnungen dieses Typs weisen drei charakteristische Merkmale auf, die den Einsatz orientierter Bounding Boxes notwendig machen. Symbole folgen der Topologie der Gleisachsen und können daher in jedem Winkel $\theta \in [0^\circ, 360^\circ)$ vorliegen. In Bahnhöfen und Weichenstraßen treten Symbole räumlich konzentriert auf, wobei AABB-basierte NMS zu Fehlunterdrückungen führen würde. Die vom OBB-Detektor gelieferte Rotation $\theta$ ist für nachgelagerte Prozesse wie die orientierungsabhängige OCR (vgl. Kapitel 6.3) essentiell, da Textlabels entsprechend ihrer Ausrichtung verarbeitet werden müssen.

Die in Kapitel 4 definierten funktionalen Anforderungen an die Symbolerkennung erfordern daher zwingend den Einsatz rotationsinvarianter Detektionsverfahren. Die in Kapitel 6 beschriebene Implementierung nutzt YOLOv8-OBB mit Standard-Konfiguration für die Detektion von 13 domänenspezifischen Symbolklassen (vgl. Tabelle \ref{tab:symbole} in Kapitel 4).
\section{Optical Character Recognition (OCR)}

Die optische Zeichenerkennung bildet in dieser Masterarbeit die verbindende Struktur zwischen der reinen visuellen Symbolerkennung und der semantischen Darstellung der Gleispläne. Zunächst erhalten bekannte Symbole wie Signale, Koppelspulen und Haltetafeln erst anhand zuverlässig extrahierter IDs und Koordinaten ihre technische Bedeutung und können so problemlos lokalisiert, validiert und versioniert werden. Gleispläne stellen daher hohe Anforderungen an die OCR: die Erkennung sehr kleiner alphanumerischer Texte, unterschiedliche Winkelorientierungen sowie die Überlagerung mit Konstruktionslinien. Darüber hinaus werden die Gleispläne entsprechend ihrem jeweiligen Export als Vektor- oder Bild-PDF mit hoher DPI bereitgestellt. Da die Pläne hinsichtlich Größe und Auflösung sehr umfangreich sind, verwendet der Prototyp eine ROI-First-Strategie, bei der aus der OBB-Detektion abgeleitete, winkelerfassende Regionen speziell und klassenabhängig vorbereitet werden. Die OCR selbst folgt einer pragmatischen Architektur mit einer Deep-Learning-Hauptengine und einem schlanken Fallback, der für die musterbasierte Validierung und die nachvollziehbaren Entscheidungen verwendet wird. Der erzeugte Text und die Metadaten fließen direkt in Verknüpfung, Zuordnung und Änderungsmanagement und spielen dort eine zentrale Rolle für die Qualität und Nachvollziehbarkeit der gesamten Pipeline.

\subsection{Einordnung und Grundlagen}

Optische Zeichenerkennung bezeichnet die automatische Erkennung des in Bildern oder gescannten Dokumenten enthaltenen Textes. Moderne OCR-Prozesse werden konzeptionell in drei Hauptschritte unterteilt: die Lokalisierung der Textbereiche, deren Normalisierung und Vorbereitung und daraufhin die Richtungsanpassung sowie die Erkennung von Zahlen und Buchstaben durch statistische oder Deep-Learning-Modelle.\cite{berg2011high}
\\
\\
Klassische Engines wie Tesseract funktionieren sehr gut mit klaren und horizontal ausgerichteten Druckbuchstaben bei hoher Effizienz. Sie stoßen jedoch in Anwendungsbereichen wie technischen Zeichnungen an ihre Grenzen. Dort machen es die kleinen Buchstabenteile, Drehungen, Scherungen und überlappenden Linien und Texte schwer, eine zuverlässige Erkennung zu gewährleisten.\cite{schlagenhauf2022textdetectiontechnicaldrawings}
\\
\\
Deep Learning basierte OCR Pipelines adressieren diese Heterogenität durch integrierte Modelle, die die Texterkennung, Winkelnormalisierung und deren Klassifizierung gemeinsam berücksichtigen. Systeme wie PaddleOCR kombinieren beispielsweise die differentielle Binarisierung (DB) für die robuste Texterkennung mit den konvolutionalen rekurrenten neuronalen Netzen (CRNN) zur Textidentifikation und sind mit einer integrierten Winkelerkennung für die automatische Winkelkorrektur verfügbar.\cite{paddleocr}
\\
\\
In dem oben beschriebenen System wird die grundlegende Architektur durch eine ROI-basierte Verarbeitung erweitert. Die aus der orientierten Objekterkennung gewonnenen Winkel und die Begrenzungen steuern die OCR gezielt auf die relevanten Regionen.Zugleich  werden domänenspezifische Muster wie Signal IDs oder die Koordinaten- bzw. GKS-Formate für die Validierung und Korrektur der erkannten Ergebnisse verwendet. Diese Kombination bildet die methodische Grundlage für den folgenden Abschnitt, der als Single Primary Engine mit Fallback-Architektur bezeichnet wird, sowie für die intelligente klassenabhängige Vorverarbeitung, die die Robustheit und Effizienz des gesamten Systems gewährleistet.

\subsection{OCR im Bahnumfeld}
Der Einsatz von OCR in der Bahnumgebung erfordert hohe Anforderungen an Robustheit, Genauigkeit und Rückverfolgbarkeit. Die Gleis- und Signalpläne enthalten meist kleine, dicke und unterschiedlich geneigte Textsegmente, die oft in unmittelbarer Nähe von Symbolen positioniert sind. Diese Textbereiche liegen häufig unregelmäßig, variieren in der Textgröße und befinden sich in komplexer grafischer Umgebung, was eine erhebliche Herausforderung für eine klassische OCR-Engine darstellt.
\cite{schlagenhauf2022textdetectiontechnicaldrawings}
\\
\\
Darüber hinaus treten Überlappungen durch Konstruktionslinien, Raster, Legenden, andere Symbole und Texte insbesondere bei eingescannten Plänen und aus CAD-Systemen exportierten Plänen auf. Bildbasierte PDF-Formate zeigen zusätzlich Störungen und komprimierte Artefakte, die die Texterkennung und anschließend die Erkennung erschweren können.\cite{Joshua} Die OCR muss dann in der Lage sein, zwischen relevanten Textinhalten und grafischen Elementen zu unterscheiden.
\\
\\
Eine weitere Herausforderung ergibt sich aus den unterschiedlichen Exporttypen der Quelldaten. Während die Vektor-PDFs eine klare Segmentierung der Textobjekte ermöglichen, müssen die Raster-PDFs pixelweise analysiert werden. Unterschiede in der Auflösung (DPI) und den Renderprozessen führen daher zu stark schwankender Textschärfe und Konturen der Zeichen, wodurch eine adaptive Vorverarbeitung durch Kontrastnormalisierung, Glättung der Kanten und geometrische Entzerrung wichtig wird.\cite{paddleocr}
\\
In der Eisenbahnumgebung existieren domänenspezifische Muster, die eine regelbasierte Validierung und automatische Fehlerkorrektur erfordern. Dazu gehören alphanumerische Signalidentifikatoren  (z.B. \verb|[A-Z]\{1,2\}\d\{2,3\}|), Ziffernfelder in Kennplatten oder Koordinatendaten in Zahlen mit Komma- oder Dezimalabstand. Solche Formatregularitäten können für die semantische Koordination und Plausibilität der OCR Ergebnisse verwendet werden.\cite{khan2025drawingsdecisionshybridvisionlanguage}
\\
\\
Schließlich müssen die OCR-Ergebnisse deterministisch und nachvollziehbar sein. Für die Anwendung in sicherheitsrelevanten Schienensystemen ist es wichtig, dass alle Transformationen, Scores und Fallback-Entscheidungen dokumentiert und reproduzierbar sind.\cite{KI-LOK2025} Neben dem erkannten Text müssen die Vertrauenswerte, Rotationsparameter sowie die angewendeten Filter und der Validierungsstatus gespeichert werden. Diese Transparenz ist eine Voraussetzung für die Prüfbarkeit und Nachvollziehbarkeit im Rahmen des Qualitätsmanagements und der regelmäßigen Überprüfung.\cite{Vromans2005ReliabilityOR}

\subsection{Architekturen von OCR}
Der aktuelle Stand der Technik in OCR ermöglicht in zwei Hauptentwicklungsbereichen erhebliche Fortschritte.
Erstens gibt es klassische OCR-Engines, wie zum Beispiel Tesseract. Diese Systeme arbeiten hauptsächlich regelbasiert und verwenden spezifische Seitenaufteilungsmodi sowie auch Zeichenlisten.\cite{Smith2007} Während sie bei horizontal ausgerichtetem Text und sauberem Druckbild sehr gut funktionieren, sind sie sehr ineffizient, wenn Rotationen, Bildstörungen oder komplexe Pläne vorliegen.\cite{Patel}
\\
\\
Auf der anderen Seite gibt es Deep-Learning-basierte Pipelines. Diese Lösungen kombinieren fortschrittliche Texterkennungsprozesse wie differentiable Binarization (DB) mit neuronalen Netzwerken zur Erkennung, z.B. wie PaddleOCR oder Transformers. Da Erkennung, geometrische Ausrichtung und eigentliche Zeichenerkennung oft optimiert integriert sind, bewältigen sie erfolgreich die Herausforderungen von schrägen, kleinen oder teilweise verdeckten Texten.\cite{liao2019realtimescenetextdetection}
\\
\\
Basierend auf diesen technologischen Lösungen wird für diese vorliegende Arbeit ein ROI-First-Ansatz gewählt. Hier funktioniert die Objekterkennung mithilfe orientierter Begrenzungsrahmen (Abschnitt 3.2.7) als Vorstufe, um die relevanten OCR-Zonen genau zu definieren und zu extrahieren. Die Begründung für diese Architekturentscheidung, insbesondere die Kombination eines primären Systems mit einem gezielten Fallback, wird in Abschnitt 5.3.3 gezeigt, gefolgt von Implementierungsdetails in Abschnitt 6.3.
\subsection{Vergleich von OCR Ansätzen für technische Gleispläne}
Im folgenden Abschnitt werden die relevanten Prozesse erklärt und auch ihre Leistungsfähigkeit für die Art der technischen Zeichenpläne dargestellt.
\begin{enumerate}
    \item Klassische OCR Engines - Klassische Ansätze, die durch die Tesseract-Engine prominent vertreten sind, basieren traditionell auf einer Kombination aus heuristischer Layoutanalyse (Seitenaufteilung) und zeichen- sowie wortbasierter Erkennung.
    \begin{itemize}
        \item Funktionalität und Stärken - Diese Systeme arbeiten besonders effizient mit horizontal geneigtem Text mit hohem Kontrast. Ein wichtiger Vorteil liegt in der hohen Konfigurierbarkeit. Durch spezifische Segmentierungsmodi und die Begrenzung der Whitelist erfolgt die Erkennung optimiert speziell auf reine Zeichenfolgen oder feste Formate. Darüber hinaus ist dieser Prozess ressourcensparend und zeigt geringere Hardwareabhängigkeiten.\cite{Smith2007}
        \item Beschränkungen im Zeichungskontext - Die Hauptschwäche klassischer Engines liegt in ihrer Empfindlichkeit gegenüber Rotation und Scherung. Da die Layoutanalyse hauptsächlich von der horizontalen Linienstruktur ausgeht, erfordert die Verarbeitung technischer Layouts aufwändige Vorverarbeitungsschritte wie Deskewing oder iterative Tests in diskreten Winkelstufen (0°, 90°, 180°, 270°). Außerdem führen überlappende Linien oft zu Fehlsegmentierungen, was die Rohleistung ohne umfangreiche Vorfilterung stark einschränkt.\cite{memon2020handwrittenopticalcharacterrecognition}
    \end{itemize}
    \item Deep Learning basierte OCR Pipelines- Moderne Ansätze aus dem Bereich der Texterkennung in Szenen basieren tatsächlich meist auf mehrstufigen Deep-Learning-Architekturen.\cite{long2020scenetextdetectionrecognition} Diese Pipelines trennen normalerweise die Texterkennung (zum Beispiel mit Hilfe von Differentiable Binarization, DB/DB++) von der eigentlichen Sequenz-Erkennung (zum Beispiel CRNN- oder Transformer-Modelle).
    \begin{itemize}
        \item Funktionalität und Stärken - Durch die Trennung von Erkennung und Identifikation sind diese Prozesse besonders für komplexe und heterogene Zeichnungen geeignet. Die Detektoren sind in der Lage, Textbereiche unabhängig von ihrer Ausrichtung zu lokalisieren. Die nachgelagerten Module zur geometrischen Anpassung (Rectifizierung) oder die integrierte Klassifikation der Ausrichtungen ermöglichen eine hohe Robustheit gegenüber geneigten und verzerrten Schriften. Darüber hinaus liefern diese Modelle Vertrauenswerte und ermöglichen eine effektive Filterung und Validierung der Ergebnisse.\cite{liao2019realtimescenetextdetection}
        \item Beschränkungen - Der Hauptnachteil liegt im hohen Rechenbedarf, der oft für die hohe Leistung den Einsatz von GPUs erfordert. Es kann auch Anpassungen an sehr spezifische Schriftarten oder Symbole mit Hilfe eines ausgefeilten Nachtrainierens (Fine Tuning) der Modelle geben.\cite{du2020ppocrpracticalultralightweight}
    \end{itemize}

    \item Leichtgewichtige Frameworks - Als Unterkategorie haben sich Frameworks wie EasyOCR etabliert, die vorgefertigte Deep-Learning-Modelle mit einer einfachen API bereitstellen.
    \begin{itemize}
        \item Eignung- Diese Systeme senken die Einstiegshürde und eignen sich für Prototyping oder Anwendungen mit moderaten Anforderungen. In komplexen Szenarien mit starken Überlappungen, extrem kleinen Schreibstilen oder ungewöhnlichen Winkeln erreichen sie jedoch oft nicht die Stabilität spezialisierter Pipelines und bieten geringere Möglichkeiten zur Feinabstimmung der Parameter.\cite{baek2019characterregionawarenesstext}
    \end{itemize}
\end{enumerate}

\subsection{ROI basierte OCR und Orientierungskonzepte}
Anstelle einer rechenintensiven, ganzseitigen Rasterisierung (vollständiges Seiten-OCR) verwendet diese Arbeit einen fokussierteren und gezielteren Ansatz, der die optische Zeichenerkennung auf definierte ROIs beschränkt. Diese ROIs werden dann aus der zuvor lokalisierten Symbolerkennung abgeleitet.\cite{long2020scenetextdetectionrecognition} Diese methodische Änderung bietet drei wichtige Vorteile:

\begin{itemize}
    \item \textbf{Effizienz:} Die Reduzierung des Suchbereichs auf relevante Bildabschnitte wird die \\Latenz der Pipeline erheblich verringern und auch verhindern, dass unnötige Bereiche durchsucht werden, in denen irrelevante Daten vorhanden sind.
    \item \textbf{Präzision:} Durch das Herausfiltern der irrelevanten Hintergrundstruktur (Störungen, Linien) und unnötiger Daten wird die Falsch-Positiv-Rate der Texterkennung minimiert.
    \item \textbf{Semantik:} Die räumliche Nähe erleichtert die logische Verbindung zwischen Text und Symbol und hält die Verknüpfung präzise und stabil.
\end{itemize}

Eine besondere Herausforderung in den technischen Bereichen wie Gleisplänen stellt die variable Textausrichtung dar. Der Ansatz integriert daher ein explizites Orientierungsmanagement. Während bei geringen Abweichungen eine diskrete Rotationsnormalisierung ausreicht, erfordern starke Abweichungen oder perspektivische Verzerrungen fortgeschrittene Entzerrungsprozesse (Rektifizierung).\cite{García} Dieser winkelbasierte Extraktionsprozess (winkelbewusstes Zuschneiden) bildet die Grundlage für eine robuste Erkennung in heterogenen Pläne.

\subsection{Bildvorverarbeitung: Kategorien und Ziele}

Die Vorverarbeitung dient als ein wesentlicher Zwischenschritt, um die Varianz der Eingangsdaten zu minimieren und die Regionen von Interesse (ROIs) für die nachgelagerten OCR-Klassifikatoren zu normalisieren. Ziel ist hier die Maximierung der Trennbarkeit zwischen Vordergrund (Text) und Hintergrund. Die Maßnahmen werden dann in drei funktionale Kategorien unterteilt:

\begin{enumerate}
    \item \textbf{Photometrische Aufbereitung} : \\
    Um die Lichtschwankungen und den geringeren Kontrast auszugleichen, wird zunächst der Prozess zur Kontraststeigerung angewendet. Für die Trennung von Text und Hintergrund eignen sich adaptive Binarisierungsverfahren. Diese sind außerdem robuster im Vergleich zu globalen Schwellenwerten wie Otsu gegen ungleichmäßige Beleuchtung und unterschiedliche Hintergründe in technischen Plänen.\cite{SAUVOLA2000225}
    \item \textbf{Artefaktunterdrückung und Strukturrekonstruktion}: \\
    Technische Zeichnungen zeigen oft spezifische Störungen wie Scanrauschen oder Hilfslinien, die die Textzeichen überlagern. Hier kommen Filter zur Rauschunterdrückung und Algorithmen zur Linienentfernung zum Einsatz, wobei die Erhaltung der Zeichenstämme entscheidend ist, um Informationsverlust zu vermeiden. \cite{Tombre} Zusätzlich werden morphologische Operationen (Dilatation, Closing) verwendet, um fragmentierte Zeichen zu reparieren oder dünne Linien für den Erkenner zu verstärken.\cite{Gonzalez}
   
    \item \textbf{Geometrische ROI Optimierung}: \\
    Da die OCR-Engines oft empfindlich auf fehlenden Kontext oder zu geringe Auflösungen reagieren, ist hier eine geometrische Normalisierung erfolgreich. Diese beinhaltet das Hinzufügen der Randbereiche (Padding), um die Randeffekte bei der Faltung (Convolution) zu vermeiden, sodass die Hochskalierung der kleineren ROIs auf die für die OCR optimale Zielauflösung (typischerweise mindestens 300 DPI) erfolgen kann. Bei mehrzeiligem Text einer ROI ist zunächst eine vertikale Segmentierung der Linien wichtig.\cite{Smith2007} 
    
\end{enumerate}

\subsection{Domänenspezifische Validierung als Qualitätsanker}

Da reine OCR-Engines keine semantische Prüfung der erkannten Zeichen vornehmen können \cite{Smith2007}, fungieren domänenspezifische Nachbearbeitungsfunktionen als wichtige Qualitätsanker. Spur-Layouts folgen einer strikten Nomenklatur, aus der starke strukturelle Erwartungswerte für den Textinhalt resultieren (wie Kombinationen aus Buchstaben und Zahlen).\cite{Kukich} Dieser Umstand ermöglicht die Nutzung von regulären Ausdrücken (Regex) und Plausibilitätsregeln, um Rohdaten zu validieren und zu filtern.\cite{Taghva} Durch den Vergleich mit dem definierten Muster können typische OCR-Substitutionsfehler, insbesondere bei visuell ähnlichen Zeichen (Homoglyphen) wie O/0, I/1/l oder S/5, automatisch erkannt und korrigiert werden.\cite{Kukich} Ergebnisse, die keinem definierten Schema entsprechen, werden als unsicher markiert, um eine effiziente manuelle Überprüfung (Human in the Loop) zu steuern.
\subsubsection{Formale Sprachen und reguläre Ausdrücke}

Die Validierung extrahierter Texte basiert auf der Theorie formaler Sprachen.\cite{hopcroft2001automata} Ein regulärer Ausdruck (Regex) definiert eine formale Sprache $L$ über einem Alphabet $\Sigma$ durch Musterregeln.

Für Gleisplan-Symbole lassen sich Validierungsregeln wie folgt formalisieren:

\begin{itemize}
    \item Signal: $L_{signal} = \{w \in \Sigma^* \mid w \text{ matches } \texttt{[A-Z]\{1,4\}[0-9]\{1,4\}}\}$
    \item Koordinate: $L_{coord} = \{w \in \Sigma^* \mid w \text{ matches } \texttt{[0-9]+[.,][0-9]+}\}$
    \item Balise: $L_{balise} = \{w \in \Sigma^* \mid w \text{ matches } \texttt{[0-9]\{4\}}\}$
\end{itemize}

Diese formalen Definitionen ermöglichen die automatische Erkennung von 
OCR-Fehlern durch Abgleich mit den erwarteten Mustern. Texte, die keinem 
Muster entsprechen, werden zur manuellen Prüfung markiert.

\subsubsection{Typische OCR-Substitutionsfehler}

Häufige Verwechslungen bei der optischen Zeichenerkennung:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|p{6cm}|}
\hline
\textbf{Korrekt} & \textbf{Fehler} & \textbf{Ursache} \\
\hline
O & 0 & Ähnliche Form (Kreis) \\
I & 1, l & Vertikale Linie \\
S & 5 & Ähnliche Kurvatur \\
B & 8 & Geschlossene Formen \\
\hline
\end{tabular}
\caption{Typische Homoglyphen bei OCR-Fehlern}
\label{tab:homoglyphs}
\end{table}

Die Kenntnis dieser Fehlerquellen ermöglicht die Implementierung 
intelligenter Korrekturheuristiken (vgl. Kapitel 6.5).
\section{Erkennung räumlicher Beziehungen in der Dokumentenanalyse}
Die automatisierte Extraktion von Informationen aus technischen Zeichnungen erfordert nicht nur die Erkennung einzelner Objekte, sondern auch das Verständnis ihrer räumlichen Beziehungen zueinander. Dieser Abschnitt behandelt die theoretische Grundlagen der Dokumentanalyse und deren Anwendung auf die automatisierte Interpretation von Gleispläne.\cite{Binmakhashen} 

\subsection{Grundkonzept räumlicher Beziehungen in Dokumenten}
Räumliche Beziehungen beschreiben die geometrische und topologische Anordnung von Objekten zueinander.\cite{Cohn1997} In technischen Dokumenten wie Gleisplänen manifestieren sich diese Beziehungen durch: 
\begin{itemize}
    \item Topologische Relationen: Berührung, Überlappung, Inklusion (z.B text innerhalb eines Symbols)
    \item Direktionale Relationen: Oberhalb, unterhalb, links von, rechts von einem gegebenen Symbol. 
    \item Metrische Relationen: Euklidische Distanz, Ausrichtung, relative Größe (z.B nächstgelegner Text zu einem Symbol)
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    scale=1.2, 
    >=latex, % Schöne Pfeilspitzen
    node distance=0.8cm and 0.5cm, % Vertikaler und Horizontaler Abstand
    font=\small
]

    % 1. Das Signal (Zentrales Element)
    % Wir definieren es als Node, damit wir uns darauf beziehen können
    \node[draw=blue, thick, fill=blue!20, minimum width=2cm, minimum height=1cm, align=center] (signal) {\textbf{Signal}};
    
    % 2. ID (Oberhalb)
    % Positioniert relativ zum Signal
    \node[above=of signal, text=green!60!black, font=\bfseries] (id) {AS102};
    
    % Der Erklärungstext kommt RECHTS neben die ID (right=of ...)
    \node[right=0.2cm of id, text=green!60!black, align=left] (id_desc) {(ID oberhalb)};
    
    % Pfeil von ID zum Signal
    \draw[->, thick, green!60!black] (id) -- (signal);
    
    % 3. Koordinate (Unterhalb)
    \node[below=of signal, text=orange!70!black, font=\bfseries] (coord) {18.456};
    
    % Der Erklärungstext kommt RECHTS neben die Zahl
    \node[right=0.2cm of coord, text=orange!70!black, align=left] (coord_desc) {(Koordinate\\unterhalb)};
    
    % Pfeil von Koordinate zum Signal
    \draw[->, thick, orange!70!black] (coord) -- (signal);
    
    % 4. Distanz-Anmerkung (Links daneben, damit es nicht stört)
    % Wir verschieben den Startpunkt etwas nach links ([xshift=-0.8cm])
    \draw[<->, dashed, gray!80!black] 
        ([xshift=-0.8cm]signal.north) -- ([xshift=-0.8cm]id.south) 
        node[midway, left, font=\scriptsize, text=black] {$d_{ID} < r_{max}$};

\end{tikzpicture}
\caption{Räumliche Beziehungen: Klare Zuordnung von ID (oben) und Koordinate (unten) zum Signal.}
\label{fig:spatial_relations_clean}
\end{figure}

Diese Relationen sind nicht nur geometrischer Natur, sondern tragen semantische Bedeutung.
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|p{4.5cm}|p{6cm}|}
\hline
\textbf{Relationstyp} & \textbf{Mathematische Basis} & \textbf{Anwendung in Gleisplänen} \\
\hline
Topologisch & Mengentheorie (Berührung, Überlappung) & Text innerhalb eines Symbols \\
\hline
Direktional & Vektorgeometrie (oberhalb, unterhalb) & Signalerkennung oberhalb, Koordinate unterhalb \\
\hline
Metrisch & Euklidische Distanz & Nächstgelegener Text zu Symbol \\
\hline
\end{tabular}
\caption{Typen räumlicher Beziehungen in technischen Dokumenten}
\label{tab:spatial_relations}
\end{table}

\subsection{Zonenklassifizierung und semantische Segmentierung}
Die Zonenklassifizierung unterteilt ein Dokument in funktional zusammengehängende Bereiche.\cite{chen2017convolutionalneuralnetworkspage} In technischen Zeichnungen umfasst dies typicherweise:

\begin{itemize}
    \item Zeichnungsbereich - Hauptinhalt mit technischen Symbolen
    \item Titelfeld - Metadaten wie Projektname, Maßstab, Datum 
    \item Stücklisten - Tabellarische Komponentenübersicht
    \item Annotationsbereiche - Freitext Bemerkungen
\end{itemize}

Die Klassifizierung erfolgt entweder regelbasiert oder durch trainierte Klassifikatoren.\cite{Pfitzmann_2022} Für Gleispläne ist insbesondere die Unterscheidung zwischen:
\begin{enumerate}
    \item Symbolregionen: Bereiche mit standardisierte Darstellungen
    \item Textregionen: Beschriftungen, Koordinaten, Kennungen 
    \item Geometrische Elemente: Gleisverläufe, Begrenzungen
\end{enumerate}
von Bedeutung. Diese Zonenklassifizierung reduziert den Suchraum für nachfolgende Assoziationsalgorithmen erheblich. 
\subsection{Proximity basierte Verknüpfung}
Die proximale Verknüpfung basiert auf der ANnahme, dass räumlich nahe Objekte mit höherer Wahrscheinlichkeit semantisch zusammenhängen.\cite{Nagy} Der klassische Ansatz folgt einem mehrstufigen Verfahren: 
\\
\textbf{Distanzbasierte Assoziation}: Für jedes erkannte Symbol S wird ein Suchradius r definiert, innerhalb dessen potenzielle Textkandidaten $T_i$ evaluiert werden. Die Auswahl erfolgt typischerweise über die euklidische Distanz\cite{Kucer}:

$$d(S,T_i) = \sqrt{(x_S - x_{T_i})^2+(y_S - y_{T_i})^2}$$

wobei $(x_S, y_S)$ das Zentrum des Symbols S und $(x_{T_i}, y_{T_i})$ das Zentrum 
des Textkandidaten $T_i$ bezeichnen.

Der Text $T^*$ mit minimaler Distanz wird dem Symbol zugeordnet:

$$T^* = arg \: min \: d(S,T_i) \quad unter \quad  d(S,T-i) \leq r$$

Kucer et al. \cite{Kucer} demonstrieren in ihrer Arbeit zu technischen Patentzeichnungen , dass ein einfacher proximity basierter Ansatz mit euklidischer Distanzberechnung zwischen geometrischen Zentren eine Genauigkeit von 97\% bei der Label Figure Zuordnung erreicht. 

\textbf{Richtungsabhängige Assoziation}
In strukturierten techinschen Zeichnungen folgen Beschriftungen häufig Konventionen(z.B. Text unter Symbolen).\cite{Nagy} Dies wird durch direktionale Beschränkungen fomalisiert:

\begin{itemize}
    \item Below: $y_T > y_S + \delta_y$
    \item Above: $y_T < y_S - \delta_y$
    \item Right: $x_T > x_S + \delta_x$
\end{itemize}

wobei $\delta$ kontextabhängige Toleranzwerte darstellen. 

\textbf{Erweiterte Metriken}
Modernere Ansätze berücksichtigen zusätzliche faktoren\cite{futrelle1995efficientanalysiscomplexdiagrams}:
\begin{itemize}
    \item Ausrichtung: Überlappung der Bounding Box Projektionen
    \item Größenverhältnis: Ähnliche Schrifthöhen indizieren Zusammengehörigkeit
    \item Semantische Plausibilität: OCR Ergebnnis muss zu Objektklasse passen (z.B. Ziffernfolge für Koordinatenangaben)
\end{itemize}
\subsection{Herausforderungen bei rotierten Layouts}
Eine besondere Schwierigkeit technischer Zeichnungen besteht in der freien Rotation von Symbolen und Texten. \cite{schlagenhauf2022textdetectiontechnicaldrawings} \cite{yolov8_ultralytics} Traditionelle achsenparallele Proximity Metriken funktionieren nicht hier. Erforderlich sind: 

\begin{enumerate}
    \item Rotationsinvariante Features: Verwendung von OBB statt AABB\cite{yolov8_ultralytics}
    \item Transformierte Distanzberechnung: Berechnung der Distanz im lokalen Koordinatensystem des Symbols.
    \item Winkeltolerante Richtungsprüfung: \enquote{Unterhalb} muss relativ zur Symbolorientierung definiert werden.
\end{enumerate}
Diese Anforderungen werden in der vorliegenden Arbeit durch eine winkelabhängige Verarbeitungspipeline adressiert (Details in Kapitel 5 und 6).


\subsection{Relevanz für die automatische Gleisplananalyse}

Die beschriebenen Konzepte bilden die theoretische Grundlage für die in dieser Arbeit entwickelte Linking Komponente. Konkrete Anwendungsfälle umfassen:

\begin{itemize}
    \item Signal Koordinaten Assoziation: Zuordnung von Positionsangaben zu Signalen
    \item Balisen Text Extraktion - Erfassung der Zeichen innerhalb von Balisen Symbolen
    \item Koordinate Blöcke: Gruppierung mehrere Koordinatenzeilen
    \item Symbole Gruppen: Erkennung zusammengehöriger Symbol Koordinate Symbol Tripel
\end{itemize}
Die Umsetzung dieser Verknüpfungslogik wird in Abschnitt 6.4 detalliert beschrieben, während die Evaluierung der Linking Genauigkeit in Kapitel erfolgt. 

\section{Change Detection in technischen Dokumenten}

Die Erkennung und Verfolgung von Änderungen in technischen Dokumenten, Daten oder Eigenschaften eines Produkts wird als Änderungserkennung oder Änderungsmanagement bezeichnet.\cite{renuka2013change} Dies spielt in vielen Bereichen der Technik eine wichtige Rolle. 
Insbesondere in der technischen Dokumentation von infrastruktur- und sicherheitskritischen Systemen wie der Eisenbahnindustrie müssen alle Änderungen zwischen den Dokumentversionen nachvollziehbar, überprüfbar und dokumentiert sein. \cite{DINISO9001} \cite{VDI1000}

Die Änderungserkennung hat sich als wirksame Methode zur Identifizierung von Abweichungen zwischen zwei oder mehr Dokumentversionen bewährt. In den Forschungsarbeiten \cite{renuka2013change} wird eine Unterscheidung der Methoden vorgenommen. 

\begin{itemize}
    \item \textbf{Struktur und Gleisplanbasierte Verfahren} - Der Fokus dieser Verfahren liegt auf dem Vergleich der geometrischen Eigenschaften und Positionen von Objekten. In Gleispläne sind davon insbesondere die Koordinaten von Symbolen, Linien und Flächen betroffen. 
    Die Analyse von Bounding Boxes oder Vektordaten ermöglicht die automatische Hinzufügung, Verschiebung oder Löschung neuer Elemente. \cite{Dohrn2014Finegrained} 
    \item \textbf{Textbasierte Verfahren} - Neben grafischen Symbolen beinhalten die Gleispläne eine Vielzahl an Textinformationen, wie etwa Signalbezeichnungen, Streckenkilometer oder Gleisabschnittsnamen. Mittels der optischen Zeichenerkennung (OCR) kann der Text extrahiert und anschließend versionsbezogen verglichen werden. 
    Dieser Ansatz ermöglicht die Identifizierung von Änderungen im Schreibstil, in Positionsangaben oder Signalbezeichnungen. \cite{Smith2007}
    \item \textbf{Kombinierte Verfahren} - In vielen Anwendungsfällen ist eine Kombination aus Objekterkennung und Texterkennung wichtig. Ein Beispiel hierfür ist ein Symbol, das optisch unverändert bleibt, aber eine neue Bezeichnung erhält. Erst durch die Kombination von Bild- und Textvergleich können solche Unterschiede zuverlässig 
    erfasst werden.
\end{itemize}
Für den entwickelten Prototyp der Masterarbeit bedeutet dies, dass ein automatischer Vergleich zwischen zwei Versionen der Gleispläne auf Basis der visuellen Objekterkennung und der OCR durchgeführt wird. Das Ergebnis ist eine strukturierte Differenzanalyse, die die Unterschiede in Symbolen und Texten dokumentiert.
Diese kann in einer Excel-Ausgabe dargestellt oder durch visuelle Hervorhebungen (z. B. Überlagerungen) im Plan angezeigt werden.

\section{Semantisches Mapping \& Ontologien im Bahnwesen}

Die reine Erkennung von Symbolen in Gleisplänen kann als erster Schritt in Richtung einer automatisierten Analyse betrachtet werden. Für die Nutzbarmachung der extrahierten Informationen für technische Planungen, Dokumentationen oder weitere digitale Prozesse ist eine semantische Organisation von essenzieller Bedeutung. Semantisches Mapping bezeichnet die visuelle Darstellung von Objekt- und Texterkennungen auf der Grundlage standardisierter und domänenspezifischer Bedeutungen.\cite{GRUBER1993199} \cite{STUDER1998161}
\\
\\
In der Eisenbahnindustrie existieren zahlreiche verschiedene Symbolsysteme, die je nach Kunde oder Land variieren können. Ein identisches grafisches Symbol kann daher in unterschiedlichen Kontexten unterschiedliche Bedeutungen haben. Um mit dieser Vielfalt umzugehen, ist eine Ontologie von entscheidender Bedeutung. Eine Ontologie ist definiert 
als eine formale Beschreibung der relevanten Konzepte (z. B. Symbole, Abweichungen, Balisen) und ihrer Beziehungen zueinander.\cite{Uschold_Gruninger_1996} Ontologien konstituieren eine maschinenlesbare Wissensbasis, die eine Klassifizierung sowie Einordnung bekannter Objekte in einen größeren kontextuellen Zusammenhang ermöglicht.
\\
\\
In der vorliegenden Masterarbeit wird die semantische Darstellung durch Zuordnung zu YAML- oder JSON-basierten Konfigurationsdaten erörtert. Im Fokus der Untersuchung steht die Abbildung kundenspezifischer Symbole in standardisierten Bezeichnungen. Ein grafisches Symbol XY wird beispielsweise mit dem OCR-Text XY123 als Symbolnummer gespeichert, 
während das grafische Symbol T mit dem OCR-Text 12345,6 in einer Tabelle dokumentiert wird. Die Verwendung einer modularen Mapping-Struktur gewährleistet die Erweiterbarkeit des Systems für verschiedene Kunden und Symbolsysteme.
\\
\\
Die semantische Zuordnung markiert folglich einen entscheidenden Schritt von der syntaktischen Erkennung (Symbol + Text) zur semantischen Interpretation (Bedeutung + Kontext in der Eisenbahnindustrie). Diese semantische Interpretation ist eine Voraussetzung für die Integration der Daten in nachgelagerte Anwendungen, wie beispielsweise in Projektionssoftware, 
Datenbanken oder Digital-Twin-Plattformen.
\subsection{Konfigurationsbasierte Semantikdefinition}

Die Umsetzung ontologischer Konzepte erfolgt in der Praxis häufig über 
strukturierte Konfigurationsdateien. Zwei etablierte Formate sind:

\begin{itemize}
    \item \textbf{YAML (YAML Ain't Markup Language)}: Menschenlesbare 
    Syntax für hierarchische Datenstrukturen. Besonders geeignet für 
    Konfigurationen aufgrund der kompakten Notation und Kommentarunterstützung.\cite{yaml_spec}
    
    \item \textbf{JSON (JavaScript Object Notation)}: Maschinenlesbares 
    Format mit strikter Syntax. Ermöglicht direkte Verarbeitung ohne 
    Parser-Bibliotheken in den meisten Programmiersprachen.\cite{json_rfc}
\end{itemize}

\subsubsection{Anwendung in der Gleisplan-Analyse}

Für die Masterarbeit werden YAML-Dateien zur Definition der Mapping-Regeln 
zwischen erkannten Symbolen und ihrer semantischen Bedeutung verwendet. 
Eine beispielhafte Struktur ist:

\begin{verbatim}
signal:
  pattern: "^[A-Z]{1,4}\\d{1,4}$"
  required_fields: ["anchor_text", "coord_text", "fahrtrichtung"]
  linking_radius: 150

balise:
  pattern: "^\\d{4}$"
  required_fields: ["anchor_text"]
  linking_radius: 100
\end{verbatim}

Diese Konfigurationen ermöglichen die Anpassung des Systems an 
kundenspezifische Symbolsysteme ohne Code-Änderungen (vgl. Kapitel 5.3.6).
\section{Rückverfolgbarkeit \& Usability im Kontext sicherheitsrelevanter Systeme}

In sicherheitsrelevanten technischen Systemen, die in Bahnsystemen zum Einsatz kommen, spielt die Rückverfolgbarkeit aller Entwicklungen und Dokumentationsschritte eine zentrale Rolle. Der Begriff \enquote{Rückverfolgbarkeit} bezeichnet die Fähigkeit, jedes erstellte Artefakt, wie Anforderungen, Modelle, Code oder Dokumentation, mit seiner Quelle und seinen Abhängigkeiten zu verknüpfen. \cite{Gotel1994AnAO} \cite{book}
\\
\\
Für bahntechnische Systeme ergibt sich daraus die Bedeutung, jede Information eindeutig bis zu ihrer Quelle zurückverfolgen zu können, sei es in der Planung im Projektionstableau oder in der Test- und Validierungsphase.
\\
\\
Derzeit impliziert dies im Bereich der automatisierten Symbol- und Datenerkennung aus den Gleisplänen, dass jeder exportierte Eintrag nicht nur die erkannten Symbolklassen und OCR-Informationen beinhaltet, sondern auch einen eindeutigen Verweis auf eine Position im Hauptdokument aufweist. Die Realisierung kann dabei beispielsweise über die Bildkoordinaten, Begrenzungsrahmen oder visuelle Vorschauabschnitte erfolgen. 
Die Rückverfolgbarkeit gewährleistet, dass Ingenieure bei Unklarheiten oder Fehlern jederzeit überprüfen können, an welcher Stelle im Gleisplan sich die Informationen befinden.
\\
\\
Neben der Rückverfolgbarkeit gewinnt auch die Benutzerfreundlichkeit des entwickelten Prototyps zunehmend an Bedeutung. Sicherheitsrelevante Systeme müssen nicht nur korrekt funktionieren, sondern auch so konzipiert sein, dass sie effizient und fehlerfrei genutzt werden können. \cite{10.5555/2821575} Eine benutzerfreundliche Oberfläche reduziert die Wahrscheinlichkeit von Bedienungsfehlern, ermöglicht eine effiziente Einarbeitung und gewährleistet 
die zuverlässige Überprüfung und weitere Verarbeitung der Ergebnisse. Dies ist insbesondere für Ingenieure in der Planung und Validierung von Bahnsystemen von großer Bedeutung, da sie mit großen Datenmengen arbeiten und gleichzeitig hohe Qualitätsstandards einhalten müssen.

\subsection{Datenpersistenz und strukturierte Speicherung}

Die automatisierte Extraktion großer Datenmengen aus technischen Zeichnungen 
erfordert effiziente Mechanismen zur persistenten Speicherung, Versionierung 
und Abfrage der Ergebnisse.

\subsubsection{Relationale vs. Dokumentenorientierte Datenbanken}

Für die Speicherung extrahierter Daten kommen grundsätzlich zwei Paradigmen 
in Betracht:

\begin{itemize}
    \item \textbf{Relationale Datenbanksysteme (RDBMS)}: Strukturierte Daten 
    werden in Tabellen mit festen Schemata organisiert. Dies gewährleistet 
    ACID-Eigenschaften (Atomicity, Consistency, Isolation, Durability) und 
    eignet sich für Anwendungen mit klaren Datenbeziehungen.
    
    \item \textbf{Dokumentenorientierte Datenbanken (NoSQL)}: Semi-strukturierte 
    Daten werden als JSON/BSON-Dokumente gespeichert. Dies bietet Flexibilität 
    bei variablen Datenstrukturen, jedoch schwächere Konsistenzgarantien.
\end{itemize}

\subsubsection{Hybrid-Ansatz: PostgreSQL mit JSONB}

Moderne relationale Datenbanksysteme wie PostgreSQL unterstützen hybride Speichermodelle durch native JSON-Unterstützung (JSONB-Datentyp). Dies 
vereint die Vorteile beider Welten\cite{postgresql_docs}:

\begin{itemize}
    \item Transaktionale Konsistenz für kritische Metadaten
    \item Schema-Flexibilität für variable Extraktionsresultate
    \item Effiziente Indizierung (GIN-Index) für JSON-Abfragen
\end{itemize}

Die konkrete Implementierung des Datenbankschemas für die Gleisplan-Extraktion, einschließlich der Tabellendefinitionen und Indizierungsstrategien, wird in Abschnitt 6.6.3 (PostgreSQL-Persistenz) beschrieben.